<!DOCTYPE html>
<html lang="en" data-theme="cyberpunk" data-content-tier="practitioner">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="description" content="A Comprehensive, Unified, and In-Depth Guide to Deep Reinforcement Learning, from Foundations to Advanced Applications. Offline single-file edition."/>
  <meta name="keywords" content="Reinforcement Learning, Deep Reinforcement Learning, RL, DRL, DQN, Dueling DQN, Munchausen RL, Reward Shaping, PBRS, Inventory Management, Robotics, Path Planning, Markov Decision Process, MDP, Bellman Equation, Q-Learning, PPO, SAC, RLHF, MLOps"/>
  <title>Deep Reinforcement Learning: The Omnibus</title>
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Orbitron:wght@700;900&family=Lora:wght@400;600&family=Russo+One&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        macros: {
          // General Math & RL
          // Removed R and E macros to avoid collision with reward and expectation variables in the text.
          // Removed problematic macros that caused infinite recursion in MathJax. 
          // Defining V, Q, Vstar and Qstar in terms of themselves led to recursive substitutions.
          // Use raw text in formulas instead.
          // Specific Algorithms
          L_DQN: "\\mathcal{L}_{\\text{DQN}}",
          grad: "\\nabla_{\\theta}"
        }
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    };
  </script>
    <!-- MathJax library restored to properly render LaTeX equations. -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10.9.0/dist/mermaid.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js"></script>
  <script src="https://cdn.plot.ly/plotly-3.0.1.min.js" charset="utf-8"></script><!-- Plotly CDN for interactive charts -->
  <script type="module">
    import {Runtime, Inspector} from "https://cdn.jsdelivr.net/npm/@observablehq/runtime@5";
    import {Library} from "https://cdn.jsdelivr.net/npm/@observablehq/stdlib@5";
    window.Observable = { Runtime, Inspector, Library };
  </script>

  <style>
    /* --- DESIGN SYSTEM & THEMES --- */
    :root {
      /* Transitions & Base Properties */
      --radius-sm: 4px;
      --radius-md: 8px;
      --radius-lg: 16px;
      --transition-fast: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
      --transition-medium: all 0.3s ease-in-out;
      --shadow-inset: inset 0 2px 4px 0 rgba(0,0,0,0.5);
    }
    
    /* 1. CYBERPUNK (Default Dark) */
    html[data-theme="cyberpunk"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Orbitron', sans-serif;
      --color-bg-primary: #0a0a14;
      --color-bg-secondary: #101222;
      --color-surface: rgba(22, 25, 48, 0.7);
      --color-surface-solid: #161930;
      --color-border: rgba(76, 89, 186, 0.3);
      --color-border-hover: rgba(121, 128, 255, 0.7);
      --color-shadow: rgba(0, 0, 0, 0.5);
      --color-text-primary: #e0e1ff;
      --color-text-secondary: #a0a3d4;
      --color-text-muted: #6a6f9a;
      --color-text-headings: #ffffff;
      --color-accent-primary: #00f7ff;
      --color-accent-secondary: #f000ff;
      --color-accent-success: #00ff9c;
      --color-accent-warning: #ffc700;
      --color-accent-danger: #ff3c5b;
      --shadow-glow-primary: 0 0 12px 0 rgba(0, 247, 255, 0.4);
      --shadow-glow-secondary: 0 0 12px 0 rgba(240, 0, 255, 0.4);
      --backdrop-blur: blur(10px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 2. SOLARIZED DARK */
    html[data-theme="solarized-dark"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #002b36;
      --color-bg-secondary: #073642;
      --color-surface: rgba(7, 54, 66, 0.8);
      --color-surface-solid: #073642;
      --color-border: #2a5f6f;
      --color-border-hover: #586e75;
      --color-shadow: rgba(0, 0, 0, 0.3);
      --color-text-primary: #839496;
      --color-text-secondary: #657b83;
      --color-text-muted: #586e75;
      --color-text-headings: #93a1a1;
      --color-accent-primary: #268bd2;
      --color-accent-secondary: #6c71c4;
      --color-accent-success: #859900;
      --color-accent-warning: #cb4b16;
      --color-accent-danger: #dc322f;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(10px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 3. SOLARIZED LIGHT */
    html[data-theme="solarized-light"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #fdf6e3;
      --color-bg-secondary: #eee8d5;
      --color-surface: rgba(238, 232, 213, 0.8);
      --color-surface-solid: #eee8d5;
      --color-border: #dcd6c4;
      --color-border-hover: #93a1a1;
      --color-shadow: rgba(0, 0, 0, 0.1);
      --color-text-primary: #657b83;
      --color-text-secondary: #839496;
      --color-text-muted: #93a1a1;
      --color-text-headings: #586e75;
      --color-accent-primary: #268bd2;
      --color-accent-secondary: #6c71c4;
      --color-accent-success: #859900;
      --color-accent-warning: #cb4b16;
      --color-accent-danger: #dc322f;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(10px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 4. NORD */
    html[data-theme="nord"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #2E3440;
      --color-bg-secondary: #3B4252;
      --color-surface: rgba(59, 66, 82, 0.8);
      --color-surface-solid: #3B4252;
      --color-border: #4C566A;
      --color-border-hover: #D8DEE9;
      --color-shadow: rgba(0, 0, 0, 0.2);
      --color-text-primary: #D8DEE9;
      --color-text-secondary: #E5E9F0;
      --color-text-muted: #81A1C1;
      --color-text-headings: #ECEFF4;
      --color-accent-primary: #88C0D0;
      --color-accent-secondary: #81A1C1;
      --color-accent-success: #A3BE8C;
      --color-accent-warning: #EBCB8B;
      --color-accent-danger: #BF616A;
      --shadow-glow-primary: 0 0 8px 0 rgba(136, 192, 208, 0.3);
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(8px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 5. DRACULA */
    html[data-theme="dracula"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #282a36;
      --color-bg-secondary: #1e1f29;
      --color-surface: rgba(68, 71, 90, 0.8);
      --color-surface-solid: #44475a;
      --color-border: #6272a4;
      --color-border-hover: #bd93f9;
      --color-shadow: rgba(0, 0, 0, 0.3);
      --color-text-primary: #f8f8f2;
      --color-text-secondary: #bd93f9;
      --color-text-muted: #6272a4;
      --color-text-headings: #f8f8f2;
      --color-accent-primary: #bd93f9;
      --color-accent-secondary: #ff79c6;
      --color-accent-success: #50fa7b;
      --color-accent-warning: #f1fa8c;
      --color-accent-danger: #ff5555;
      --shadow-glow-primary: 0 0 8px 0 rgba(189, 147, 249, 0.4);
      --shadow-glow-secondary: 0 0 8px 0 rgba(255, 121, 198, 0.4);
      --backdrop-blur: blur(8px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 6. GRUVBOX DARK */
    html[data-theme="gruvbox-dark"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #282828;
      --color-bg-secondary: #1d2021;
      --color-surface: rgba(60, 56, 54, 0.8);
      --color-surface-solid: #3c3836;
      --color-border: #504945;
      --color-border-hover: #bdae93;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #ebdbb2;
      --color-text-secondary: #d5c4a1;
      --color-text-muted: #928374;
      --color-text-headings: #fbf1c7;
      --color-accent-primary: #83a598;
      --color-accent-secondary: #d65d0e;
      --color-accent-success: #b8bb26;
      --color-accent-warning: #fe8019;
      --color-accent-danger: #fb4934;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: none;
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 7. MONOKAI PRO */
    html[data-theme="monokai"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #272822;
      --color-bg-secondary: #191a14;
      --color-surface: rgba(62, 61, 50, 0.8);
      --color-surface-solid: #3E3D32;
      --color-border: #555650;
      --color-border-hover: #fcfcfa;
      --color-shadow: rgba(0, 0, 0, 0.5);
      --color-text-primary: #F8F8F2;
      --color-text-secondary: #c2c2bf;
      --color-text-muted: #75715E;
      --color-text-headings: #fcfcfa;
      --color-accent-primary: #66D9EF; /* Cyan */
      --color-accent-secondary: #F92672; /* Pink */
      --color-accent-success: #A6E22E; /* Green */
      --color-accent-warning: #FD971F; /* Orange */
      --color-accent-danger: #F92672;
      --shadow-glow-primary: 0 0 10px 0 rgba(102, 217, 239, 0.5);
      --shadow-glow-secondary: 0 0 10px 0 rgba(249, 38, 114, 0.5);
      --backdrop-blur: blur(5px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 8. GITHUB DARK */
    html[data-theme="github-dark"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #0d1117;
      --color-bg-secondary: #010409;
      --color-surface: rgba(22, 27, 34, 0.8);
      --color-surface-solid: #161b22;
      --color-border: #30363d;
      --color-border-hover: #8b949e;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #c9d1d9;
      --color-text-secondary: #8b949e;
      --color-text-muted: #484f58;
      --color-text-headings: #f0f6fc;
      --color-accent-primary: #58a6ff;
      --color-accent-secondary: #a5d6ff;
      --color-accent-success: #3fb950;
      --color-accent-warning: #d29922;
      --color-accent-danger: #f85149;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(8px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 9. ACADEMIC LIGHT */
    html[data-theme="academic-light"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Lora', serif;
      --color-bg-primary: #fbfaf7;
      --color-bg-secondary: #ffffff;
      --color-surface: rgba(255, 255, 255, 0.8);
      --color-surface-solid: #ffffff;
      --color-border: #e0e0e0;
      --color-border-hover: #b0b0b0;
      --color-shadow: rgba(0, 0, 0, 0.1);
      --color-text-primary: #333333;
      --color-text-secondary: #555555;
      --color-text-muted: #777777;
      --color-text-headings: #000000;
      --color-accent-primary: #00529B;
      --color-accent-secondary: #1E88E5;
      --color-accent-success: #006400;
      --color-accent-warning: #E65100;
      --color-accent-danger: #B71C1C;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: none;
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 10. MATRIX */
    html[data-theme="matrix"] {
      --font-sans: 'JetBrains Mono', monospace;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'JetBrains Mono', monospace;
      --color-bg-primary: #000000;
      --color-bg-secondary: #050505;
      --color-surface: rgba(0, 20, 0, 0.5);
      --color-surface-solid: #001400;
      --color-border: #003B00;
      --color-border-hover: #00FF41;
      --color-shadow: rgba(0, 255, 65, 0.3);
      --color-text-primary: #00FF41;
      --color-text-secondary: #00c732;
      --color-text-muted: #008f23;
      --color-text-headings: #c0ffc0;
      --color-accent-primary: #00FF41;
      --color-accent-secondary: #50fa7b;
      --color-accent-success: #00FF41;
      --color-accent-warning: #c0ffc0;
      --color-accent-danger: #c0ffc0;
      --shadow-glow-primary: 0 0 10px 0 var(--color-accent-primary);
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(2px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 11. 80S SYNTHWAVE */
    html[data-theme="synthwave"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Russo One', sans-serif;
      --color-bg-primary: #24133f;
      --color-bg-secondary: #1a0e2d;
      --color-surface: rgba(45, 27, 83, 0.7);
      --color-surface-solid: #2d1b53;
      --color-border: rgba(255, 107, 222, 0.3);
      --color-border-hover: #ff6bde;
      --color-shadow: rgba(0, 0, 0, 0.5);
      --color-text-primary: #f0e8ff;
      --color-text-secondary: #c8bde8;
      --color-text-muted: #82799f;
      --color-text-headings: #ffffff;
      --color-accent-primary: #f92a95; /* Hot Pink */
      --color-accent-secondary: #00e8f5; /* Cyan */
      --color-accent-success: #72f1b8;
      --color-accent-warning: #f9a62a;
      --color-accent-danger: #ff4c65;
      --shadow-glow-primary: 0 0 12px 0 rgba(249, 42, 149, 0.6);
      --shadow-glow-secondary: 0 0 12px 0 rgba(0, 232, 245, 0.6);
      --backdrop-blur: blur(8px);
      --bg-image: 
        linear-gradient(rgba(36, 19, 63, 0.9), rgba(36, 19, 63, 0.9)),
        linear-gradient(to bottom, transparent 98%, var(--color-accent-secondary) 100%);
      --bg-size: 100%, 100% 4px;
    }

    /* 12. BLUEPRINT */
    html[data-theme="blueprint"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'JetBrains Mono', monospace;
      --color-bg-primary: #0a234f;
      --color-bg-secondary: #081c40;
      --color-surface: rgba(10, 48, 102, 0.5);
      --color-surface-solid: #0a3066;
      --color-border: #3a75c4;
      --color-border-hover: #a6d5ff;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #a6d5ff;
      --color-text-secondary: #73b3f2;
      --color-text-muted: #4e83ba;
      --color-text-headings: #ffffff;
      --color-accent-primary: #ffffff;
      --color-accent-secondary: #ffe180;
      --color-accent-success: #a6d5ff;
      --color-accent-warning: #ffe180;
      --color-accent-danger: #ffabab;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: none;
      --bg-image: 
        linear-gradient(rgba(58,117,196,0.15) 1px, transparent 1px),
        linear-gradient(90deg, rgba(58,117,196,0.15) 1px, transparent 1px);
      --bg-size: 2rem 2rem;
    }

    /* --- RESET & BASE STYLES --- */
    *, *::before, *::after { 
      box-sizing: border-box; 
    }
    
    html {
      scroll-padding-top: 80px; /* Header height */
      scroll-behavior: smooth;
      font-size: 16px;
    }
    
    body {
      margin: 0;
      font-family: var(--font-sans);
      background-color: var(--color-bg-primary);
      color: var(--color-text-primary);
      line-height: 1.7;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
      background-image: var(--bg-image);
      background-size: var(--bg-size);
      transition: background-color 0.3s, color 0.3s;
    }

    /* --- THEMATIC SCROLLBAR --- */
    html {
        scrollbar-width: thin;
        scrollbar-color: var(--color-border) var(--color-bg-secondary);
    }
    ::-webkit-scrollbar {
        width: 10px;
        height: 10px;
    }
    ::-webkit-scrollbar-track {
        background: var(--color-bg-secondary);
    }
    ::-webkit-scrollbar-thumb {
        background-color: var(--color-border);
        border-radius: 5px;
        border: 2px solid var(--color-bg-secondary);
    }
    ::-webkit-scrollbar-thumb:hover {
        background-color: var(--color-border-hover);
    }
    
    /* --- TYPOGRAPHY --- */
    h1, h2, h3, h4, h5, h6 {
      font-family: var(--font-display);
      font-weight: 700;
      line-height: 1.3;
      margin: 2.5rem 0 1.5rem 0;
      color: var(--color-text-headings);
      letter-spacing: 1px;
      text-shadow: 0 0 5px var(--color-shadow);
    }
    
    h1 { 
      font-size: clamp(2.5rem, 5vw, 3.5rem); 
      font-weight: 900;
      background: linear-gradient(90deg, var(--color-accent-primary), var(--color-accent-secondary), var(--color-accent-primary));
      background-size: 200% auto;
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      animation: gradient-text 10s linear infinite;
    }
    h2 { 
      font-size: clamp(2rem, 4vw, 2.5rem); 
      border-bottom: 2px solid var(--color-border);
      padding-bottom: 0.75rem;
      margin-top: 4rem;
      border-image: linear-gradient(90deg, var(--color-accent-primary), var(--color-accent-secondary)) 1;
    }
    h3 { font-size: 1.75rem; }
    h4 { font-size: 1.25rem; }
    h5 { font-size: 1.125rem; }
    h6 { font-size: 1rem; }
    h1 .fas, h2 .fas, h3 .fas { margin-right: 0.75rem; color: var(--color-accent-primary); }

    p { 
      margin: 0 0 1.5rem 0; 
      color: var(--color-text-secondary);
      max-width: 90ch;
    }
    
    a { 
      color: var(--color-accent-primary);
      text-decoration: none;
      transition: var(--transition-fast);
      font-weight: 500;
    }
    a:hover { 
      filter: brightness(1.2);
      text-shadow: var(--shadow-glow-primary);
    }

    strong { color: var(--color-accent-warning); font-weight: 600; }
    
    /* --- THEMATIC SEPARATOR --- */
    hr {
        border: 0;
        height: 2px;
        margin: 4rem auto;
        width: 50%;
        background-image: linear-gradient(to right, transparent, var(--color-accent-primary), var(--color-accent-secondary), var(--color-accent-primary), transparent);
    }
    
    /* --- LAYOUT & MAIN COMPONENTS --- */
    .container {
      display: grid;
      grid-template-columns: 280px 1fr;
      grid-template-rows: auto 1fr auto;
      grid-template-areas:
        "header header"
        "sidebar main"
        "footer footer";
      min-height: 100vh;
      transition: grid-template-columns 0.3s ease;
    }
    .container.toc-hidden { grid-template-columns: 0px 1fr; }

    header {
      grid-area: header;
      background: var(--color-surface);
      backdrop-filter: var(--backdrop-blur);
      -webkit-backdrop-filter: var(--backdrop-blur);
      border-bottom: 1px solid var(--color-border);
      padding: 1rem 2rem;
      position: sticky;
      top: 0;
      z-index: 1000;
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 1rem;
    }
    header h1 { margin: 0; font-size: 1.5rem; }
    
    #sidebar {
      grid-area: sidebar;
      position: sticky;
      top: 73px; /* Header height */
      height: calc(100vh - 73px);
      background: var(--color-bg-secondary);
      border-right: 1px solid var(--color-border);
      padding: 1.5rem;
      overflow-y: auto;
      transition: all 0.3s ease;
    }
    
    main {
      grid-area: main;
      max-width: 1000px;
      margin: 0 auto;
      padding: 2rem 3rem;
      width: 100%;
      min-width: 0; /* Fix for grid layout shift */
    }
    
    footer {
      grid-area: footer;
      grid-column: 1 / -1; /* Ensure footer spans all columns */
      padding: 2rem;
      background: var(--color-bg-secondary);
      border-top: 1px solid var(--color-border);
      color: var(--color-text-muted);
      font-size: 0.875rem;
    }
    .footer-content {
      max-width: 1000px;
      margin: 0 auto;
      text-align: center; 
    }
    .feedback-container {
      display: flex;
      justify-content: center;
      align-items: center;
      flex-wrap: wrap;
      gap: 1rem;
      margin: 1.5rem 0;
    }
    .feedback-container p { margin: 0; }
    .footer-socials { display: inline-flex; }
    .footer-socials a {
      color: var(--color-text-muted);
      font-size: 1.3rem;
      margin: 0 0.5rem;
      text-decoration: none;
    }
    .footer-socials a:hover { color: var(--color-accent-primary); }
    .footer-meta {
      font-size: 0.875rem;
      border-top: 1px solid var(--color-border);
      padding-top: 1rem;
      margin-top: 1.5rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    /* --- UI CONTROLS (BUTTONS, ETC) --- */
    .control-button {
      background: transparent;
      border: 1px solid var(--color-border);
      color: var(--color-text-secondary);
      font-size: 1.2rem;
      cursor: pointer;
      width: 44px;
      height: 44px;
      border-radius: var(--radius-md);
      display: flex;
      align-items: center;
      justify-content: center;
      transition: var(--transition-fast);
      position: relative;
    }
    .control-button:hover {
      color: var(--color-accent-primary);
      border-color: var(--color-accent-primary);
      box-shadow: var(--shadow-glow-primary);
      transform: translateY(-2px);
    }
    
    #search-container {
      flex-grow: 1;
      max-width: 400px;
      position: relative;
    }
    #search-input {
      width: 100%;
      background: var(--color-bg-primary);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-md);
      padding: 0.5rem 1rem 0.5rem 2.5rem;
      color: var(--color-text-primary);
      font-family: var(--font-sans);
      transition: var(--transition-fast);
    }
    #search-input:focus {
      outline: none;
      border-color: var(--color-accent-primary);
      box-shadow: var(--shadow-glow-primary);
    }
    #search-container .fa-search {
      position: absolute;
      left: 1rem;
      top: 50%;
      transform: translateY(-50%);
      color: var(--color-text-muted);
    }
    
    /* Theme Switcher */
    #theme-switcher {
      position: absolute;
      top: calc(100% + 10px);
      right: 0;
      background: var(--color-surface-solid);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-md);
      padding: 0.5rem;
      box-shadow: 0 10px 20px var(--color-shadow);
      z-index: 1001;
      opacity: 0;
      visibility: hidden;
      transform: translateY(10px);
      transition: var(--transition-medium);
      display: flex;
      flex-direction: column;
      gap: 0.25rem;
      width: 200px;
    }
    #theme-toggle-btn-container.active #theme-switcher {
        opacity: 1;
        visibility: visible;
        transform: translateY(0);
    }
    .theme-option {
      padding: 0.5rem 1rem;
      border-radius: var(--radius-sm);
      cursor: pointer;
      transition: var(--transition-fast);
      text-align: left;
      font-size: 0.9rem;
      color: var(--color-text-secondary);
      background: none;
      border: none;
      width: 100%;
    }
    .theme-option:hover, .theme-option.active {
      background: var(--color-surface);
      color: var(--color-accent-primary);
    }
    
    /* Tiered Content Toggle */
    #tier-switcher {
        display: flex;
        gap: 0.5rem;
        background: var(--color-surface-solid);
        padding: 0.25rem;
        border-radius: var(--radius-md);
        border: 1px solid var(--color-border);
    }
    .tier-option {
        padding: 0.3rem 0.8rem;
        border-radius: var(--radius-sm);
        cursor: pointer;
        transition: var(--transition-fast);
        font-size: 0.8rem;
        color: var(--color-text-secondary);
        background: none;
        border: none;
        font-family: var(--font-mono);
    }
    .tier-option:hover, .tier-option.active {
        background: var(--color-surface);
        color: var(--color-accent-primary);
        box-shadow: var(--shadow-glow-primary);
    }


    /* --- SIDEBAR NAVIGATION --- */
    #sidebar-nav { list-style: none; padding: 0; margin: 0; }
    #sidebar-nav ul { padding-left: 1rem; list-style: none; border-left: 1px solid var(--color-border); }
    #sidebar-nav li a {
      display: block;
      padding: 0.4rem 0.75rem;
      border-radius: var(--radius-sm);
      color: var(--color-text-secondary);
      font-weight: 500;
      font-size: 0.9rem;
      transition: var(--transition-fast);
      position: relative;
    }
    #sidebar-nav > li > a { font-weight: 700; color: var(--color-text-headings); font-size: 1rem; }
    #sidebar-nav li a:hover, #sidebar-nav li a.active {
      background: var(--color-surface);
      color: var(--color-accent-primary);
      transform: translateX(4px);
    }
    #sidebar-nav li a.active::before {
      content: '';
      position: absolute;
      left: -1rem;
      top: 0;
      bottom: 0;
      width: 2px;
      background-color: var(--color-accent-primary);
      box-shadow: var(--shadow-glow-primary);
    }

    /* --- CODE & MATH --- */
    code, pre { font-family: var(--font-mono); font-size: 0.9em; }
    :not(pre) > code {
      background: var(--color-surface);
      color: var(--color-accent-warning);
      padding: 0.2rem 0.4rem;
      border-radius: var(--radius-sm);
      border: 1px solid var(--color-border);
    }
    .code-container {
        margin: 1.5rem 0;
        border-radius: var(--radius-md);
        border: 1px solid var(--color-border);
        overflow: hidden;
        background: var(--color-surface);
    }
    pre {
      padding: 0;
      margin: 0;
      overflow: hidden;
      box-shadow: var(--shadow-inset);
      position: relative;
    }
    pre > code {
      display: block;
      padding: 1.5rem;
      overflow-x: auto;
    }
    .code-block-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      background-color: var(--color-surface-solid);
      padding: 0.5rem 1rem;
      border-bottom: 1px solid var(--color-border);
    }
    .code-block-header span { font-family: var(--font-mono); color: var(--color-text-muted); text-transform: uppercase; font-size: 0.8rem; }
    .code-block-header .buttons { display: flex; gap: 0.5rem; }
    .code-block-header button, .code-block-header a {
      font-size: 0.8rem;
      padding: 0.25rem 0.75rem;
      background: var(--color-surface);
      color: var(--color-text-secondary);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-sm);
      display: flex;
      align-items: center;
      gap: 0.5rem;
      cursor: pointer;
      transition: var(--transition-fast);
      line-height: 1;
      font-family: var(--font-sans);
      text-decoration: none;
    }
    .code-block-header button:hover, .code-block-header a:hover { 
        background: var(--color-surface-solid);
        color: var(--color-text-primary);
        border-color: var(--color-border-hover);
        transform: translateY(-1px); 
    }
    .code-variants summary {
        background: none;
        padding: 0.5rem 1rem;
        font-size: 0.9rem;
        color: var(--color-text-muted);
    }
    .code-variants .details-content {
        padding: 0;
        border-top: 1px solid var(--color-border);
    }
    
    /* --- INTERACTIVE & UI ELEMENTS --- */
    details {
      border: 1px solid var(--color-border);
      border-radius: var(--radius-md);
      margin: 1.5rem 0;
      background: var(--color-surface);
      backdrop-filter: var(--backdrop-blur);
      -webkit-backdrop-filter: var(--backdrop-blur);
      transition: var(--transition-medium);
      overflow: hidden;
    }
    details:hover { box-shadow: 0 0 15px var(--color-shadow); }
    summary {
      cursor: pointer;
      font-weight: 600;
      padding: 1.25rem 1.5rem;
      display: flex;
      align-items: center;
      justify-content: space-between;
      color: var(--color-text-headings);
      background: rgba(0,0,0,0.1);
    }
    summary:hover { background: rgba(0,0,0,0.2); }
    summary::after {
      content: '▼';
      font-size: 0.875rem;
      color: var(--color-text-muted);
      transition: transform 0.3s ease, color 0.3s ease;
    }
    details[open] > summary::after { transform: rotate(180deg); color: var(--color-accent-primary); }
    .details-content { padding: 0.5rem 1.5rem 1.5rem 1.5rem; }
    
    /* Tiered Content Visibility */
    html[data-content-tier="beginner"] .deep-dive,
    html[data-content-tier="beginner"] .research-tier { display: none; }
    html[data-content-tier="practitioner"] .research-tier { display: none; }
    .deep-dive summary, .research-tier summary {
        font-style: italic;
    }


    /* Tooltips */
    .tooltip {
      position: relative;
      cursor: help;
      border-bottom: 1px dotted var(--color-accent-primary);
    }
    .tooltip::after {
      content: attr(data-tooltip);
      position: absolute;
      bottom: 125%;
      left: 50%;
      transform: translateX(-50%);
      background-color: var(--color-surface-solid);
      color: var(--color-text-primary);
      padding: 0.5rem 1rem;
      border-radius: var(--radius-md);
      border: 1px solid var(--color-border);
      font-size: 0.85rem;
      white-space: nowrap;
      opacity: 0;
      visibility: hidden;
      transition: var(--transition-fast);
      z-index: 10;
    }
    .tooltip:hover::after { opacity: 1; visibility: visible; }
    
    /* --- ADMONITIONS --- */
    .admonition {
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-left: 4px solid;
      border-radius: var(--radius-md);
      background: var(--color-surface);
      position: relative;
    }
    .admonition-title {
      font-weight: 700;
      margin-bottom: 0.5rem;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      font-family: var(--font-display);
    }
    .admonition.info { border-color: var(--color-accent-primary); }
    .admonition.info .admonition-title { color: var(--color-accent-primary); }
    .admonition.danger { border-color: var(--color-accent-danger); }
    .admonition.danger .admonition-title { color: var(--color-accent-danger); }
    .admonition.tip { border-color: var(--color-accent-success); }
    .admonition.tip .admonition-title { color: var(--color-accent-success); }
    .admonition.warning { border-color: var(--color-accent-warning); }
    .admonition.warning .admonition-title { color: var(--color-accent-warning); }
    
    /* --- TABLES --- */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 2rem 0;
      background: var(--color-surface);
      border-radius: var(--radius-md);
      overflow: hidden;
      border: 1px solid var(--color-border);
    }
    th, td {
      padding: 0.75rem 1rem;
      text-align: left;
      border-bottom: 1px solid var(--color-border);
    }
    th {
      background: var(--color-surface-solid);
      font-weight: 600;
      color: var(--color-text-headings);
      font-family: var(--font-mono);
    }
    tr:last-child td { border-bottom: none; }
    tr:hover { background: var(--color-surface-solid); }

    /* --- VISUALIZATIONS & TOOLBAR --- */
    .visualization-container {
        position: relative;
        margin: 1.5rem 0;
        border: 1px solid var(--color-border);
        border-radius: var(--radius-md);
        background: var(--color-surface);
        overflow: hidden;
        padding: 1rem;
    }
    .visualization-container > .mermaid,
    .visualization-container > svg,
    .visualization-container > img {
        display: block;
        width: 100%;
        height: auto;
        cursor: grab;
        transition: transform 0.2s ease-out;
    }
    .visualization-container > .mermaid:active,
    .visualization-container > svg:active,
    .visualization-container > img:active {
        cursor: grabbing;
    }
    .vis-toolbar {
        position: absolute;
        top: 1rem;
        right: 1rem;
        z-index: 10;
        display: flex;
        gap: 0.5rem;
        background: var(--color-surface-solid);
        padding: 0.5rem;
        border-radius: var(--radius-md);
        border: 1px solid var(--color-border);
        opacity: 0;
        visibility: hidden;
        transform: translateY(-10px);
        transition: var(--transition-medium);
    }
    .visualization-container:hover .vis-toolbar {
        opacity: 1;
        visibility: visible;
        transform: translateY(0);
    }
    .vis-toolbar button {
        background: var(--color-surface);
        border: 1px solid var(--color-border);
        color: var(--color-text-secondary);
        width: 32px;
        height: 32px;
        border-radius: var(--radius-sm);
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 0.9rem;
        transition: var(--transition-fast);
    }
    .vis-toolbar button:hover {
        color: var(--color-accent-primary);
        border-color: var(--color-accent-primary);
        box-shadow: var(--shadow-glow-primary);
    }

    /* --- INTERACTIVE LAB --- */
    .interactive-lab {
        border: 1px solid var(--color-border);
        border-radius: var(--radius-lg);
        background: var(--color-bg-secondary);
        padding: 1.5rem;
        margin-top: 2rem;
    }
    .lab-header {
        font-family: var(--font-display);
        color: var(--color-accent-secondary);
        font-size: 1.5rem;
        margin-bottom: 1rem;
        display: flex;
        align-items: center;
    }
    .lab-header .fas {
        margin-right: 0.75rem;
    }
    .lab-controls {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-bottom: 1.5rem;
        align-items: center;
    }
    .lab-control {
        display: flex;
        flex-direction: column;
        gap: 0.5rem;
    }
    .lab-control label {
        font-family: var(--font-mono);
        font-size: 0.9rem;
        color: var(--color-text-muted);
    }
    .lab-control input[type="range"] {
        width: 200px;
    }
    
    /* --- OTHER UI --- */
    #back-to-top {
        position: fixed;
        bottom: 2rem;
        right: 2rem;
        z-index: 100;
        opacity: 0;
        visibility: hidden;
        transform: translateY(20px);
        transition: var(--transition-medium);
    }
    #back-to-top.visible {
        opacity: 1;
        visibility: visible;
        transform: translateY(0);
    }
    
    mark {
        background-color: var(--color-accent-warning);
        color: var(--color-bg-primary);
        padding: 2px 4px;
        border-radius: var(--radius-sm);
    }
    
    /* --- PROGRESS BAR & ANIMATIONS --- */
    .progress-bar {
      position: fixed;
      top: 0;
      left: 0;
      height: 3px;
      background: linear-gradient(90deg, var(--color-accent-primary), var(--color-accent-secondary));
      box-shadow: var(--shadow-glow-primary);
      z-index: 9999;
      transition: width 0.1s linear;
    }
    @keyframes gradient-text { 
      0% { background-position: 0% 50%; } 
      50% { background-position: 100% 50%; } 
      100% { background-position: 0% 50%; } 
    }
    @keyframes scanline {
      0% { background-position: 0 0; }
      100% { background-position: 0 100%; }
    }
    html[data-theme="synthwave"] body::after {
        content: '';
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background-image: linear-gradient(to bottom, transparent 98%, var(--color-accent-secondary) 100%);
        background-size: 100% 4px;
        z-index: -1;
        pointer-events: none;
        animation: scanline 8s linear infinite;
    }
    
    .fade-in {
      animation: fadeIn 0.5s ease-out forwards;
    }
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(15px); }
      to { opacity: 1; transform: translateY(0); }
    }
    
    /* --- RESPONSIVENESS --- */
    @media (max-width: 1024px) {
      .container {
        grid-template-columns: 1fr;
        grid-template-areas:
          "header"
          "main"
          "footer";
      }
      #sidebar { display: none; }
      #toc-toggle-btn { display: none; }
      main { padding: 1.5rem; }
      header { padding: 1rem; }
      #search-container { max-width: 250px; }
    }
    @media (max-width: 768px) {
      h1 { font-size: 2rem; }
      h2 { font-size: 1.75rem; }
      main { padding: 1rem; }
      header { flex-wrap: wrap; justify-content: center; gap: 0.5rem; row-gap: 1rem; }
      header > h1 { order: -2; width: 100%; text-align: center;}
      #search-container { order: 3; width: 100%;}
      #toc-toggle-btn, #theme-toggle-btn-container { order: -1; }
      #tier-switcher { order: 2; }

  /* Part-level toggle button styling for collapsible sections */
  button.part-toggle {
    margin-left: 1rem;
    padding: 0.25rem 0.75rem;
    font-size: 0.9rem;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    background-color: var(--color-border-hover);
    color: var(--color-text-primary, #fff);
    transition: background-color 0.2s ease;
  }
  button.part-toggle:hover {
    background-color: var(--color-accent, #7a82ff);
  }
    }
  </style>
</head>

<body>
  <div class="progress-bar" id="progress-bar"></div>
  
  <div class="container" id="container">
    <header>
      <button id="toc-toggle-btn" class="control-button" title="Toggle Table of Contents">
        <i class="fas fa-bars"></i>
      </button>
      
      <div id="tier-switcher">
        <button class="tier-option" data-tier="beginner" title="Show content for beginners">Beginner</button>
        <button class="tier-option active" data-tier="practitioner" title="Show content for practitioners">Practitioner</button>
        <button class="tier-option" data-tier="researcher" title="Show content for researchers">Researcher</button>
      </div>
      
      <div id="search-container">
        <i class="fas fa-search"></i>
        <input type="search" id="search-input" placeholder="Search document...">
      </div>

      <div id="theme-toggle-btn-container" style="position: relative;">
        <button class="control-button" id="theme-toggle-btn" title="Change Theme">
            <i class="fas fa-palette"></i>
        </button>
        <div id="theme-switcher">
            </div>
      </div>
    </header>

    <aside id="sidebar">
      <nav id="sidebar-nav">
        <p>Loading Table of Contents...</p>
      </nav>
    </aside>

    <main id="main-content">
      <section id="title-section">
        <h1 style="text-align: center; margin-bottom: 2rem;">
            <i class="fas fa-brain"></i>
            Deep Reinforcement Learning
        </h1>
      </section>
      <hr/>
      <section id="part0">
        <h2 id="part0-title">
          <i class="fas fa-compass"></i>
          Part 0: Orientation & Tooling
        </h2>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-hand-sparkles"></i>
            0.1 Welcome & Philosophy
          </summary>
          <div class="details-content">
            <h4>
              <i class="fas fa-rocket"></i>
              Welcome to the Deep RL Omnibus!
            </h4>
            <p>
                This document is a living, comprehensive guide to Deep Reinforcement Learning, designed from the ground up for clarity, depth, and practical application. Unlike static textbooks, the Omnibus treats learning as an active process. Here, theory is not just described but visualized, and algorithms are not just presented but made transparent.
            </p>

            <h5>A Brief History of Reinforcement Learning</h5>
            <p>The ideas behind RL have a rich history, drawing from computer science, psychology, and control theory. Key milestones include Richard Bellman's work on <strong>dynamic programming</strong> in the 1950s, which gave us the foundational Bellman equations. This was followed by the development of model-free methods like TD-learning in the 1980s by Sutton and Barto. The modern era of <em>Deep</em> RL began in earnest with DeepMind's 2015 paper demonstrating a DQN agent that could master Atari games from raw pixels, culminating in breakthroughs like AlphaGo's defeat of the world's best Go player in 2016.</p>
            
<details class="deep-dive" open>
  <summary><i class="fas fa-timeline"></i> Deep Dive: A Visual History of RL Milestones</summary>
  <div class="details-content">
    <div class="visualization-container">
      <div class="vis-toolbar">
        <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
        <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
        <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
        <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
      </div>

      <!-- NEW animated SVG timeline -->
      <svg id="rl-timeline-svg" viewBox="0 0 800 160" style="width:100%;height:auto;cursor:pointer">
        <defs>
          <!-- dynamic gradient tied to theme accents -->
          <linearGradient id="tlGradient" x1="0%" y1="0%" x2="100%" y2="0%">
            <stop offset="0%"  stop-color="var(--color-accent-primary)" stop-opacity=".25"/>
            <stop offset="50%" stop-color="var(--color-accent-secondary)" stop-opacity=".25"/>
            <stop offset="100%" stop-color="var(--color-accent-primary)" stop-opacity=".25"/>
          </linearGradient>
        </defs>

        <!-- era backgrounds -->
        <rect x="0"   y="0" width="266" height="160" fill="url(#tlGradient)"/>
        <rect x="266" y="0" width="267" height="160" fill="url(#tlGradient)"/>
        <rect x="533" y="0" width="267" height="160" fill="url(#tlGradient)"/>

        <!-- milestone circles -->
        <circle cx="60"  cy="80" r="8" class="tl-milestone" data-tip="Bellman Equation (1957)"/>
        <circle cx="120" cy="80" r="8" class="tl-milestone" data-tip="Samuel’s Checkers (1959)"/>
        <circle cx="200" cy="80" r="8" class="tl-milestone" data-tip="TD(λ) & Sutton/Barto (1988)"/>
        <circle cx="300" cy="80" r="8" class="tl-milestone" data-tip="TD-Gammon (1992)"/>
        <circle cx="380" cy="80" r="8" class="tl-milestone" data-tip="Deadly Triad Theorems (1997)"/>
        <circle cx="460" cy="80" r="8" class="tl-milestone" data-tip="Neural Fitted Q (2005)"/>
        <circle cx="540" cy="80" r="8" class="tl-milestone" data-tip="DQN on Atari (2015)"/>
        <circle cx="620" cy="80" r="8" class="tl-milestone" data-tip="AlphaGo (2016)"/>
        <circle cx="700" cy="80" r="8" class="tl-milestone" data-tip="RLHF & Diffusion (2020s)"/>

        <!-- labels -->
        <text x="60"  y="110" class="tl-label">Bellman</text>
        <text x="120" y="110" class="tl-label">Samuel</text>
        <text x="200" y="110" class="tl-label">TD(λ)</text>
        <text x="300" y="110" class="tl-label">TD-Gammon</text>
        <text x="380" y="110" class="tl-label">Deadly Triad</text>
        <text x="460" y="110" class="tl-label">NFQ</text>
        <text x="540" y="110" class="tl-label">DQN</text>
        <text x="620" y="110" class="tl-label">AlphaGo</text>
        <text x="700" y="110" class="tl-label">RLHF</text>

        <style>
          #rl-timeline-svg .tl-milestone{fill:var(--color-accent-primary);transition:fill .4s}
          #rl-timeline-svg .tl-label{font-size:10px;font-family:var(--font-mono);fill:var(--color-text-primary);text-anchor:middle}
        </style>
      </svg>

      <!-- micro-tooltip for each milestone -->
      <div id="tl-tooltip" style="position:absolute;top:0;left:0;background:var(--color-surface-solid);border:1px solid var(--color-border);padding:4px 8px;border-radius:4px;font-size:12px;pointer-events:none;opacity:0;transition:opacity .2s"></div>
    </div>
  </div>

  <!-- animator & tooltip script -->
  <script>
  (function(){
    const svg   = document.getElementById('rl-timeline-svg');
    const tips  = {60:'Bellman Equation (1957)',120:'Samuel’s Checkers (1959)',200:'TD(λ) & Sutton/Barto (1988)',
                   300:'TD-Gammon (1992)',380:'Deadly Triad Theorems (1997)',460:'Neural Fitted Q (2005)',
                   540:'DQN on Atari (2015)',620:'AlphaGo (2016)',700:'RLHF & Diffusion (2020s)'};
    const tooltip = document.getElementById('tl-tooltip');
    let hue = 0;
    setInterval(()=>{hue=(hue+1)%360;svg.querySelectorAll('.tl-milestone').forEach(c=>c.style.fill=`hsl(${hue},100%,50%)`);},50);

    svg.addEventListener('mousemove', e=>{
      const el = e.target;
      if (el.classList.contains('tl-milestone')) {
        tooltip.textContent = tips[el.getAttribute('cx')];
        tooltip.style.left = (e.pageX+10)+'px';
        tooltip.style.top  = (e.pageY-20)+'px';
        tooltip.style.opacity = 1;
      }
    });
    svg.addEventListener('mouseleave',()=>tooltip.style.opacity=0);
  })();
  </script>
</details>            
            <div class="admonition info">
              <span class="admonition-title">
                <i class="fas fa-lightbulb"></i>
                Our Learning Philosophy
              </span>
              <ul>
                <li><strong>First-Principles Thinking:</strong> We deconstruct complex algorithms into their core mathematical and intuitive components. You won't just learn <em>what</em> works, but <em>why</em> it works.</li>
                <li><strong>Visual Intuition:</strong> Abstract concepts are made tangible through animated diagrams, interactive plots, and clear visualizations. We believe seeing a concept in action is as important as reading its definition.</li>
                <li><strong>Progressive Disclosure:</strong> Information unfolds naturally. We start with high-level intuition, move to the formal mathematics, and then dive into implementation details, allowing you to learn at your preferred level of depth.</li>
                <li><strong>Practicality First:</strong> While rigorously mathematical, our focus is on building practical understanding. All code is annotated with implementation notes, common pitfalls, and best practices.</li>
              </ul>
            </div>

            <h4>
              <i class="fas fa-users"></i>
              Who This Is For & How to Navigate
            </h4>
            <p>This guide is crafted for a diverse audience. You can use the toggles in the header to filter content based on your learning goals. The main text follows the <strong>Practitioner Path</strong>, while collapsible blocks offer deeper dives for researchers or simplified explanations for beginners.</p>

<!-- ==== FULL REPLACEMENT (keeps all personas, adds motion) ==== -->
<div class="visualization-container">
  <div class="vis-toolbar">
    <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
    <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
    <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
  </div>

  <svg id="persona-path-svg" viewBox="0 0 600 120" style="width:100%">
    <defs>
      <linearGradient id="personaGrad" x1="0%" y1="0%" x2="100%" y2="0%">
        <stop offset="0%"  stop-color="var(--color-accent-success)"/>
        <stop offset="50%" stop-color="var(--color-accent-primary)"/>
        <stop offset="100%" stop-color="var(--color-accent-secondary)"/>
      </linearGradient>
    </defs>

    <!-- animated flow line -->
    <path id="pFlow" d="M 50 60 Q 150 20 300 60 T 550 60"
          stroke="url(#personaGrad)" stroke-width="3" fill="none">
      <animate attributeName="stroke-dasharray" values="0 600;600 0" dur="3s" repeatCount="indefinite"/>
    </path>

    <!-- persona icons -->
    <g data-tier="beginner">
      <circle cx="50"  cy="60" r="22" fill="var(--color-accent-success)"/>
      <text x="50" y="66" text-anchor="middle" font-size="18">🎓</text>
    </g>
    <g data-tier="practitioner">
      <circle cx="300" cy="60" r="22" fill="var(--color-accent-primary)"/>
      <text x="300" y="66" text-anchor="middle" font-size="18">💻</text>
    </g>
    <g data-tier="researcher">
      <circle cx="550" cy="60" r="22" fill="var(--color-accent-secondary)"/>
      <text x="550" y="66" text-anchor="middle" font-size="18">🔬</text>
    </g>

    <!-- labels -->
    <text x="50"  y="100" class="p-label">Beginner</text>
    <text x="300" y="100" class="p-label">Practitioner</text>
    <text x="550" y="100" class="p-label">Researcher</text>

    <style>
      #persona-path-svg .p-label{font-size:12px;font-family:var(--font-sans);fill:var(--color-text-primary);text-anchor:middle}
    </style>
  </svg>
</div>
<script>
(function(){
  const path = document.getElementById('pFlow');
  let hue = 0;
  setInterval(()=>{hue=(hue+2)%360;path.setAttribute('stroke',`hsl(${hue},100%,50%)`);},60);
})();
</script>            
            <details class="research-tier">
                <summary><i class="fas fa-calculator"></i> Crash Course in Probability & MDP Notation</summary>
                <div class="details-content">
                    <p>A rigorous understanding of RL requires familiarity with the language of probability theory.</p>
                    <ul>
                        <li><strong>Probability Space:</strong> An MDP is defined over a probability space $(\Omega, \mathcal{F}, P)$, where $\Omega$ is the set of all possible outcomes (e.g., all possible state-action-reward trajectories), $\mathcal{F}$ is a <span class="tooltip" data-tooltip="A collection of subsets of Ω that is closed under complements and countable unions. It defines the set of events we can assign probabilities to.">$\sigma$-algebra</span> of events, and $P$ is a probability measure.</li>
                        <li><strong>Random Variables:</strong> Quantities like state $S_t$, action $A_t$, and reward $R_t$ are random variables mapping outcomes in $\Omega$ to real values.</li>
                        <li><strong>Conditional Independence:</strong> The Markov Property is a statement of conditional independence. We say random variable $X$ is conditionally independent of $Y$ given $Z$, written $X \perp Y | Z$, if $P(X|Y,Z) = P(X|Z)$. The Markov property for states is $S_{t+1} \perp \{S_0, \dots, S_{t-1}\} | S_t$. This means the current state $S_t$ is a <span class="tooltip" data-tooltip="A function of the data that captures all the relevant information about an unknown parameter. Here, the state is a sufficient statistic of the history.">sufficient statistic</span> of the history.</li>
                        <li><strong>Hidden Markov Models (HMMs):</strong> A POMDP (Partially Observable MDP) can be viewed as an MDP where the true state is hidden, forming an HMM. The agent receives an observation $O_t$ that is probabilistically related to the true state $S_t$.</li>
                    </ul>
                </div>
            </details>

          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-tools"></i>
            0.2 Interactive Environment
          </summary>
          <div class="details-content">
            <p>
              To bridge the gap between theory and practice, the code snippets in this guide are designed to be both readable and functional. While this document is static for security and performance, we provide one-click links to run the code in an interactive <span class="tooltip" data-tooltip="A free, cloud-based Jupyter notebook environment provided by Google.">Google Colab</span> environment.
            </p>
            
            <details class="deep-dive">
              <summary><i class="fas fa-server"></i> Deep Dive: Advanced Tooling for Reproducible Research</summary>
              <div class="details-content">
                  <p>For serious projects, moving beyond Colab to a robust local setup is crucial for reproducibility and performance.</p>
                  <ul>
                      <li><strong>Dependency Management:</strong> Tools like <a href="https://python-poetry.org/" target="_blank">Poetry</a> or <a href="https://docs.conda.io/en/latest/" target="_blank">Conda</a> manage complex dependencies and create isolated environments. Using a `conda-lock` or `poetry.lock` file ensures that anyone can replicate your exact package versions, which is critical as minor version changes in libraries like PyTorch or Gymnasium can alter agent behavior.</li>
                      <li><strong>Containerization:</strong> <a href="https://www.docker.com/" target="_blank">Docker</a> provides the highest level of reproducibility. A `Dockerfile` can package your entire application—code, dependencies, system libraries, and even CUDA versions—into a portable container. This is the industry standard for deploying machine learning models.</li>
                      <li><strong>GPU Profiling:</strong> To debug performance bottlenecks, use profilers like NVIDIA's <a href="https://developer.nvidia.com/nsight-systems" target="_blank">Nsight Systems</a> or the built-in <a href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html" target="_blank">PyTorch Profiler</a>. These tools can visualize exactly how much time is spent on data loading, GPU computation, and CPU overhead, helping you optimize your training loop.</li>
                  </ul>
              </div>
            </details>

            <h4>
              <i class="fas fa-download"></i>
              Required Libraries
            </h4>
            <p>
              The Colab environments are configured with the complete scientific Python stack. The first cell in each notebook will handle the setup, which typically includes:
            </p>
            <ul>
              <li><strong>PyTorch:</strong> The primary deep learning framework used.</li>
              <li><strong>NumPy/SciPy:</strong> For numerical and scientific computing.</li>
              <li><strong>Matplotlib/Seaborn:</strong> For generating visualizations.</li>
              <li><strong>Gymnasium:</strong> The standard toolkit for reinforcement learning environments, a community-maintained fork of OpenAI's Gym.</li>
            </ul>
            
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Example Setup Cell in Colab</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://colab.research.google.com/notebooks/intro.ipynb" target="_blank" class="colab-button" title="Run this code in Google Colab">
                            <i class="fas fa-play"></i> Run in Colab
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
# This code block would typically be the first cell in a Colab notebook.
# It handles the installation of required packages and sets up the environment.

# Install necessary libraries
!pip install -q torch numpy matplotlib gymnasium

# Import core libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym
import random
from collections import deque, namedtuple

# --- Environment Setup ---
print("🚀 Initializing environment...")

# Reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

print("✅ Environment Initialized!")
print(f"PyTorch Version: {torch.__version__}")
print(f"NumPy Version: {np.__version__}")
print(f"Gymnasium Version: {gym.__version__}")
                </code></pre>
            </div>
            <div class="admonition warning">
                <span class="admonition-title"><i class="fas fa-question-circle"></i>Open Questions</span>
                <p>The tooling for RL is constantly evolving. How might future hardware, such as neuromorphic chips or quantum computers, change the way we design and train RL agents? How will the rise of foundation models and large-scale pre-training affect the need for specialized RL environments?</p>
            </div>
          </div>
        </details>
        
        <div class="interactive-lab">
            <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Foundational Concepts</div>
            <p>Test your understanding of the core concepts introduced in this section.</p>
            <h5><i class="fas fa-question-circle"></i> Micro-Quiz: The Markov Property</h5>
            <p>Which of the following scenarios best satisfies the Markov property from the agent's perspective?</p>
            <ol>
              <li>An agent playing poker where the state is only its own two cards.</li>
              <li>An agent playing chess where the state is the complete board position.</li>
              <li>An agent controlling a robot arm where the state is just the arm's end-effector position (but not its velocity).</li>
            </ol>
            <details>
                <summary>Reveal Answer</summary>
                <div class="details-content">
                  <p><strong>Answer: 2.</strong> The complete chess board position contains all necessary information to determine the possible future moves and outcomes. How the pieces arrived at their current positions is irrelevant for future planning.
                  <br/>
                  <em>Why the others are wrong:</em> In poker (1), the history of betting and folded cards (which are not part of the state) provides crucial information. For the robot arm (3), the velocity is needed to predict its future position, so position alone is not a Markov state.
                  </p>
                </div>
            </details>
        </div>

      </section>
      
      <hr/>
<section id="part1">
        <h2 id="part1-title">
          <i class="fas fa-chess-board"></i>
          Part 1: The Bedrock of Reinforcement Learning
        </h2>
        
        <details open class="fade-in">
            <summary>
              <i class="fas fa-gamepad"></i>
              1.1 The RL Paradigm: Learning from Interaction
            </summary>
            <div class="details-content">
                <p>
                    At its most fundamental level, Reinforcement Learning (RL) is a computational approach to learning goal-directed behavior through <strong>interaction</strong>. It is a paradigm of learning from the consequences of actions, rather than from explicit instruction. It stands as one of the three primary pillars of machine learning, but its philosophy is distinct, focusing on an <strong class="tooltip" data-tooltip="The learner and decision-maker.">agent</strong> situated within an <strong class="tooltip" data-tooltip="Everything outside the agent.">environment</strong>, learning to make optimal decisions over time to maximize a cumulative reward.
                </p>
        
                <h4>
                  A Comparison of Machine Learning Paradigms
                </h4>
                <div class="admonition info">
                    <span class="admonition-title">
                      <i class="fas fa-book-reader"></i>
                      A Learning Analogy
                    </span>
                    <p>Imagine three ways a student might learn to play a video game:</p>
                    <ul>
                        <li><strong>Supervised Learning:</strong> The student watches millions of hours of expert gameplay videos. Their goal is to mimic the expert's actions perfectly in any given situation. The data is labeled (situation -> correct action), and the feedback is immediate and direct ("You zigged, but the expert zagged."). This is learning by imitation.
                            <ul><li><em>Real World Example:</em> Training a Convolutional Neural Network (CNN) to classify images of cats and dogs from a labeled dataset.</li></ul>
                        </li>
                        <li><strong>Unsupervised Learning:</strong> The student is given all the game's assets (character models, maps, sounds) and told to find interesting patterns. They might cluster characters into "heroes" and "villains" or identify that certain sounds often precede danger. They learn the <em>structure</em> of the game's world, but not how to <em>act</em> in it. This is learning by observation.
                            <ul><li><em>Real World Example:</em> Using k-means clustering to segment customers into different purchasing groups based on unlabeled sales data.</li></ul>
                        </li>
                        <li><strong>Reinforcement Learning:</strong> The student is given a controller and put directly into the game. They aren't told what to do. They press buttons (actions), see the game change (next state), and watch their score go up or down (reward). Their goal is to figure out for themselves which sequence of actions leads to the highest final score. This is learning by trial, error, and evaluative feedback.
                            <ul><li><em>Real World Example:</em> Training an agent to play chess, where the only feedback is winning (+1) or losing (-1) at the end of the game.</li></ul>
                        </li>
                    </ul>
                </div>
                
                <h4>
                  The Agent-Environment Interaction Loop
                </h4>
                <p>
                    The interaction between agent and environment is a continuous feedback loop. This loop is the fundamental data-generating process in all of RL. Let's break down a single step with precision:
                </p>
        
<!-- Syntax-correct & polished diagram -->
<div class="visualization-container">
  <div class="vis-toolbar">
    <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
    <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
    <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
  </div>
  <div class="mermaid">
    graph TD
      A["🤖 Agent"] --> E["🌍 Environment"]

      subgraph AGENT
        P["Policy π(a|s)"] --> I["Action A_t"]
        V["Value Func V(s)"] --> P
        L["Learning Algorithm"] --> P
        L --> V
      end

      subgraph ENV
        D["State S_t"]
        T["Transition P(s'|s,a)"]
        R["Reward R(s,a)"]
      end

      D --> P
      I --> T
      T --> NS["Next State S_{t+1}"]
      T --> RW["Reward R_{t+1}"]

      NS --> D
      subgraph EXP
        D_t["s_t"]
        A_t["a_t"]
        R_t1["r_{t+1}"]
        S_t1["s_{t+1}"]
      end
      EXP --> L

      classDef agent fill:#00f7ff,stroke:#e0e1ff,stroke-width:2px,color:#101222
      classDef env   fill:#f000ff,stroke:#e0e1ff,stroke-width:2px,color:#101222
      classDef flow  fill:#161930,stroke:#4c59ba,stroke-width:1px,color:#e0e1ff
  </div>
</div>
                <div class="admonition tip">
                    <span class="admonition-title"><i class="fas fa-atom"></i>Anatomy of an Experience Tuple</span>
                    <p>The fundamental unit of data in RL is the experience tuple $(S_t, A_t, R_{t+1}, S_{t+1})$. This is what the agent's learning algorithm ingests.
                    <ul>
                        <li>$S_t$: The state of the environment at the current time $t$.</li>
                        <li>$A_t$: The action the agent chose to take in state $S_t$.</li>
                        <li>$R_{t+1}$: The immediate reward received after taking action $A_t$.</li>
                        <li>$S_{t+1}$: The new state the environment transitioned to as a result of action $A_t$.</li>
                    </ul>
                    This tuple encapsulates a single cause-and-effect event that the agent uses to learn about the world.
                    </p>
                </div>

                <h4>
                  The Temporal Credit Assignment Problem
                </h4>
                <p>
                    The central challenge RL is designed to solve is the <strong>temporal credit assignment problem</strong>. If a sequence of a hundred actions in a chess game leads to a final victory, which of those actions were critical? A brilliant queen sacrifice on move 20 might be the true cause of a win on move 50, but the immediate feedback is negative (loss of a powerful piece). The goal is to properly attribute the final return, $G_t$, to the sequence of actions $(A_t, A_{t+1}, \dots)$ that produced it. Value functions, which we will see in <a href="#part2">Part 2</a>, are the primary mechanism for solving this.
                </p>
                
                <h4>
                  Key Challenges in Reinforcement Learning
                </h4>
                <p>While the interaction loop seems simple, several fundamental challenges make RL a difficult problem:</p>
                <ul>
                    <li><strong>The Exploration-Exploitation Trade-off:</strong> How does an agent balance exploiting its current knowledge to get high rewards with exploring the environment to possibly find even better strategies? We can formalize this with the concept of <strong>regret</strong>: the difference between the agent's total reward and the total reward it <em>would have</em> received if it had acted optimally from the start. Good exploration strategies aim to minimize this regret over time.</li>
                    <li><strong>The Curse of Dimensionality:</strong> Real-world problems have enormous state and action spaces. A game of Go has more states than atoms in the universe. This necessitates the use of function approximation, the core topic of <a href="#part5">Part 5</a>.</li>
                    <li><strong>Reward Function Design:</strong> The entire behavior of the agent is driven by the reward signal. Designing a reward function that elicits the desired behavior without creating unintended loopholes (reward hacking) is a major practical challenge. A cleaning robot rewarded for "amount of dirt collected" might learn to dump its bin and re-collect the same dirt repeatedly.</li>
                    <li><strong>Sample Inefficiency:</strong> RL agents often require millions or billions of interactions with the environment to learn a good policy, a key motivation for developing more advanced, data-efficient algorithms.</li>
                </ul>

                <h4>
                  Exploration Strategies: Multi-Armed Bandits
                </h4>
                <p>
                  The exploration-exploitation trade-off can be studied in its purest form in the <strong>Multi-Armed Bandit</strong> problem. Here, an agent must choose between several slot machines ("arms") with unknown payout probabilities to maximize its total reward. The code below compares the simple $\epsilon$-greedy strategy with a more advanced method, the <strong>Upper Confidence Bound (UCB)</strong> algorithm, which makes more intelligent exploration decisions by balancing an arm's estimated value with its uncertainty.
                </p>
                
                <div class="code-container">
                    <div class="code-block-header">
                        <span><i class="fab fa-python"></i> Python: Bandit Algorithms & Regret</span>
                        <div class="buttons">
                            <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                            <a href="https://github.com/rl-omnibus/bandit-simulations" target="_blank" class="colab-button" title="View on GitHub">
                                <i class="fab fa-github"></i> View on GitHub
                            </a>
                        </div>
                    </div>
                    <pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

class Bandit:
    def __init__(self, n_arms=10):
        self.n_arms = n_arms
        self.true_means = np.random.normal(0, 1, n_arms)
        self.optimal_arm = np.argmax(self.true_means)
        self.optimal_value = np.max(self.true_means)

    def pull(self, arm):
        return np.random.normal(self.true_means[arm], 1)

def run_bandit_simulation(agent_class, n_steps=1000, n_runs=100, **kwargs):
    all_rewards = np.zeros((n_runs, n_steps))
    all_regrets = np.zeros((n_runs, n_steps))

    for i in range(n_runs):
        bandit = Bandit()
        agent = agent_class(n_arms=bandit.n_arms, **kwargs)
        for t in range(n_steps):
            action = agent.select_action()
            reward = bandit.pull(action)
            agent.update(action, reward)
            all_rewards[i, t] = reward
            all_regrets[i, t] = bandit.optimal_value - reward
            
    return np.mean(all_rewards, axis=0), np.mean(np.cumsum(all_regrets, axis=1), axis=0)

class EpsilonGreedyAgent:
    def __init__(self, n_arms, epsilon=0.1):
        self.n_arms = n_arms
        self.epsilon = epsilon
        self.estimates = np.zeros(n_arms)
        self.counts = np.zeros(n_arms)

    def select_action(self):
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_arms)
        else:
            return np.argmax(self.estimates)

    def update(self, action, reward):
        self.counts[action] += 1
        self.estimates[action] += (1 / self.counts[action]) * (reward - self.estimates[action])

class UCBAgent:
    def __init__(self, n_arms, c=2):
        self.n_arms = n_arms
        self.c = c
        self.estimates = np.zeros(n_arms)
        self.counts = np.zeros(n_arms)
        self.t = 0

    def select_action(self):
        self.t += 1
        # Play each arm once initially
        if self.t <= self.n_arms:
            return self.t - 1
        
        # UCB calculation: Q(a) + c * sqrt(log(t) / N(a))
        ucb_values = self.estimates + self.c * np.sqrt(np.log(self.t) / self.counts)
        return np.argmax(ucb_values)

    def update(self, action, reward):
        self.counts[action] += 1
        self.estimates[action] += (1 / self.counts[action]) * (reward - self.estimates[action])

# --- Run Simulations ---
avg_rewards_eps, regret_eps = run_bandit_simulation(EpsilonGreedyAgent, epsilon=0.1)
avg_rewards_ucb, regret_ucb = run_bandit_simulation(UCBAgent, c=2)

# --- Plotting ---
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
ax1.plot(np.cumsum(avg_rewards_eps) / np.arange(1, 1001), label="Epsilon-Greedy (ε=0.1)")
ax1.plot(np.cumsum(avg_rewards_ucb) / np.arange(1, 1001), label="UCB (c=2)")
ax1.set_ylabel("Average Reward")
ax1.set_title("Bandit Algorithm Performance (Averaged over 100 runs)")
ax1.legend()

ax2.plot(regret_eps, label="Epsilon-Greedy (ε=0.1)")
ax2.plot(regret_ucb, label="UCB (c=2)")
ax2.set_xlabel("Steps")
ax2.set_ylabel("Cumulative Regret")
ax2.set_title("Cumulative Regret vs. Time")
ax2.legend()
plt.show()
                </code></pre>
            </div>
          </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-project-diagram"></i>
              1.2 The Mathematical Framework: Markov Decision Processes (MDPs)
            </summary>
            <div class="details-content">
                <p>
                    To move from the high-level paradigm to concrete algorithms, we must formalize the problem of sequential decision-making under uncertainty. The universal mathematical framework for this in reinforcement learning is the <strong>Markov Decision Process (MDP)</strong>. An MDP provides a formal language to describe the interaction between a decision-making agent and its environment, grounded in the theory of probability.
                </p>
                
                <h4>
                  1.2.1 The Markov Property: "The Future is Independent of the Past Given the Present"
                </h4>
                <p>
                    The entire framework of MDPs hinges on a single, critical simplifying assumption: the <strong>Markov Property</strong>. A state signal $S_t$ is said to be Markov if and only if it contains all the information from the past that is relevant for predicting the future. Once the current state is known, the full history of how the agent arrived in that state can be discarded without losing any predictive power.
                </p>
                
                <div class="admonition tip">
                    <span class="admonition-title"><i class="fas fa-calculator"></i>Math Breakdown: The Markov Property</span>
                    <p>Mathematically, a state $S_t$ is Markov if the probability of the next state and reward is conditionally independent of the past history, given the current state and action.
                    $$P(S_{t+1}=s', R_{t+1}=r | S_t, A_t) = P(S_{t+1}=s', R_{t+1}=r | S_t, A_t, R_t, S_{t-1}, A_{t-1}, \dots, S_0, A_0)$$
                    Let's unpack this:
                    <ul>
                        <li><strong>Left Side: $P(S_{t+1}=s', R_{t+1}=r | S_t, A_t)$</strong><br/> This is the probability of transitioning to a specific next state $s'$ and receiving reward $r$, given that you are currently in state $S_t$ and take action $A_t$. It depends <em>only</em> on the present.</li>
                        <li><strong>Right Side: $P(\dots | S_t, A_t, R_t, S_{t-1}, \dots)$</strong><br/> This is the probability of the same event, but given the <em>entire history</em> of states, actions, and rewards.</li>
                    </ul>
                    The Markov Property states that these two probabilities are equal. The history ($R_t, S_{t-1}, \dots$) provides no extra information for predicting the future if you already know $S_t$ and $A_t$. The state $S_t$ is a <span class="tooltip" data-tooltip="In statistics, a sufficient statistic is a function of the data that contains all the information needed for any inference about a parameter. Here, the state is a sufficient statistic of the history.">sufficient statistic</span> of the past.
                    </p>
                </div>

                <div class="admonition danger">
                    <span class="admonition-title">
                      <i class="fas fa-exclamation-triangle"></i>
                      A Property of the Representation, Not the World
                    </span>
                    <p>It is a common misconception to think of the Markov property as a property of the <em>environment</em>. In reality, almost no real-world environment is truly Markovian. The Markov property is a feature of the <strong>state representation</strong> that we, as designers, provide to the agent. The core challenge of state design is to craft a representation $S_t$ that is a sufficient statistic of the history, thereby making the problem Markovian from the agent's perspective.</p>
                    <ul>
                        <li><strong>Good Representation (Chess):</strong> The current board position is a Markov state. It perfectly captures everything needed to play optimally. How the pieces got to their current squares is irrelevant for future planning.</li>
                        <li><strong>Bad Representation (Driving):</strong> If the state is only the car's <em>position</em>, it's not Markov. To predict the future position, you also need to know the car's current <em>velocity</em> and <em>acceleration</em>. A proper Markov state for a car would be a vector like $[position, velocity, acceleration]$.</li>
                        <li><strong>Deep RL Solution:</strong> For complex tasks like Atari games, a single frame is not Markov (you can't tell a ball's velocity). DeepMind's solution was to stack the last 4 frames together to form the state, allowing the network to infer dynamics.</li>
                    </ul>
                </div>

                <h4>
                  1.2.2 Beyond Markov: Partially Observable MDPs (POMDPs)
                </h4>
                <p>
                    What happens when the agent cannot observe the full state? This is known as <strong>partial observability</strong>, and it is the norm in the real world. Such problems are modeled as <strong>Partially Observable Markov Decision Processes (POMDPs)</strong>. In a POMDP, the agent maintains a <strong>belief state</strong>, $b(s_t)$, which is a probability distribution over all possible environment states, conditioned on its history of actions and observations. This belief state is updated using Bayesian filtering. The key insight is that this belief state <em>is</em> Markovian, allowing the problem to be reformulated as a (much harder) MDP over the space of beliefs.
                </p>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-list-ol"></i>
              1.3 The MDP Tuple: (S, A, P, R, γ) - A Rigorous Unpacking
            </summary>
            <div class="details-content">
                <p>
                    An MDP is formally defined by a 5-element tuple. Mastering these components is essential to understanding any RL algorithm. Let's make this concrete with a simple <strong>Grid World</strong> example: a 3x4 grid where an agent (🤖) tries to reach a goal (🏆) while avoiding a pitfall (🔥). There's also a wall (🧱) the agent cannot pass through.
                </p>
                
                <table>
                    <thead>
                        <tr>
                          <th>Component</th>
                          <th>Symbol</th>
                          <th>Description</th>
                          <th>Grid World Example</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>State Space</strong></td>
                            <td>$S$</td>
                            <td>The set of all possible states the agent can be in. Must be Markov.</td>
                            <td>The set of all non-wall grid coordinates: $S = \{(0,0), \dots, (2,3)\} \setminus \{(1,1)\}$. $|S|=11$.</td>
                        </tr>
                        <tr>
                            <td><strong>Action Space</strong></td>
                            <td>$A$</td>
                            <td>The set of all possible actions the agent can take. Can be discrete or continuous.</td>
                            <td>$A = \{\text{up, down, left, right}\}$. $|A|=4$.</td>
                        </tr>
                        <tr>
                            <td><strong>Transition Model</strong></td>
                            <td>$P(s' | s, a)$</td>
                            <td>The probability of transitioning to state $s'$ after taking action $a$ in state $s$. Defines the environment's dynamics.</td>
                            <td>If the world is "slippery", from $s=(0,1)$ taking $a=\text{right}$: $P((0,2)|s,a)=0.8$ (intended), $P((0,0)|s,a)=0.1$ (slip left), $P((1,1)-\text{wall}|s,a)=0.1$ (slip down). Agent stays at (0,1).</td>
                        </tr>
                        <tr>
                            <td><strong>Reward Function</strong></td>
                            <td>$R(s, a, s')$</td>
                            <td>The immediate scalar reward for a transition. Defines the agent's goal.</td>
                            <td>$R = +1$ for entering the goal 🏆.<br>$R = -1$ for entering the pitfall 🔥.<br>$R = -0.04$ for all other moves (a small cost for time to encourage efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>Discount Factor</strong></td>
                            <td>$\gamma$</td>
                            <td>A value between $[0, 1)$ that determines the present value of future rewards. A key lever for controlling agent myopia.</td>
                            <td>$\gamma = 0.99$. A reward of 1 in 10 steps is worth $1 \times 0.99^9 \approx 0.91$. This makes the agent prefer shorter paths. If $\gamma=0$, the agent is myopic and only cares about the immediate reward.</td>
                        </tr>
                    </tbody>
                </table>
        
                <div class="admonition tip">
                    <span class="admonition-title">
                      <i class="fas fa-check-circle"></i>
                      Theorem: Existence of an Optimal Policy
                    </span>
                    <p>For any finite Markov Decision Process, a foundational theorem in dynamic programming gives us confidence that our search is not futile:
                    <br><strong>There exists at least one policy $\pi^*$ that is optimal, meaning it achieves the highest possible value from every state, $V^*(s) \ge V^\pi(s)$ for all $s \in S$ and all policies $\pi$.</strong>
                    <br>Furthermore, at least one of these optimal policies is guaranteed to be <strong>deterministic</strong>. The proof relies on showing that the Bellman optimality operator is a contraction mapping, which guarantees a unique fixed point (the optimal value function), from which an optimal policy can be derived.
                    </p>
                </div>
            </div>
        </details>

        <!-- Removed misplaced interactive multi-armed bandit playground from Part 1. It is now housed in Part 4. -->

      </section>
      <hr/>

<section id="part2">
        <h2 id="part2-title">
          <i class="fas fa-bullseye"></i>
          Part 2: The Objective: Policies, Returns, and Value Functions
        </h2>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-chart-line"></i>
            2.1 The Return (G_t): Defining the Goal
          </summary>
          <div class="details-content">
            <p>
              Now that we have established the MDP as our mathematical playground, we can precisely define what the agent is trying to achieve. The agent's goal is not merely to get a high reward on the next step, but to select actions that will lead to the greatest possible cumulative reward over its lifetime. This long-term objective is formalized as the <strong>Return</strong>.
            </p>

            <h4>
              Episodic vs. Continuing Tasks
            </h4>
            <p>Before defining the return, we must distinguish between two fundamental types of tasks in RL:</p>
            
            <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                    <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                    <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
                </div>
                <div class="mermaid">
                  graph LR
                    A[Task Types] --> B[Episodic];
                    A --> C[Continuing];
                    
                    B --> B1["Definite starting and ending points"];
                    B1 --> B2["Example: A game of chess, navigating a maze"];
                    
                    C --> C1["No natural end point; goes on indefinitely"];
                    C1 --> C2["Example: A stock-trading bot, climate control"];
                    
                    classDef typeNode fill:#161930,stroke:#4c59ba,stroke-width:2px,color:#e0e1ff;
                    class A typeNode;
                </div>
            </div>

            <ul>
              <li><strong>Episodic Tasks:</strong> These tasks have a natural end point, called a <strong class="tooltip" data-tooltip="A state that ends the episode, such as checkmate in chess or reaching the goal in a maze.">terminal state</strong>. Each run of the task, from a start state to a terminal state, is called an "episode." For these tasks, the sequence of rewards is finite.</li>
              <li><strong>Continuing Tasks:</strong> These tasks have no terminal state and could, in principle, go on forever. For these tasks, the simple sum of rewards could diverge to infinity. The discount factor, $\gamma$, becomes mathematically essential to ensure the return is a finite, well-defined quantity that our algorithms can optimize.</li>
            </ul>

            <h4>
              Defining the Discounted Return
            </h4>
            <p>
              The <strong>Return</strong> at time $t$, denoted $G_t$, is the total discounted reward from time $t$ onward. It's defined as the sum of all future rewards, each discounted by how far in the future it is received.
            </p>
            $$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
            
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-calculator"></i>Math Breakdown: The Return</span>
                <p>Let's dissect the equation for the return, $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$:</p>
                <ul>
                    <li>$G_t$: This is the <strong>Return</strong>, the quantity we want to maximize. It represents the total value of all future rewards as seen from time step $t$.</li>
                    <li>$\sum_{k=0}^{\infty}$: This is a sum over all future time steps, starting from the immediate next step ($k=0$) and going on forever.</li>
                    <li>$R_{t+k+1}$: This is the reward received at the future time step $t+k+1$. When $k=0$, it's the immediate reward $R_{t+1}$. When $k=1$, it's the reward at $t+2$, and so on.</li>
                    <li>$\gamma^k$: This is the <strong>discount factor</strong> $\gamma$ raised to the power of $k$. Since $\gamma$ is between 0 and 1, this term gets smaller as $k$ gets larger. It's what makes rewards further in the future less valuable than immediate rewards.</li>
                </ul>
            </div>
            
            <p>The discount factor $\gamma \in [0, 1)$ has two key interpretations:</p>
            <ol>
                <li><strong>Economic Analogy (Time Value of Money):</strong> A dollar today is worth more than a dollar tomorrow. Similarly, a reward received now is more valuable than the same reward received in the future. $\gamma$ acts like an interest rate, controlling this preference. A smaller $\gamma$ (e.g., 0.9) makes the agent "impatient" or myopic, while a larger $\gamma$ (e.g., 0.999) makes it more "patient" and farsighted.</li>
                <li><strong>Mathematical Convenience:</strong> It ensures that for continuing tasks, the infinite sum of rewards converges to a finite value as long as the rewards are bounded. Without it, the return could be infinite, making it impossible to compare different strategies.</li>
            </ol>
            
            <div class="admonition tip">
              <span class="admonition-title">
                <i class="fas fa-recycle"></i>
                The Recursive Nature of the Return
              </span>
              <p>The definition of the return has a powerful recursive property that is the foundation of the Bellman equations we will see in <a href="#part3">Part 3</a>. We can express the return at time $t$ in terms of the return at time $t+1$:</p>
              $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$$
              $$G_t = R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \dots)$$
              $$G_t = R_{t+1} + \gamma G_{t+1}$$
              <p>This simple relationship is profound. It shows that the total future reward is simply the immediate reward plus the discounted total future reward from the next state. This allows us to break a large, complex problem into smaller, self-similar subproblems, which is the core idea of dynamic programming.</p>
            </div>
            
            <h4>
              Visualizing the Discount Factor
            </h4>
            <p>
              The code below calculates the discounted return for each step in a trajectory and visualizes how the present value of a future reward decays based on different values of $\gamma$.
            </p>
            
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Calculating Discounted Returns</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i></button>
                        <a href="https://github.com/rl-omnibus/return-calculator" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

def calculate_returns_from_trajectory(rewards: list[float], gamma: float) -> np.ndarray:
    """
    Calculates the discounted return G_t for each time step t in a trajectory.
    """
    returns = np.zeros_like(rewards, dtype=float)
    future_return = 0.0
    for t in reversed(range(len(rewards))):
        future_return = rewards[t] + gamma * future_return
        returns[t] = future_return
    return returns

# --- Visualize the effect of the discount factor ---
fig, ax = plt.subplots(figsize=(10, 6))
future_reward = 100
steps_in_future = np.arange(1, 21)
gammas = [0.8, 0.9, 0.95, 0.99, 1.0]

for gamma in gammas:
    present_values = future_reward * (gamma ** (steps_in_future - 1))
    label = f'γ = {gamma}'
    if gamma == 1.0:
        label += ' (undiscounted)'
    ax.plot(steps_in_future, present_values, 'o-', label=label)

ax.set_title("Present Value of a Future Reward of 100")
ax.set_xlabel("Steps into the Future")
ax.set_ylabel("Present Value")
ax.legend()
ax.grid(True, alpha=0.3)
plt.show()

# --- Example Calculation ---
rewards = [-1, -1, -1, 10]
gamma = 0.99
returns = calculate_returns_from_trajectory(rewards, gamma)
print("\n=== Return Calculation Example ===")
print(f"Rewards:     {rewards}")
print(f"Gamma:       {gamma}")
print(f"Returns (G_t): {[f'{r:.2f}' for r in returns]}")
                </code></pre>
            </div>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-brain"></i>
            2.2 The Policy (π): The Agent's "Brain"
          </summary>
          <div class="details-content">
              <p>
                  The agent's behavior—its decision-making process—is defined by its <strong>policy</strong>, denoted by the Greek letter $\pi$. A policy is a mapping from states to a selection of actions. It is the core of the RL agent, representing its strategy at a given moment.
              </p>
              <p>Policies can be categorized into two types:</p>
              <ul>
                  <li><strong>Deterministic Policy</strong>: For any given state, a deterministic policy always outputs the <em>same</em> action. It is a direct mapping: $a = \pi(s)$. This is like a chess engine that, given the same board, will always make the same move.</li>
                  <li><strong>Stochastic Policy</strong>: A stochastic policy outputs a <em>probability distribution</em> over the set of available actions for a given state: $\pi(a|s) \doteq P(A_t = a | S_t = s)$. This is like a rock-paper-scissors player who plays rock 40% of the time, paper 30%, and scissors 30%. A deterministic strategy in that game would be easily beaten. Stochastic policies are crucial for exploration and for finding optimal solutions in partially observable environments.</li>
              </ul>
              <p>
                  The ultimate objective of a Reinforcement Learning agent is to find an <strong>optimal policy</strong>, denoted $\pi^*$. An optimal policy is one that achieves the highest possible expected return from any starting state.
              </p>

              <h4>
                Representing a Policy
              </h4>
              <p>How do we store a policy in code? The representation depends on the complexity of the problem.</p>
              
              <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                    <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                    <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
                </div>
                <div class="mermaid">
                  graph TD
                      subgraph Tabular["Small State Space"]
                          direction LR
                          State_T[State ID] --> Table["Lookup Table / Dict"]
                          Table --> Action_T[Action or Probs]
                      end
                      
                      subgraph Parametric["Large/Continuous State Space"]
                          direction LR
                          State_P[State Features] --> NN["Neural Network π(s; θ)"]
                          NN --> Action_P[Action or Probs]
                      end
                      classDef tabular fill:#00f7ff,color:#101222,stroke:#a0a3d4
                      classDef parametric fill:#f000ff,color:#101222,stroke:#a0a3d4
                      class State_T,Table,Action_T tabular;
                      class State_P,NN,Action_P parametric;
                </div>
              </div>
              
              <ul>
                <li><strong>Tabular Policies:</strong> For small, discrete state spaces, a policy can be a simple lookup table (e.g., a dictionary or an array). This is simple and exact but fails completely when the number of states is large.</li>
                <li><strong>Parameterized Policies:</strong> For large or continuous state spaces, storing a table is impossible. Instead, we use a function approximator with parameters $\theta$, written as $\pi_\theta(a|s)$. This function takes a state as input and outputs the action (or action probabilities). In Deep RL, this is typically a <strong>neural network</strong>. The key advantage is generalization: the network can learn a sensible policy for states it has never seen before based on their features.</li>
              </ul>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale"></i>
            2.3 Value Functions: Judging the Future
          </summary>
          <div class="details-content">
              <p>
                  A policy tells the agent how to act. But to improve its policy, the agent needs a way to <em>evaluate</em> it. This is the role of <strong>value functions</strong>. Value functions are state-dependent (or state-action dependent) functions that estimate the expected return, providing a quantitative measure of "how good" a situation is.
              </p>
              
              <h4>
                State-Value Function ($V^\pi(s)$)
              </h4>
              <p>The <strong>state-value function</strong>, $V^\pi(s)$, answers the question: <em>"What is the expected total future reward I will get if I am in state $s$ and follow my policy $\pi$ from this point onward?"</em></p>
              <p>It is formally defined as the expected return, given that the agent starts in state $s$ and follows policy $\pi$:</p>
              $$V^\pi(s) \doteq \mathbb{E}_\pi [G_t | S_t = s] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s \right]$$
              <p>The expectation $\mathbb{E}_\pi[\cdot]$ averages over all possible random trajectories that can unfold, due to both the agent's stochastic policy and the environment's stochastic dynamics.</p>
              
              <h4>
                Action-Value Function ($Q^\pi(s, a)$)
              </h4>
              <p>The <strong>action-value function</strong>, $Q^\pi(s, a)$, answers a slightly more specific question: <em>"What is the expected total future reward I will get if I am in state $s$, I decide to take action $a$, and <em>then</em> follow my policy $\pi$ from that point onward?"</em></p>
              <p>It is formally defined as the expected return after taking action $a$ in state $s$ and subsequently following policy $\pi$:</p>
              $$Q^\pi(s, a) \doteq \mathbb{E}_\pi [G_t | S_t = s, A_t = a] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s, A_t = a \right]$$

              <div class="admonition danger">
                  <span class="admonition-title">
                    <i class="fas fa-question-circle"></i>
                    The Key Difference: Why Q-Functions are Crucial for Control
                  </span>
                  <p>It might seem redundant to have two types of value functions, but they serve different purposes, especially when the agent doesn't have a model of the environment's dynamics ($P$ and $R$). This is known as <strong>model-free</strong> learning.</p>
                  <ul>
                      <li>The <strong>State-Value Function $V(s)$</strong> tells you the long-term value of being in a state. It's great for analysis, but to act on it, you need a model. To choose an action, you'd have to perform a one-step lookahead for every possible action $a$, calculate $\sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V(s')]$, and then pick the action that gives the best result. <strong>This requires knowing the transition probabilities $P$ and rewards $R$.</strong></li>
                      <li>The <strong>Action-Value Function $Q(s, a)$</strong> tells you the long-term value of taking a specific action in a specific state. This is much more direct for control. To choose the best action, you simply find the action with the highest Q-value: $\arg\max_a Q(s, a)$. <strong>You don't need to know anything about the environment's dynamics to make an optimal decision.</strong> This is why model-free algorithms like Q-Learning learn Q-values directly. They implicitly contain all the information needed for action selection.</li>
                  </ul>
              </div>
              
              <h4>
                Visualizing Value Functions
              </h4>
              <p>
                Let's visualize how value functions might look for our Grid World example. High values should appear near the goal, and low values near the pitfall. A good policy will have values that smoothly guide the agent from the start to the goal. The code below computes the value function for a given (suboptimal) policy.
              </p>
              
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Computing a Value Function</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i></button>
                        <a href="https://github.com/rl-omnibus/value-function-vis" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# This code uses the GridWorld class defined in Part 3.
# A simplified version is included here for this self-contained example.
class SimpleGridWorld:
    def __init__(self):
        self.shape = (3, 4)
        self.n_states = 12
        self.goal_state, self.pit_state, self.wall_state = 3, 7, 5
        self.terminal_states = [self.goal_state, self.pit_state]
    def step(self, s, a):
        # Simplified deterministic transitions for this example
        row, col = divmod(s, 4)
        if a == 0 and row > 0: next_s = s - 4
        elif a == 1 and row < 2: next_s = s + 4
        elif a == 2 and col > 0: next_s = s - 1
        elif a == 3 and col < 3: next_s = s + 1
        else: next_s = s
        if next_s == self.wall_state: next_s = s
        reward = -0.04
        if next_s == self.goal_state: reward = 1.0
        if next_s == self.pit_state: reward = -1.0
        return next_s, reward

def compute_value_function_for_policy(env, policy, gamma=0.99, theta=1e-6):
    V = np.zeros(env.n_states)
    while True:
        delta = 0
        for s in range(env.n_states):
            if s in env.terminal_states or s == env.wall_state: continue
            v_old = V[s]
            action = policy[s]
            next_s, reward = env.step(s, action)
            V[s] = reward + gamma * V[next_s]
            delta = max(delta, abs(v_old - V[s]))
        if delta < theta: break
    V[env.wall_state] = np.nan
    return V.reshape(env.shape)

# Policy 1: A purely random policy
random_policy = np.random.randint(0, 4, size=12)
# Policy 2: A somewhat smarter policy (tends to move right and towards goal)
smart_policy = np.array([3, 3, 3, 0, 0, 0, 3, 0, 0, 3, 3, 0])

env = SimpleGridWorld()
random_values = compute_value_function_for_policy(env, random_policy)
smart_values = compute_value_function_for_policy(env, smart_policy)

# Visualize the two value functions side-by-side
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
sns.heatmap(random_values, annot=True, fmt=".2f", cmap="viridis", cbar=False, ax=ax1, linewidths=.5)
ax1.set_title("Value Function of a Random Policy")
sns.heatmap(smart_values, annot=True, fmt=".2f", cmap="viridis", cbar=False, ax=ax2, linewidths=.5)
ax2.set_title("Value Function of a 'Smarter' Policy")
fig.tight_layout()
plt.show()
                </code></pre>
            </div>
          </div>
        </details>
      </section>
      <hr/>
<section id="part3">
        <h2 id="part3-title">
          <i class="fas fa-infinity"></i>
          Part 3: The Bellman Equations: Unpacking the Recursion
        </h2>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-calculator"></i>
            3.1 The Bellman Expectation Equation: What a Policy is Worth
          </summary>
          <div class="details-content">
              <p>
                  The value functions we have defined possess a beautiful, recursive structure. This structure was formalized by the American mathematician Richard Bellman in the 1950s, and the resulting equations are the absolute cornerstone of most reinforcement learning algorithms. The <strong>Bellman Equations</strong> express a fundamental relationship between the value of a state and the values of its successor states. They decompose the value of a state (a long-term measure) into the sum of the immediate reward and the discounted value of the next state, providing a framework for both evaluation and optimization in RL.
              </p>
              
              <h4>
                Derivation for the State-Value Function $V^\pi(s)$
              </h4>
              <p>The derivation links the definition of $V^\pi(s)$ with the recursive nature of the return $G_t$ that we established in <a href="#part2">Part 2</a>. It's a straightforward but powerful sequence of substitutions that reveals the self-consistent nature of value functions under a policy.</p>
              <ol>
                <li>Start with the definition of the value function: $$V^\pi(s) \doteq \mathbb{E}_\pi [G_t | S_t = s]$$</li>
                <li>Substitute the recursive definition of the return, $G_t = R_{t+1} + \gamma G_{t+1}$: $$V^\pi(s) = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s]$$</li>
                <li>Use the linearity of expectation, which allows us to split the expectation of a sum into a sum of expectations: $$V^\pi(s) = \mathbb{E}_\pi [R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s]$$</li>
                <li>Recognize that the second term, $\mathbb{E}_\pi [G_{t+1} | S_t = s]$, is just the expected value of the *next* state, $V^\pi(S_{t+1})$. This brings us to the final, compact form of the <strong>Bellman Expectation Equation</strong>:</li>
              </ol>
              $$V^\pi(s) = \mathbb{E}_\pi [R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s]$$
              <p>To make this computationally concrete for an environment with known dynamics, we expand the expectation over all possibilities dictated by the policy $\pi$ and the environment's transition model $P$.</p>
              $$V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left( r + \gamma V^\pi(s') \right)$$
              
              <div class="admonition info">
                <span class="admonition-title">
                  <i class="fas fa-lightbulb"></i>
                  Intuition Behind the Bellman Expectation Equation
                </span>
                <p>The equation tells us that the value of a state under a policy is the expected immediate reward plus the discounted expected value of whatever state we land in next, averaged over the actions the policy chooses and the environment's stochastic outcomes. It’s a recursive breakdown that turns a long-term problem into a series of one-step problems.</p>
              </div>
              
              <h4>
                Derivation for the Action-Value Function $Q^\pi(s,a)$
              </h4>
              <p>Similarly, for $Q^\pi(s,a)$, the value of taking action $a$ in state $s$ under policy $\pi$, we derive:</p>
              $$Q^\pi(s,a) = \mathbb{E}_\pi [R_{t+1} + \gamma Q^\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$$
              <p>Expanding with environment dynamics:</p>
              $$Q^\pi(s,a) = \sum_{s', r} p(s', r | s, a) \left( r + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a') \right)$$
              <p>This connects immediate rewards to future action-values under the policy, crucial for methods like SARSA.</p>

              <h4>
                Iterative Policy Evaluation
              </h4>
              <p>
                The Bellman expectation equation provides a system of equations, one for each state. For small state spaces, we could solve this system directly via matrix inversion (V = (I - γPπ)^(-1)R). However, a more general and computationally scalable approach is an iterative method called <strong>Iterative Policy Evaluation</strong>. We start with arbitrary value estimates (e.g., all zeros) and repeatedly sweep through the state space, applying the Bellman update to each state until the values converge. Convergence is guaranteed as the Bellman operator is a contraction mapping with factor γ < 1.
              </p>
              
              <details>
                <summary><i class="fas fa-calculator"></i> Proof of Convergence for Policy Evaluation</summary>
                <div class="details-content">
                  <p>The update $V_{k+1}(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V_k(s')]$ defines an operator Tπ(V) = Rπ + γPπV. Since ||Tπ(V1) - Tπ(V2)||_∞ ≤ γ||V1 - V2||_∞, Tπ is a contraction in the sup norm with modulus γ < 1. By the Banach fixed-point theorem, iteration converges to the unique solution Vπ at rate O(γ^k).</p>
                </div>
              </details>
              
              <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Iterative Policy Evaluation</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://github.com/rl-omnibus/bellman-eval" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

# A simple GridWorld Environment for demonstration
class GridWorld:
    def __init__(self):
        self.shape = (3, 4)
        self.n_states = self.shape[0] * self.shape[1]
        self.goal_state = 3
        self.pit_state = 7
        self.wall_state = 5
        self.bad_states = [self.goal_state, self.pit_state, self.wall_state]
        self.n_actions = 4  # 0:Up, 1:Down, 2:Left, 3:Right

    def step(self, s, a):
        # Simplified deterministic transitions
        row, col = divmod(s, self.shape[1])
        if a == 0: row = max(0, row - 1)
        elif a == 1: row = min(self.shape[0] - 1, row + 1)
        elif a == 2: col = max(0, col - 1)
        elif a == 3: col = min(self.shape[1] - 1, col + 1)
        
        next_s = row * self.shape[1] + col
        if next_s == self.wall_state:
            next_s = s  # Bump into wall
            
        # Define rewards
        if next_s == self.goal_state: reward = 1.0
        elif next_s == self.pit_state: reward = -1.0
        else: reward = -0.04
            
        return next_s, reward

def policy_evaluation(env, policy, gamma=0.99, theta=1e-6):
    """Evaluate a policy using iterative Bellman expectation updates."""
    V = np.zeros(env.n_states)
    deltas = []
    
    while True:
        delta = 0
        for s in range(env.n_states):
            if s in env.bad_states: continue
            
            v_old = V[s]
            action = policy[s]
            next_s, reward = env.step(s, action)
            
            # Bellman Expectation Update
            V[s] = reward + gamma * V[next_s]
            delta = max(delta, abs(v_old - V[s]))
        
        deltas.append(delta)
        if delta < theta:
            break
            
    return V, deltas

# Define a simple policy: Go right if possible, otherwise up.
policy = np.full(12, 3, dtype=int)  # Default to right
policy[[3, 7, 11]] = 0  # From right edge, go up

env = GridWorld()
values, deltas = policy_evaluation(env, policy)

# Visualize convergence and value function
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
ax1.set_title("Policy Evaluation Convergence")
ax1.plot(deltas)
ax1.set_xlabel('Iteration')
ax1.set_ylabel('Max Value Change (Δ)')
ax1.set_yscale('log')
ax1.grid(True, alpha=0.3)

# Value heatmap
V_grid = values.reshape(env.shape)
cax = ax2.imshow(V_grid, cmap='viridis_r')
ax2.set_title("Value Function Under Policy")
fig.colorbar(cax, ax=ax2, label='Value')
for s in range(env.n_states):
    row, col = divmod(s, env.shape[1])
    ax2.text(col, row, f"{values[s]:.2f}", ha='center', va='center', color='white' if values[s] < 0.5 else 'black')
plt.tight_layout()
plt.show()
                </code></pre>
            </div>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-star"></i>
            3.2 The Bellman Optimality Equation: Finding the Best Possible Value
          </summary>
          <div class="details-content">
              <p>
                  The expectation equations are for evaluation. But our goal is to find the <em>best</em> policy. The <strong>Bellman Optimality Equation</strong> describes the value of the optimal policy, $\pi^*$. It is a statement about the condition that the value function for an optimal policy must satisfy, forming the basis for algorithms seeking to maximize return.
              </p>
              <p>
                  The key difference from the expectation equation is the introduction of a <strong>$\max$ operator</strong>. An optimal policy must act greedily with respect to the optimal value function. Instead of averaging over a policy's actions, we simply choose the best one, reflecting a deterministic choice at each state for maximal long-term reward.
              </p>
              
              <h4>
                Optimal Value Functions $V^*(s)$ and $Q^*(s,a)$
              </h4>
              <p>The value of a state under an optimal policy, $V^*(s)$, must be equal to the expected return of taking the <em>single best possible action</em> from that state.</p>
              $$V^*(s) = \max_{a \in A} Q^*(s, a)$$
              <p>By substituting the expression for $Q^*$, we get the <strong>Bellman Optimality Equation for $V^*$</strong>:</p>
              $$V^*(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma V^*(S_{t+1}) | S_t=s, A_t=a]$$
              <p>Expanding with environment dynamics:</p>
              $$V^*(s) = \max_{a} \sum_{s', r} p(s', r | s, a) \left( r + \gamma V^*(s') \right)$$
              <p>The optimality equation for $Q^*$ is even more important for model-free learning. It states that the optimal value of taking action $a$ in state $s$ is the immediate reward plus the discounted value of the <em>best possible action from the next state</em>.</p>
              $$Q^*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') | S_t=s, A_t=a]$$
              <p>Or explicitly:</p>
              $$Q^*(s, a) = \sum_{s', r} p(s', r | s, a) \left( r + \gamma \max_{a'} Q^*(s', a') \right)$$
              <p>This equation is the foundation of the famous Q-learning algorithm, which we'll explore in the next sections.</p>
              
              <div class="admonition danger">
                <span class="admonition-title">
                  <i class="fas fa-exclamation-triangle"></i>
                  Non-Linearity and Complexity
                </span>
                <p>The $\max$ operator makes the optimality equation non-linear, unlike the linear system in policy evaluation. This non-linearity prevents direct matrix solutions and necessitates iterative methods or approximations, especially in large or continuous state spaces.</p>
              </div>
              
              <h4>
                Value Iteration
              </h4>
              <p>
                The Bellman optimality equation gives us a straightforward algorithm for finding the optimal policy, known as <strong>Value Iteration</strong>. It is similar to policy evaluation, but instead of applying a policy-specific backup, we apply the optimality backup at each step, maximizing over actions. Convergence to $V^*$ is guaranteed for γ < 1, as the max operator combined with discounting forms a contraction.
              </p>
              
              <details>
                <summary><i class="fas fa-calculator"></i> Proof of Convergence for Value Iteration</summary>
                <div class="details-content">
                  <p>The optimality operator T*(V) = max_a [R_a + γP_a V] is a contraction: ||T*(V1) - T*(V2)||_∞ ≤ γ||V1 - V2||_∞, since max preserves the bound. By Banach's theorem, iteration converges to $V^*$, and the greedy policy π(s) = argmax_a Q^*(s,a) is optimal.</p>
                </div>
              </details>
              
              <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Value Iteration</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://github.com/rl-omnibus/bellman-eval" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
def value_iteration(env, gamma=0.99, theta=1e-6):
    """Solve MDP using value iteration (Bellman optimality)."""
    V = np.zeros(env.n_states)
    deltas = []
    
    while True:
        delta = 0
        for s in range(env.n_states):
            if s in env.bad_states: continue
            
            v_old = V[s]
            action_values = np.zeros(env.n_actions)
            for a in range(env.n_actions):
                next_s, reward = env.step(s, a)
                action_values[a] = reward + gamma * V[next_s]
            
            # Bellman Optimality Update
            V[s] = np.max(action_values)
            delta = max(delta, abs(v_old - V[s]))
        
        deltas.append(delta)
        if delta < theta:
            break
    
    # Extract optimal policy
    policy = np.zeros(env.n_states, dtype=int)
    for s in range(env.n_states):
        if s in env.bad_states: continue
        action_values = np.zeros(env.n_actions)
        for a in range(env.n_actions):
            next_s, reward = env.step(s, a)
            action_values[a] = reward + gamma * V[next_s]
        policy[s] = np.argmax(action_values)
        
    return V, policy, deltas

env = GridWorld()
V_star, policy_star, deltas = value_iteration(env)

# Visualize convergence and final policy
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Convergence plot
ax1.plot(deltas)
ax1.set_xlabel('Iteration')
ax1.set_ylabel('Max Value Change (Δ)')
ax1.set_title('Value Iteration Convergence')
ax1.set_yscale('log')
ax1.grid(True, alpha=0.3)

# Policy plot
V_grid = V_star.reshape(env.shape)
cax = ax2.imshow(V_grid, cmap='viridis_r')
ax2.set_title('Optimal Policy (Arrows) & Value Function (Colors)')
fig.colorbar(cax, ax=ax2, label='Value')
arrows = ['↑', '↓', '←', '→']
for s in range(env.n_states):
    if s in env.bad_states: continue
    row, col = divmod(s, env.shape[1])
    ax2.text(col, row, arrows[policy_star[s]], 
             ha='center', va='center', color='white' if V_star[s] < 0.5 else 'black', fontsize=18, weight='bold')
plt.tight_layout()
plt.show()
                </code></pre>
            </div>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale"></i>
            3.3 Connections and Implications
          </summary>
          <div class="details-content">
              <p>The Bellman equations unify evaluation and optimization, linking policy assessment (expectation) to improvement (optimality). They underpin dynamic programming (exact solutions), temporal difference learning (sample-based updates), and deep RL approximations (function approximation in <a href="#part5">Part 5</a>).</p>
              <table>
                <thead>
                  <tr><th>Equation</th><th>Purpose</th><th>Key Operation</th><th>Algorithms</th></tr>
                </thead>
                <tbody>
                  <tr><td>Expectation ($V^\pi$)</td><td>Evaluate policy</td><td>Average over π</td><td>Policy Evaluation, SARSA</td></tr>
                  <tr><td>Optimality ($V^*$)</td><td>Find best policy</td><td>Max over actions</td><td>Value Iteration, Q-Learning</td></tr>
                </tbody>
              </table>
              <p>Implications include computational challenges: expectation is linear but optimality’s max introduces non-linearity, critical for stability issues in deep RL (see <a href="#part6">Part 6</a>’s Deadly Triad).</p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-question-circle"></i>
            3.4 Limitations and Open Questions
          </summary>
          <div class="details-content">
              <ul>
                <li><strong>Scalability:</strong> Bellman updates require full state sweeps, infeasible for large/continuous spaces. How can sampling or approximation scale this?</li>
                <li><strong>Partial Observability:</strong> Equations assume full state visibility (MDP). What modifications handle POMDPs with hidden states?</li>
                <li><strong>Non-Stationarity:</strong> Dynamics assume fixed P(s',r|s,a). How to adapt to changing environments?</li>
              </ul>
          </div>
        </details>
      </section>
      <hr/>

<section id="part4">
        <h2 id="part4-title">
          <i class="fas fa-question-circle"></i>
          Part 4: The Core Dilemma: Exploration vs. Exploitation
        </h2>
        <details open class="fade-in">
            <summary>
              <i class="fas fa-landmark"></i>
              4.1 The Multi-Armed Bandit: A Pure Exploration Problem
            </summary>
            <div class="details-content">
                <p>
                    We have established that an agent's goal is to find an optimal policy $\pi^*$ by learning an optimal value function, $Q^*$, and then acting greedily with respect to it. However, a critical problem emerges during the learning process itself: how can the agent learn the <em>true</em> optimal value function if it only ever follows its <em>current</em> best guess? This creates the most fundamental trade-off in online decision-making: the <strong>exploration versus exploitation</strong> dilemma.
                </p>
                <h4>Historical Context and Origins</h4>
                <p>
                    The roots of this problem are deep, originating from statistical decision theory. The problem gained prominence during World War II, where Allied scientists, including Alan Turing, used similar sequential analysis techniques for code-breaking. It was also conceptually similar to challenges in adaptive clinical trials, where doctors needed to decide which of several experimental treatments to administer to patients. The goal was to quickly identify the best treatment (exploit) while still gathering data on the others (explore) to ensure the conclusion was statistically sound. The problem was formally defined as the "multi-armed bandit problem" by Herbert Robbins in his seminal 1952 paper, which laid the mathematical groundwork for decades of research. Today, these principles are the backbone of A/B testing platforms at companies like Google and Netflix.
                </p>
                <div class="admonition info">
                    <span class="admonition-title">Analogy: Choosing a Restaurant (Quantitative Extension)</span>
                    <p>Imagine your restaurant choices have rewards drawn from Gaussian distributions.
                    <ul>
                        <li><strong>Restaurant A (Exploit):</strong> Your favorite. Reliable, with a known reward distribution of $\mathcal{N}(8, 1)$.</li>
                        <li><strong>Restaurant B (Explore):</strong> A new place. Its true reward distribution is $\mathcal{N}(10, 9)$—a high average but with high variance, meaning it could be amazing or terrible. You don't know this.</li>
                    </ul>
                    A pure exploitation strategy yields an expected cumulative reward over 30 days of $30 \times 8 = 240$. A balanced strategy might explore for 5 nights. You might get unlucky with draws of 3, 5, 6, and 4 from Restaurant B, giving a low initial estimate. A smart exploration strategy needs to account for this variance and recognize that there's still a chance Restaurant B is better, despite early negative results.
                    </p>
                </div>
                
                <h4>Mathematical Formulation</h4>
                <p>
                    The purest distillation of this dilemma is the <strong>k-armed bandit</strong> problem. It's a simplified RL problem with only one state. At each time step $t=1, 2, \dots, T$, an agent chooses an action (or "arm") $A_t$ from $k$ possibilities and receives a reward $R_t$. The value of an action $a$ is its expected reward: $q_*(a) = \mathbb{E}[R_t | A_t = a]$. The agent's goal is to find a policy that maximizes the cumulative reward $\sum_{t=1}^T R_t$.
                </p>
                <p>The agent maintains an estimate $Q_t(a)$ for each action's value. A common way to update this estimate after receiving reward $r_t$ for action $a$ is via an incremental update rule:</p>
                $$Q_{t+1}(a) = Q_t(a) + \alpha_t (r_t - Q_t(a))$$
                <p>The **step-size parameter** $\alpha_t$ determines how much new information overrides old. For stationary problems (where $q_*(a)$ is constant), using a sample-average step-size $\alpha_t = 1/N_t(a)$ guarantees convergence. For **non-stationary bandits**, where reward distributions change over time (e.g., ad click-through rates changing with trends), a constant step-size $\alpha \in (0, 1]$ is preferred. This gives more weight to recent rewards, allowing the agent to "forget" outdated information.</p>

                <h5>Limitations and Extensions: Contextual Bandits</h5>
                <p>The simple bandit problem assumes that the arms are independent and the best choice is always the same. This is often not true. For instance, in a recommendation system, the best movie to recommend depends on the user. This gives rise to the <strong>contextual bandit</strong> problem, where at each step, the agent first observes a context (or state) $s_t$ before choosing an action. The goal is then to learn a policy $\pi(a|s)$ that maximizes reward. This is a crucial bridge between simple bandits and full reinforcement learning.</p>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-chart-line"></i>
              4.2 Regret Analysis: Quantifying Performance
            </summary>
            <div class="details-content">
                <p>How do we measure how well an exploration strategy performs? The key metric is <strong>regret</strong>. The total regret after $T$ steps is the cumulative opportunity loss—the difference between the total reward of an optimal strategy and the agent's total reward.</p>
                <p>Let $q_*(a^*) = \max_a q_*(a)$ be the value of the truly optimal action $a^*$. The total or cumulative regret is:</p>
                $$L_T = \sum_{t=1}^T (q_*(a^*) - q_*(A_t))$$
                <p>The goal of a good exploration strategy is to have <strong>sub-linear total regret</strong>, meaning that the average regret per step, $L_T/T$, approaches zero as $T \to \infty$. This indicates the agent is successfully converging on the optimal action.</p>
                
                <table>
                    <thead>
                        <tr><th>Strategy</th><th>Regret Bound</th><th>Description</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>Greedy</td><td>$O(T)$</td><td><strong>Linear.</strong> Can get stuck on a suboptimal arm forever.</td></tr>
                        <tr><td>ε-Greedy</td><td>$O(k \log T)$</td><td><strong>Sub-linear.</strong> Guaranteed to converge but explores inefficiently.</td></tr>
                        <tr><td>UCB1</td><td>$O(k \log T)$</td><td><strong>Logarithmic.</strong> Near-optimal regret by balancing value with uncertainty.</td></tr>
                        <tr><td>Thompson Sampling</td><td>$O(\sqrt{kT \log k})$</td><td><strong>Square-root.</strong> Bayesian approach, often empirically the best.</td></tr>
                        <tr><td>EXP3 (Adversarial)</td><td>$O(\sqrt{T k \log k})$</td><td><strong>Square-root.</strong> For non-stationary or adversarial reward settings.</td></tr>
                    </tbody>
                </table>
                <p>These bounds show that principled exploration strategies grow their regret far more slowly than simpler methods. For example, in online advertising, minimizing regret means minimizing the number of times a less effective ad is shown, directly translating to higher revenue.</p>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-dice"></i>
              4.3 Strategy 1: The ε-Greedy Strategy
            </summary>
            <div class="details-content">
                <p>The <strong>$\epsilon$-greedy</strong> policy is the simplest and most common baseline. It is defined by a single hyperparameter, $\epsilon \in [0, 1]$:</p>
                <ul>
                    <li>With probability <strong>$1 - \epsilon$</strong>, the agent <strong>exploits</strong>: it chooses the action with the highest estimated Q-value, $a_t = \arg\max_a Q_t(a)$.</li>
                    <li>With probability <strong>$\epsilon$</strong>, the agent <strong>explores</strong>: it chooses an action uniformly at random from the set of all available actions.</li>
                </ul>
                
                <h4>Inefficiencies and Variants</h4>
                <p>The uniform nature of ε-greedy's exploration is its main weakness. A common practical improvement is to <strong>anneal</strong> or <strong>decay</strong> $\epsilon$ over time. The agent starts with a high $\epsilon$ (e.g., $\epsilon=1.0$) and gradually decreases it. Another variant is **ε-first**, which has a pure exploration phase for $N$ steps followed by pure exploitation.</p>

                <div class="admonition danger">
                    <span class="admonition-title"><i class="fas fa-exclamation-triangle"></i>Counterexample: Sparse Rewards</span>
                    <p>Imagine a 1000-arm bandit where only one arm gives a reward of +1 and all others give 0. With ε=0.1, the agent will spend 90% of its time pulling what it initially thinks is the best arm (which is likely a 0-reward arm). It explores randomly only 10% of the time, so the expected number of pulls to find the single good arm is very high. More directed exploration strategies are needed here.</p>
                </div>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-balance-scale"></i>
              4.4 Strategy 2: Upper-Confidence-Bound (UCB) Action Selection
            </summary>
            <div class="details-content">
                <p>The UCB family of algorithms provides a more intelligent exploration strategy based on the principle of <strong>"optimism in the face of uncertainty."</strong> It selects an action using the following formula:</p>
                $$A_t = \arg\max_{a \in A} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]$$
                <p>The second term is the <strong>uncertainty bonus</strong>. If an action has been tried very few times ($N_t(a)$ is small), the bonus is large, encouraging exploration. As $N_t(a)$ grows, the bonus shrinks, and the selection becomes dominated by the known value $Q_t(a)$.</p>
                
                <details>
                    <summary><i class="fas fa-calculator"></i> Derivation Sketch of UCB's Regret Bound</summary>
                    <div class="details-content">
                        <p>The logarithmic regret bound for UCB is derived from concentration inequalities. For rewards bounded in $[0, 1]$, <strong>Hoeffding's Inequality</strong> states that the probability of the empirical mean $\hat{\mu}$ of $n$ samples deviating from the true mean $\mu$ by more than $\epsilon$ is bounded:</p>
                        $$P(|\hat{\mu} - \mu| \ge \epsilon) \le 2e^{-2n\epsilon^2}$$
                        <p>By setting the right side to a small probability $p$ and solving for $\epsilon$, we find that with probability $1-p$, the true mean is bounded by $\mu \le \hat{\mu} + \sqrt{\frac{\ln(2/p)}{2n}}$. The UCB bonus term $\sqrt{\frac{c \ln t}{N_t(a)}}$ is derived from this form by setting $p = 1/t^x$ for some $x$, ensuring that the probability of our bound being wrong decreases polynomially with time. This guarantees that we don't overestimate indefinitely and that the number of times a suboptimal arm is pulled is bounded logarithmically.</p>
                    </div>
                </details>
                
                <h4>UCB Variants and Limitations</h4>
                <ul>
                    <li><strong>KL-UCB:</strong> Uses the Kullback-Leibler (KL) divergence to create tighter, distribution-aware confidence bounds for non-Gaussian rewards.</li>
                    <li><strong>Discounted UCB:</strong> For non-stationary bandits, past observations are discounted when calculating $Q_t(a)$ and $N_t(a)$, allowing the agent to adapt to changing reward distributions.</li>
                </ul>
                <p>The main limitation of UCB is its sensitivity to the exploration parameter $c$. Setting $c$ too high leads to excessive exploration, while setting it too low leads to premature exploitation. This parameter often requires careful tuning via cross-validation for each specific problem, such as selecting optimal paths in a stochastic network routing application.</p>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-chart-pie"></i>
              4.5 Strategy 3: Thompson Sampling (Bayesian Approach)
            </summary>
            <div class="details-content">
                <p><strong>Thompson Sampling</strong>, also known as posterior sampling, is a Bayesian approach that is often highly effective. Instead of maintaining a single point estimate, it maintains a full <strong>probability distribution</strong> representing our belief about each arm's value.</p>
                <ol>
                    <li><strong>Define a Prior:</strong> Assume a prior distribution for each arm's value. For Bernoulli rewards (0 or 1), the <strong>Beta distribution</strong> is the conjugate prior. For Gaussian rewards with known variance, the prior on the mean is a <strong>Gaussian distribution</strong>.</li>
                    <li><strong>Sample from Posterior:</strong> At each step, draw one random sample from each arm's current posterior distribution.</li>
                    <li><strong>Act Greedily:</strong> Choose the arm corresponding to the largest sample.</li>
                    <li><strong>Update Posterior:</strong> After observing the reward, update the parameters of the chosen arm's distribution using Bayes' rule. For a Gaussian prior $\mathcal{N}(\mu_0, \sigma_0^2)$ and observed rewards with variance $\sigma^2$, the posterior mean and variance are updated accordingly.</li>
                </ol>
                <p>This approach elegantly handles exploration. An arm with high uncertainty (a wide posterior distribution) has a chance of producing a high sample, encouraging exploration. An arm with low uncertainty (a narrow posterior) will produce samples close to its estimated mean. This method has proven highly effective in large-scale systems like the news feed ranking at LinkedIn.</p>
            </div>
        </details>
        
        <details open class="fade-in">
            <summary>
                <i class="fas fa-vial"></i>
                4.6 Head-to-Head Comparison of Bandit Strategies
            </summary>
            <div class="details-content">
                <p>The conceptual plots above are illustrative. Below is a complete simulation testbed that empirically compares these strategies across multiple runs to provide a statistically robust view of their performance.</p>
                <div class="code-container">
                    <div class="code-block-header">
                        <span><i class="fab fa-python"></i> Python: Full Bandit Strategy Testbed</span>
                        <div class="buttons">
                            <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                            <a href="https://github.com/rl-omnibus/advanced-bandit-sim" target="_blank" class="colab-button" title="View on GitHub">
                               <i class="fab fa-github"></i> View on GitHub
                            </a>
                        </div>
                    </div>
                    <pre><code class="language-python">
# Assumes the Agent classes from Part 1 are defined
# (EpsilonGreedyAgent, UCBAgent)
# A ThompsonSamplingAgent would need to be added.

class BanditTestbed:
    def __init__(self, n_arms=10, n_steps=1000, n_runs=200):
        self.n_arms = n_arms
        self.n_steps = n_steps
        self.n_runs = n_runs
        self.results = {}

    def run(self, agent_class, **kwargs):
        agent_name = agent_class.__name__.replace("Agent", "") + str(kwargs)
        all_rewards = np.zeros((self.n_runs, self.n_steps))
        all_regrets = np.zeros((self.n_runs, self.n_steps))

        for i in range(self.n_runs):
            bandit = Bandit(self.n_arms)
            agent = agent_class(self.n_arms, **kwargs)
            for t in range(self.n_steps):
                action = agent.select_action()
                reward = bandit.pull(action)
                agent.update(action, reward)
                all_rewards[i, t] = reward
                all_regrets[i, t] = bandit.optimal_value - reward
        
        self.results[agent_name] = {
            'rewards': np.mean(all_rewards, axis=0),
            'regret': np.cumsum(np.mean(all_regrets, axis=0))
        }
        print(f"Finished running {agent_name}")

    def plot(self):
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
        for name, data in self.results.items():
            ax1.plot(data['rewards'], label=name)
            ax2.plot(data['regret'], label=name)
        
        ax1.set_ylabel("Average Reward")
        ax1.set_title(f"Bandit Algorithm Performance (Averaged over {self.n_runs} runs)")
        ax1.legend()
        ax2.set_xlabel("Steps")
        ax2.set_ylabel("Cumulative Regret")
        ax2.set_title("Cumulative Regret vs. Time")
        ax2.legend()
        plt.tight_layout()
        plt.show()

# --- Run Full Comparison ---
# testbed = BanditTestbed()
# testbed.run(EpsilonGreedyAgent, epsilon=0.1)
# testbed.run(UCBAgent, c=2)
# testbed.plot()
# Note: A full run requires the Agent classes. This code is for illustration.
                    </code></pre>
                </div>
            </div>
        </details>
        
        <details open class="fade-in">
            <summary>
              <i class="fas fa-forward"></i>
              4.7 From Bandits to Full Reinforcement Learning
            </summary>
            <div class="details-content">
                <p>
                    The strategies developed for bandits form the basis for exploration in full RL, where the challenge is to explore a vast <strong>state-action space</strong>.
                </p>
                <ul>
                    <li><strong>DQN's ε-Greedy Policy:</strong> The standard DQN algorithm uses an ε-greedy strategy. The Q-values it learns are state-dependent ($Q(s,a)$), but the exploration mechanism is identical to the bandit version.</li>
                    <li><strong>Noisy Networks:</strong> An advanced technique where noise is added directly to the network's weights. The network can learn to control the amount of noise, effectively learning its own adaptive exploration policy that is state-dependent, far more sophisticated than simple ε-greedy.</li>
                    <li><strong>Bayesian Deep Learning:</strong> Methods like Bootstrapped DQN train an ensemble of Q-networks. The variance in their predictions for a given state-action pair serves as a measure of uncertainty, similar to Thompson Sampling, to guide exploration.</li>
                    <li><strong>Curiosity-Driven Exploration:</strong> In sparse-reward settings, an agent may never receive a signal to learn from. Curiosity methods provide an intrinsic reward for visiting novel states. For example, the Intrinsic Curiosity Module (ICM) rewards the agent for actions that lead to states that are hard for the agent to predict, encouraging it to explore parts of the environment it doesn't understand.</li>
                </ul>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-lightbulb"></i>
              4.8 Information-Theoretic and Other Advanced Approaches
            </summary>
            <div class="details-content">
                <p>State-of-the-art exploration often frames the problem in terms of information theory. The goal is not just to get rewards, but to take actions that maximally reduce the agent's uncertainty about the environment.</p>
                <ul>
                    <li><strong>Information Gain:</strong> An agent can be motivated to take actions that maximize the information gained about the environment's dynamics. This is often quantified by the change in entropy of the agent's belief distribution over the true MDP.</li>
                    <li><strong>Variational Information Maximizing Exploration (VIME):</strong> An algorithm that formalizes this by training a model of the environment's dynamics and rewarding the agent based on the KL-divergence between its belief before and after observing a transition.</li>
                </ul>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-cogs"></i>
              4.9 Practical Considerations and Open Challenges
            </summary>
            <div class="details-content">
                <p>While powerful, many advanced exploration techniques face challenges in practice:</p>
                <ul>
                    <li><strong>Computational Cost:</strong> Bayesian methods like Thompson Sampling can be computationally expensive for high-dimensional action or parameter spaces. Ensembles of neural networks multiply the cost of training and inference.</li>
                    <li><strong>Scalability to Continuous Actions:</strong> Most bandit strategies are designed for a discrete, finite number of actions. Scaling exploration to continuous action spaces (e.g., in robotics) is a major open research area, often involving techniques like Gaussian processes or parameter-space noise.</li>
                    <li><strong>Safe Exploration:</strong> How can we ensure that an agent's exploration doesn't lead to catastrophic outcomes, such as a physical robot breaking itself? This is a critical challenge for deploying RL in the real world and an active area of research. One open question is how to effectively incorporate human feedback into the exploration process to guide the agent away from dangerous regions of the state space.</li>
                </ul>
            </div>
        </details>

      <!-- Interactive multi-armed bandit playground (moved from Part 1). This interactive demo reinforces the exploration–exploitation trade-off discussed in Part 4. -->
      <details class="fade-in">
        <summary>
          <i class="fas fa-gamepad"></i>
          4.10 Interactive Multi‑Armed Bandit Playground
        </summary>
        <div class="details-content">
          <p>
            To internalize the exploration–exploitation dilemma, try the interactive multi‑armed bandit below. Each arm hides an unknown reward distribution (between 0 and 1). Use the slider to set an $\epsilon$ value and press <em>Next step</em> to let the agent choose an arm via the $\epsilon$‑greedy strategy. Watch how the estimated values converge as you explore and exploit.
            Formally, a $K$‑armed bandit problem involves $K$ independent arms with unknown reward distributions; each arm $i$ yields reward $R_i$ drawn from a distribution with expected value $\mu_i$. The goal is to maximize the cumulative reward over $T$ trials, balancing exploration and exploitation. The $\epsilon$‑greedy algorithm selects a random arm with probability $\epsilon$ and the best‑known arm with probability $1-\epsilon$.
          </p>
          <div id="bandit-interface" style="margin:1rem 0;">
            <label for="epsilon-range">Epsilon (exploration rate): </label>
            <input type="range" id="epsilon-range" min="0" max="1" step="0.05" value="0.1" style="vertical-align:middle;">
            <span id="epsilon-value">0.10</span>
            <button id="bandit-step-btn" style="margin-left:1rem;">Next step</button>
          </div>
          <div id="bandit-chart" style="width:100%;height:300px;"></div>
          <p id="bandit-message" style="margin-top:1rem;"></p>
        </div>
      </details>
      </section>
      <hr/>
<section id="part5">
        <h2 id="part5-title">
          <i class="fas fa-rocket"></i>
          Part 5: The Leap to Deep Reinforcement Learning
        </h2>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-table-cells-large"></i>
            5.1 From Tables to Functions: The Necessity of Approximation
          </summary>
          <div class="details-content">
            <p>
              The methods we have discussed so far, rooted in the <a href="#part3">Bellman equations</a>, form the bedrock of <strong>tabular RL</strong>. Their power lies in their simplicity and theoretical guarantees of convergence. This approach saw early successes, such as Arthur Samuel's 1959 checkers program, which used a form of tabular value function (a weighted linear combination of features) to achieve a high level of play. However, these successes were confined to problems with small, discrete, or carefully engineered state spaces.
            </p>
            
            <h4>The Curse of Dimensionality</h4>
            <p>
              The primary limitation of tabular methods is the <strong>curse of dimensionality</strong>. This refers to the exponential growth of the state space volume as the number of state variables (dimensions) increases. If a state is defined by $n$ variables, each of which can take $d$ discrete values, the total number of states is $|S| = d^n$.
            </p>
            
            <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                    <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                    <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
                </div>
                <div class="mermaid">
                  graph LR
                    subgraph StateSpaceExplosion["State Space Size vs. Dimensions (d=10)"]
direction LR
                        D1["1 Dim<br/>10¹ states"] --> D2["2 Dims<br/>10² states"]
                        D2 --> D3["3 Dims<br/>10³ states"]
                        D3 --> D10["...<br/>10 Dims<br/>10¹⁰ states"]
                        D10 --> D_Protein["...<br/>Protein Folding<br/> >10³⁰⁰ states"]
                    end
                    classDef highDim fill:#ff3c5b,color:white;
                    class D_Protein highDim;
                </div>
            </div>
            
            <div class="admonition danger">
                <span class="admonition-title">
                  <i class="fas fa-bomb"></i>
                  The Twin Failures of Tabular RL
                </span>
                <ol>
                    <li><strong>Computational and Memory Infeasibility</strong>: It is physically impossible to store a Q-table for problems like Atari. For a self-driving car with just 10 sensors each discretized into 100 values, the state space is $100^{10} = 10^{20}$, far exceeding any feasible memory capacity.</li>
                    <li><strong>Statistical and Sample Infeasibility</strong>: Even with infinite memory, learning the value of every state requires visiting it. Sample complexity bounds for tabular methods are polynomial in $|S|$ and $|A|$, often requiring $O(|S| |A| / \epsilon^2)$ samples for $\epsilon$-accuracy. For a state space of $10^{20}$, this is impossible.</li>
                </ol>
            </div>
            <p>Tabular methods cannot <strong>generalize</strong>. Learning that a specific board state in chess is good provides no information about a nearly identical board state.</p>
            
            <h4>The Solution: Function Approximation</h4>
            <p>
              The only way to scale RL is to use a <strong>parameterized function</strong> to estimate the value function. Instead of a table entry for each state, we learn the parameters $\theta$ of a function $\hat{q}(s, a; \theta)$ that approximates the true value function $Q^*(s,a)$.
            </p>
            <p>
              The learning problem becomes finding parameters $\theta$ that minimize the Mean Squared Bellman Error (MSBE), derived from the Bellman residual:
            </p>
            $$L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( \underbrace{r + \gamma \max_{a'} \hat{q}(s', a'; \theta^-)}_{\text{TD Target}} - \underbrace{\hat{q}(s,a;\theta)}_{\text{Prediction}} \right)^2 \right]$$
            <p>This loss is then minimized using stochastic gradient descent (SGD):</p>
            $$\theta \leftarrow \theta - \alpha \nabla_{\theta} L(\theta)$$
            <p>By choosing a function with an appropriate <strong>inductive bias</strong> (e.g., assuming smoothness), the model can generalize from visited states to make intelligent predictions about the value of new, unseen states.</p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-cogs"></i>
            5.2 Historical Breakthroughs and Failures
          </summary>
          <div class="details-content">
            <h4>Breakthrough: TD-Gammon</h4>
            <p>
              One of the most significant early successes was Gerald Tesauro's <strong>TD-Gammon</strong> (1995). It was a backgammon-playing agent that used a multi-layer perceptron trained with TD learning and backpropagation. By learning through self-play, TD-Gammon achieved a level of play comparable to the best human players, demonstrating that neural networks could learn high-quality features and generalize effectively in a complex, stochastic game.
            </p>
            <h4>Historical Failures: The Deadly Triad</h4>
            <p>
              Despite TD-Gammon's success, many early attempts to combine neural networks with RL failed catastrophically. Long-Ji Lin's 1992 work, which introduced experience replay, sometimes diverged due to correlated updates. Later work by Boyan and Moore (1995) and Tsitsiklis and Van Roy (1997) demonstrated simple MDPs where the combination of function approximation, bootstrapping, and off-policy learning would cause value estimates to diverge to infinity. This <strong>Deadly Triad</strong>, which we will revisit in <a href="#part6">Part 6</a>, highlighted a fundamental instability that wasn't reliably solved until the innovations of DQN.
            </p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-tools"></i>
            5.3 A Tour of Function Approximators
          </summary>
          <div class="details-content">
            <ul>
              <li><strong>Linear Models</strong>: Approximates the value function as a linear combination of hand-crafted features: $\hat{v}(s, \theta) \approx \theta^T \phi(s)$.</li>
              <li><strong>Fourier Features</strong>: A powerful basis for linear models, especially for periodic functions, using cosine features: $\hat{v}(s) = \sum_k w_k \cos(\pi \mathbf{c}_k \cdot s)$.</li>
              <li><strong>Decision Trees & Ensembles</strong>: Methods like Gradient Boosting can learn non-linear value functions and are often more interpretable than neural networks.</li>
              <li><strong>Radial Basis Functions (RBFs)</strong>: A kernel-based method that provides smooth, local approximations. Its complexity often scales poorly with the number of data points.</li>
              <li><strong>Deep Neural Networks (DNNs)</strong>: The default choice for DRL due to their ability to perform <strong>representation learning</strong> from raw, high-dimensional inputs. The Universal Approximation Theorem provides theoretical justification for their expressiveness.</li>
            </ul>
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Approximating a Value Function on a Chain MDP</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://github.com/rl-omnibus/approx-tour" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.kernel_ridge import KernelRidge
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import SplineTransformer
from sklearn.metrics import mean_squared_error

# A simple 1D "Chain" MDP value function to approximate
states = np.arange(20)
gamma = 0.9
true_values = gamma**(19 - states)

# Generate train/test data
X_train = np.random.choice(states, size=10, replace=False).reshape(-1, 1)
y_train = gamma**(19 - X_train.ravel())
X_test = states.reshape(-1, 1)
y_test = true_values

# Models to compare
models = {
    "Linear (High Bias)": LinearRegression(),
    "RBF Kernel": KernelRidge(kernel='rbf', gamma=0.1),
    "Spline (B-Spline)": make_pipeline(SplineTransformer(n_knots=5, degree=3), LinearRegression()),
    "MLP (Good Fit)": MLPRegressor(hidden_layer_sizes=(32, 16), max_iter=10000, random_state=42, learning_rate_init=0.01)
}

fig, ax = plt.subplots(figsize=(12, 7))
ax.plot(X_test, y_test, 'g-', linewidth=4, label='True Value Function', alpha=0.8)
ax.plot(X_train, y_train, 'ko', markersize=8, label='Training Data')

print("--- Approximator Performance ---")
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f"{name:<25} | Test MSE: {mse:.4f}")
    ax.plot(X_test, y_pred, '--', label=f"{name} (MSE: {mse:.4f})")

ax.set_title("Approximating a Value Function in a Chain MDP")
ax.set_xlabel("State")
ax.set_ylabel("Value V(s)")
ax.legend()
ax.grid(True, alpha=0.3)
plt.show()
                </code></pre>
            </div>
          </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-exchange-alt"></i>
              5.4 Trade-offs, Failures, and Modern Advancements
            </summary>
            <div class="details-content">
                <h4>The Bias-Variance Trade-off</h4>
                <p>Using function approximators forces us to confront the classic bias-variance trade-off.</p>
                <details>
                    <summary><i class="fas fa-calculator"></i> Derivation: Bias-Variance Decomposition</summary>
                    <div class="details-content">
                        <p>The Mean Squared Error (MSE) of an estimator $\hat{f}(x)$ for a true function $f(x)$ with noise $\epsilon$ (where $y = f(x) + \epsilon$) can be decomposed. The expected prediction error at a point $x$ is:</p>
                        $$ \mathbb{E}[(y - \hat{f}(x))^2] = (\mathbb{E}[\hat{f}(x)] - f(x))^2 + \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2] + \sigma^2 $$
                        $$ \text{MSE} = \text{Bias}(\hat{f}(x))^2 + \text{Var}(\hat{f}(x)) + \text{Irreducible Error} $$
                        <ul>
                            <li><strong>Bias:</strong> Error from wrong assumptions. High bias causes underfitting.</li>
                            <li><strong>Variance:</strong> Error from sensitivity to the training set. High variance causes overfitting.</li>
                        </ul>
                    </div>
                </details>
                <p>Techniques like L2 regularization, dropout, and early stopping are essential for managing this trade-off in deep RL.</p>

                <h4>Modern Alternatives and Advancements</h4>
                <p>While CNNs and MLPs are the workhorses of DRL, recent advancements have introduced more powerful architectures:</p>
                <ul>
                    <li><strong>Transformers:</strong> Architectures like the Decision Transformer frame RL as a sequence modeling problem. They take a sequence of past states, actions, and rewards as input and autoregressively predict the next action.</li>
                    <li><strong>Diffusion Models:</strong> These generative models are being explored for policy generation in offline RL, learning to generate trajectories that are "close" to the expert data distribution.</li>
                    <li><strong>Case Study - Gato:</strong> DeepMind's Gato is a multi-modal, multi-task agent that uses a single, large Transformer network to perform a wide variety of tasks, from playing Atari to controlling a real robot arm, demonstrating the scaling potential of a single, generalist approximator.</li>
                </ul>

                <div class="admonition warning">
                    <span class="admonition-title"><i class="fas fa-question-circle"></i>Open Research Questions</span>
                    <ul>
                        <li>How do the inductive biases of large pre-trained models (like LLMs) change RL paradigms, especially in areas like RL from Human Feedback (RLHF)?</li>
                        <li>How can we incorporate physics-informed priors into approximators for better sim-to-real transfer in robotics?</li>
                    </ul>
                </div>
            </div>
        </details>
      </section>
      <hr/>

<section id="part6">
        <h2 id="part6-title">
          <i class="fas fa-bolt"></i>
          Part 6: Deep Q-Networks (DQN): The Revolution Begins
        </h2>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-biohazard"></i>
            6.1 The Challenge of Stability: The Deadly Triad
          </summary>
          <div class="details-content">
              <p>
                  The idea of combining neural networks with reinforcement learning is not new. However, early attempts were plagued by instability. As we saw in <a href="#part5">Part 5</a>, when a flexible, non-linear function approximator is used to estimate a value function, the learning process often becomes erratic and can diverge completely. This happens because the standard assumptions of supervised learning are fundamentally violated in the RL setting. For instance, early attempts like Long-Ji Lin's 1992 Q-learning with experience replay diverged due to correlated updates, and Riedmiller's Neural Fitted Q-Iteration (2005) showed promise but could diverge on simple tasks if not carefully configured. These failures highlighted the need for mechanisms to handle non-stationarity and correlation in data.
              </p>
              
              <table>
                <thead>
                  <tr><th>Year</th><th>Milestone</th><th>Outcome</th></tr>
                </thead>
                <tbody>
                  <tr><td>1992</td><td>Lin's Q-Learning with Replay</td><td>Divergence due to correlated updates</td></tr>
                  <tr><td>1997</td><td>Tsitsiklis and Van Roy's Divergence Theorems</td><td>Proofs of instability in approximate DP</td></tr>
                  <tr><td>2005</td><td>Riedmiller's NFQ</td><td>Promise but required careful tuning to avoid divergence</td></tr>
                </tbody>
              </table>
              
              <div class="admonition danger">
                  <span class="admonition-title">
                    <i class="fas fa-skull-crossbones"></i>
                    The Deadly Triad of Divergence
                  </span>
                  <p>Training becomes pathologically unstable when three elements are combined, creating a dangerous feedback loop where errors can be correlated and recursively amplified:</p>
                  <ul>
                    <li><strong>Function Approximation:</strong> Using a powerful, non-linear model (like a neural network) that generalizes.</li>
                    <li><strong>Bootstrapping:</strong> Updating an estimate with another estimate (e.g., the Bellman update).</li>
                    <li><strong>Off-Policy Learning:</strong> Learning about a greedy policy while following a more exploratory policy to gather data.</li>
                  </ul>
              </div>

              <details>
                <summary><i class="fas fa-calculator"></i> Mathematical Intuition for Instability: Variance Amplification</summary>
                <div class="details-content">
                    <p>The instability can be understood as variance amplification. The variance of the TD target, $Y_t = R_t + \gamma \max_{a'} \hat{q}(S_{t+1}, a'; \theta)$, depends on the variance of the next-state value estimate. In an off-policy setting, the Bellman operator is not a contraction in the L2 norm, and it can be shown that the variance of the updated value can be greater than the variance of the previous estimate:</p>
                    $$\text{Var}[Q_{t+1}] = \text{Var}[R] + \gamma^2 \text{Var}[Q_t] + 2\gamma \text{Cov}[R, Q_t]$$
                    <p>If $\gamma \approx 1$ and covariance terms are positive, errors compound, leading to an explosive feedback loop where variance grows unbounded, rather than diminishing as in stable methods.</p>
                </div>
              </details>
              
              <p>
                  In 2015, researchers at DeepMind introduced the <strong>Deep Q-Network (DQN)</strong> algorithm, the first method to successfully stabilize training in the face of this triad. It enabled an agent to master a wide range of complex Atari 2600 games directly from raw pixel inputs, a landmark achievement that launched the modern era of Deep Reinforcement Learning and influenced systems like AlphaGo. DQN's success came from two clever innovations: <strong>Experience Replay</strong> and a <strong>Fixed Target Network</strong>.
              </p>
              
              <h4>Why the Triad is "Deadly" in Practice</h4>
              <p>In real-world scenarios, the triad can lead to catastrophic failures. For example, in a cliff-walking MDP (a grid where one path has high rewards but risks falling off a cliff), off-policy bootstrapping with approximation can cause value overestimation, leading the policy to repeatedly choose risky actions and collapse. This has implications for robotic control, where instability might result in unsafe behaviors like erratic movements, potentially damaging hardware or the environment.</p>
              
              <table>
                <thead>
                  <tr><th>Method</th><th>Policy Type</th><th>Stability</th><th>Regret Bound (Theoretical)</th></tr>
                </thead>
                <tbody>
                  <tr><td>Naive Q-Learning with NN</td><td>Off-Policy</td><td>Unstable (Diverges)</td><td>None (due to divergence)</td></tr>
                  <tr><td>SARSA</td><td>On-Policy</td><td>Stable (Converges)</td><td>O(1/ε^2) samples</td></tr>
                  <tr><td>DQN</td><td>Off-Policy</td><td>Stable (Empirical)</td><td>Sub-linear in practice</td></tr>
                </tbody>
              </table>
              <p>These comparisons highlight how on-policy methods like SARSA avoid the triad but sacrifice exploration efficiency, while DQN tames it for off-policy learning.</p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-cogs"></i>
            6.2 DQN Innovations: Taming the Beast
          </summary>
          <div class="details-content">
              <h4>
                Innovation 1: Experience Replay
              </h4>
              <p>This technique addresses the problem of correlated, non-stationary data by creating a large dataset of past experiences, effectively making the RL problem more like supervised learning. It decorrelates transitions (reducing covariance between consecutive samples to near zero after sufficient mixing) and allows reuse of data for efficiency. Storage complexity is O(N) for N transitions, with O(1) push and O(batch_size) sampling time.</p>
              
              <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                    <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                    <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
                </div>
                <div class="mermaid">
                  graph LR
                    subgraph ExperienceReplay["Experience Replay Process"]
                        direction LR
                        A["Agent-Env Interaction"] -- "Generates" --> B["Experience<br>(s, a, r, s')"]
                        B -- "Store" --> C[("Replay Buffer<br>~1M transitions")]
                        C -- "Sample Random Minibatch" --> D[Minibatch]
                        D -- "Train On" --> E[Q-Network]
                    end
                    style C fill:#00ff9c, color:black
                    style E fill:#00f7ff, color:black
                </div>
              </div>
              <p>(Description: Imagine an animated flow where transitions enter the buffer, get mixed, and are sampled randomly, reducing correlation—visualize as a swirling pool of data points.)</p>
              
              <div class="admonition tip">
                  <span class="admonition-title">
                    <i class="fas fa-info-circle"></i>
                    Advanced Variant: Prioritized Experience Replay (PER)
                  </span>
                  <p>
                    Standard experience replay samples transitions uniformly. However, some experiences are more valuable for learning than others. <strong>Prioritized Experience Replay (PER)</strong> samples transitions with a probability $P(i) \propto |\delta_i|^\omega$, where $\delta_i$ is the TD error of transition $i$ and $\omega$ is an exponent that controls the degree of prioritization. To correct for the bias introduced by this non-uniform sampling, PER uses <strong>importance sampling weights</strong> during the gradient descent step: $w_i = (N \cdot P(i))^{-\beta}$, where $\beta$ is annealed from 0 to 1. This ensures unbiased gradient estimation, as the expected update matches uniform sampling.
                  </p>
              </div>
              
              <p>Another variant is <strong>Combined Experience Replay (CER)</strong>, which mixes a small number of recent transitions with random samples to improve performance in small-buffer scenarios. For sparse-reward environments, <strong>Hindsight Experience Replay (HER)</strong> relabels failed trajectories with achieved goals to create positive learning signals. Limitations include high memory overhead in very large state spaces, where buffer size N must balance storage and diversity.</p>
              
              <h4>
                Innovation 2: Fixed Target Network
              </h4>
              <p>This technique addresses the problem of non-stationary targets. By using a separate, periodically updated target network to generate the TD target, DQN makes the optimization process more stable, akin to supervised learning with fixed labels. It reduces "target chasing," where rapid updates to the same network cause oscillating targets.</p>
              
              <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                    <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                    <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
                </div>
                <div class="mermaid">
                    graph TD
                        OnlineNet["Online Network Q(s,a; θ)"] -- "Gradients update every step" --> OnlineNet
                        TargetNet["Target Network Q(s,a; θ⁻)"] -- "Weights frozen for C steps" --> TargetNet
                        
                        subgraph LossCalculation["Loss Calculation"]
                            OnlineNet -- "Predicts Q(s,a)" --> Loss((L))
                            TargetNet -- "Calculates Target y" --> Loss
                        end
                        
                        OnlineNet -- "Copy weights every C steps" --> TargetNet
                        
                        style OnlineNet fill:#00f7ff,color:black
                        style TargetNet fill:#f000ff,color:black
                </div>
              </div>
              <p>(Description: Visualize as an animated diagram where gradients flow to the online net, targets remain stable, and periodic copies sync the networks—highlight reduction in oscillation waves.)</p>
              
              <details>
                <summary><i class="fas fa-calculator"></i> Stability Proof via Contraction Mapping</summary>
                <div class="details-content">
                    <p>The update for the online network parameters $\theta_i$ at step $i$ aims to minimize the loss with respect to a target $y_i$ generated by a fixed target network $\theta^-$. This process can be viewed as applying an operator $\mathcal{T}$ to the Q-function, where the operator is defined by the target network. Because the target network $\theta^-$ is fixed, this operator is a contraction mapping with modulus $\gamma$ in the sup norm: $\|\mathcal{T} Q_1 - \mathcal{T} Q_2\|_\infty \leq \gamma \|Q_1 - Q_2\|_\infty$. Therefore, the sequence of Q-functions generated by this process is guaranteed to converge to the fixed point, preventing the oscillations seen in earlier attempts where a single network was used for both prediction and target generation. Convergence rate is O(γ^k) for k iterations.</p>
                </div>
              </details>

              <p>A common refinement is to use <strong>soft target updates (Polyak averaging)</strong>, where weights are updated slowly after each step: $\theta^- \leftarrow \tau\theta + (1-\tau)\theta^-$, for a small $\tau$ (e.g., 0.005). This provides smoother updates and better performance in practice. For faster credit assignment, extensions like multi-step targets can be used: $y_j = \sum_{k=0}^{n-1} \gamma^k r_{j+k} + \gamma^n \max_{a'} Q(s_{j+n}, a'; \theta^-)$. Choosing n involves a bias-variance trade-off: small n reduces variance but increases bias, and vice versa.</p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-list-ol"></i>
            6.3 The Complete DQN Algorithm
          </summary>
          <div class="details-content">
              <p>The full DQN algorithm integrates these innovations into a cohesive training loop. Below is a line-by-line explanation of the pseudocode, including time complexities (e.g., O(batch_size * |θ|) per network update, O(1) for buffer operations assuming efficient deque).</p>
              
              <table>
                <thead>
                  <tr><th>Line</th><th>Explanation</th><th>Complexity</th></tr>
                </thead>
                <tbody>
                  <tr><td>Initialize replay memory D</td><td>Sets up buffer for transitions</td><td>O(N)</td></tr>
                  <tr><td>Initialize networks θ and θ^-</td><td>Online for predictions, target for stable TD targets</td><td>O(|θ|)</td></tr>
                  <tr><td>for episode = 1, M</td><td>Main training loop over episodes</td><td>O(M * T)</td></tr>
                  <tr><td>ε-greedy action selection</td><td>Balances exploration/exploitation</td><td>O(|A|)</td></tr>
                  <tr><td>Store transition in D</td><td>Adds experience to buffer</td><td>O(1)</td></tr>
                  <tr><td>Sample minibatch</td><td>Random samples for training</td><td>O(batch_size)</td></tr>
                  <tr><td>Calculate y_j with target net</td><td>Stable TD target computation</td><td>O(batch_size * |θ|)</td></tr>
                  <tr><td>Gradient descent on loss</td><td>Updates online network</td><td>O(batch_size * |θ|)</td></tr>
                  <tr><td>Update target every C steps</td><td>Syncs networks periodically</td><td>O(|θ|)</td></tr>
                </tbody>
              </table>
              
              <pre><code>
Initialize replay memory D to capacity N
Initialize online Q-network with random weights θ
Initialize target network Q̂ with weights θ⁻ = θ

for episode = 1, M do:
    Initialize state s₁
    for t = 1, T do:
        // ACT
        With probability ε select a random action aₜ
        otherwise select aₜ = argmaxₐ Q(sₜ, a; θ)

        // INTERACT
        Execute action aₜ, observe reward rₜ and next state sₜ₊₁
        Store transition (sₜ, aₜ, rₜ, sₜ₊₁) in D

        // LEARN
        Sample random minibatch of transitions (sⱼ, aⱼ, rⱼ, sⱼ₊₁) from D
        
        // Calculate target using the target network
        if terminal sⱼ₊₁:
            yⱼ = rⱼ
        else:
            yⱼ = rⱼ + γ * maxₐ' Q̂(sⱼ₊₁, a'; θ⁻)

        // Perform a gradient descent step on the loss (e.g., Huber loss)
        // L = (yⱼ - Q(sⱼ, aⱼ; θ))²
        Update online network weights θ

        // Periodically update the target network
        Every C steps, set θ⁻ ← θ
    end for
end for
              </code></pre>
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Complete DQN Training and Evaluation on CartPole</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://github.com/rl-omnibus/dqn-cartpole" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
import gymnasium as gym
import math
import random
import matplotlib.pyplot as plt
from collections import namedtuple, deque
from itertools import count
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter  # For logging

# --- Environment Setup ---
env = gym.make("CartPole-v1")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

# --- Replay Memory (with optional PER extension in comments) ---
class ReplayMemory(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)
    def push(self, *args):
        self.memory.append(Transition(*args))
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)  # For PER: implement priority sampling here
    def __len__(self):
        return len(self.memory)

# --- DQN Model ---
class DQN(nn.Module):
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)

# --- Hyperparameters ---
BATCH_SIZE, GAMMA, LR = 128, 0.99, 1e-4
EPS_START, EPS_END, EPS_DECAY = 0.9, 0.05, 1000
TARGET_UPDATE, N_EPISODES, EVAL_EVERY = 10, 600, 50

n_actions = env.action_space.n
state, _ = env.reset()
n_observations = len(state)

policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())

optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = ReplayMemory(10000)
steps_done = 0
episode_durations, episode_losses, eval_returns = [], [], []

writer = SummaryWriter()  # For TensorBoard logging

def select_action(state):
    global steps_done
    sample = random.random()
    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1
    if sample > eps_threshold:
        with torch.no_grad():
            return policy_net(state).max(1)[1].view(1, 1)
    else:
        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)

def optimize_model(episode):
    if len(memory) < BATCH_SIZE: return 0
    transitions = memory.sample(BATCH_SIZE)
    batch = Transition(*zip(*transitions))

    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)
    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
    
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    state_action_values = policy_net(state_batch).gather(1, action_batch)
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        # Ablation flag: Set use_target=False to simulate without target network
        use_target = True
        if use_target:
            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]
        else:
            next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1)[0]
    
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    criterion = nn.SmoothL1Loss()
    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
    optimizer.step()
    
    writer.add_scalar('Loss/train', loss.item(), episode)
    return loss.item()

def evaluate_policy():
    eval_env = gym.make("CartPole-v1")
    total_return = 0
    for _ in range(10):  # Average over 10 episodes
        state, _ = eval_env.reset()
        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
        done = False
        episode_return = 0
        while not done:
            with torch.no_grad():
                action = policy_net(state).max(1)[1].view(1, 1)
            observation, reward, terminated, truncated, _ = eval_env.step(action.item())
            episode_return += reward
            state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)
            done = terminated or truncated
        total_return += episode_return
    return total_return / 10

# --- Training Loop ---
for i_episode in range(N_EPISODES):
    state, _ = env.reset()
    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
    episode_loss = 0
    num_updates = 0
    for t in count():
        action = select_action(state)
        observation, reward, terminated, truncated, _ = env.step(action.item())
        reward = torch.tensor([reward], device=device)
        done = terminated or truncated

        if terminated: next_state = None
        else: next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)

        memory.push(state, action, next_state, reward)
        state = next_state
        # Ablation flag: Set update=True to enable learning; False for random policy baseline
        update = True
        if update:
            loss = optimize_model(i_episode)
            if loss > 0:
                episode_loss += loss
                num_updates += 1

        if done:
            avg_loss = episode_loss / num_updates if num_updates > 0 else 0
            episode_durations.append(t + 1)
            episode_losses.append(avg_loss)
            writer.add_scalar('Duration/train', t + 1, i_episode)
            writer.add_scalar('Epsilon/train', EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY), i_episode)
            break
            
    if i_episode % TARGET_UPDATE == 0:
        target_net.load_state_dict(policy_net.state_dict())
    
    if i_episode % EVAL_EVERY == 0:
        avg_return = evaluate_policy()
        eval_returns.append(avg_return)
        writer.add_scalar('Eval/average_return', avg_return, i_episode)

writer.close()
print('Complete')

# --- Plotting Results ---
fig, axs = plt.subplots(3, 1, figsize=(10, 15))
axs[0].set_title('Episode Durations')
axs[0].plot(episode_durations, label='Duration')
durations_t = torch.tensor(episode_durations, dtype=torch.float)
if len(durations_t) >= 100:
    means = durations_t.unfold(0, 100, 1).mean(1).view(-1)
    means = torch.cat((torch.zeros(99), means))
    axs[0].plot(means.numpy(), label='100-episode moving avg', color='red')
axs[0].legend()
axs[0].grid(True)

axs[1].set_title('Average Loss per Episode')
axs[1].plot(episode_losses, label='Avg Loss')
axs[1].legend()
axs[1].grid(True)

axs[2].set_title('Evaluation Average Return (Greedy Policy)')
eval_episodes = range(0, N_EPISODES, EVAL_EVERY)
axs[2].plot(eval_episodes, eval_returns, label='Avg Return (10 eps)')
axs[2].legend()
axs[2].grid(True)

plt.tight_layout()
plt.show()
                </code></pre>
            </div>
            <p>(Note: For ablation studies, toggle flags like 'use_target' or 'update' in the code and compare plots—e.g., without target network, loss may explode; without updates, performance matches random baseline.)</p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-vial"></i>
            6.4 Ablation Studies and Empirical Insights
          </summary>
          <div class="details-content">
            <p>The original DQN paper (Mnih et al., 2015) included a thorough ablation study to demonstrate the importance of its two key innovations. When either experience replay or the target network was removed, performance on the Atari benchmark degraded significantly, with training often becoming unstable and diverging. This empirically validated the necessity of both components for stabilizing the learning process. For example, without replay, correlated updates led to forgetting; without targets, oscillating values caused policy instability.</p>
            <p>(Description: Additional plots could show reward curves for ablations—e.g., vanilla DQN reaches high scores quickly, while no-replay variants plateau low, and no-target variants diverge with exploding losses.)</p>
            <h4>Atari Benchmarks</h4>
            <p>DQN's performance was evaluated on 49 Atari 2600 games. The table below shows a sample of its normalized scores compared to a professional human player and a random agent, highlighting superhuman levels in many cases.</p>
            <table>
              <thead>
                <tr><th>Game</th><th>Random Agent Score</th><th>Human Pro Score</th><th>DQN Score</th><th>Frames to Mastery</th></tr>
              </thead>
              <tbody>
                <tr><td>Pong</td><td>-20.7</td><td>14.6</td><td>20.0</td><td>~10M</td></tr>
                <tr><td>Breakout</td><td>1.7</td><td>30.5</td><td>401.2</td><td>~50M</td></tr>
                <tr><td>Space Invaders</td><td>148</td><td>1,669</td><td>1,976</td><td>~20M</td></tr>
                <tr><td>Seaquest</td><td>68</td><td>28,010</td><td>5,286</td><td>~100M</td></tr>
                <tr><td>Enduro</td><td>0</td><td>860</td><td>2,098</td><td>~30M</td></tr>
                <tr><td>Q*bert</td><td>157</td><td>13,455</td><td>16,396</td><td>~40M</td></tr>
              </tbody>
            </table>
            <p>DQN achieved superhuman performance on many games, demonstrating its ability to learn complex control policies from high-dimensional, raw pixel data. However, it struggled in sparse-reward games like Montezuma's Revenge, where exploration is key—a counterexample to its limitations in long-horizon tasks.</p>
            <p>Empirical insights extend beyond gaming: in traffic signal optimization, DQN variants have reduced congestion by 20-30% in simulated cities, showcasing generalization to real-time control.</p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-question-circle"></i>
            6.5 Limitations, Extensions, and Open Questions
          </summary>
          <div class="details-content">
            <h4>Overestimation Bias and Double DQN</h4>
            <p>A key limitation of the standard DQN algorithm is its tendency to overestimate Q-values. This is because the $\max$ operator in the TD target update simultaneously uses the same network to both select the best action and evaluate that action's value. In toy MDPs like chain worlds with noisy rewards, vanilla DQN overestimates suboptimal actions, leading to greedy policies that fail to explore better paths—a clear counterexample of bias propagation.</p>
            <details>
                <summary><i class="fas fa-calculator"></i> Proof Sketch: Maximization Bias</summary>
                <div class="details-content">
                  <p>Let $X_1, \dots, X_k$ be random variables representing noisy estimates of the true action values, with $X_i = q_i + \epsilon_i$ where $\epsilon_i$ is zero-mean noise. Due to Jensen's inequality for the concave max function, the expectation of the maximum is greater than or equal to the maximum of the expectations:
                  $$\mathbb{E}[\max_i X_i] \ge \max_i \mathbb{E}[X_i]$$
                  <p>For example, if all $q_i = 0$ and $\epsilon_i \sim \mathcal{N}(0,1)$, $\mathbb{E}[\max X_i]$ is positive (e.g., ~1.16 for k=3), introducing a positive bias that propagates through bootstrapping and leads to suboptimal policies.</p>
                </div>
            </details>
            <p><strong>Double DQN</strong> solves this by decoupling action selection from action evaluation. It uses the online network to select the best action and the target network to evaluate its value. The Double DQN target is:</p>
            $$y_j^{\text{Double}} = r_j + \gamma Q_{\theta^-}(s_{j+1}, \arg\max_{a'} Q_{\theta}(s_{j+1}, a'))$$
            <p>This simple change dramatically reduces overestimation and often leads to better performance, as shown in van Hasselt et al. (2016).</p>
            
            <h4>Rainbow DQN</h4>
            <p>Rainbow DQN (Hessel et al., 2018) is not a single new idea, but an integration of several successful improvements to the DQN algorithm. It combines Double DQN, Prioritized Experience Replay, Dueling Networks (see <a href="#part7">Part 7</a>), Multi-step Learning, Distributional RL, and Noisy Nets into a single agent, achieving state-of-the-art performance on the Atari benchmark with up to 50% score improvements.</p>
            <table>
              <thead>
                <tr><th>Component</th><th>Improvement</th><th>Performance Gain on Atari</th></tr>
              </thead>
              <tbody>
                <tr><td>Double DQN</td><td>Reduces overestimation</td><td>+10-20%</td></tr>
                <tr><td>PER</td><td>Faster learning from important transitions</td><td>+15%</td></tr>
                <tr><td>Dueling</td><td>Separates value and advantage</td><td>+10%</td></tr>
                <tr><td>Multi-step</td><td>Better credit assignment</td><td>+5-10%</td></tr>
                <tr><td>Distributional</td><td>Models return distribution</td><td>+20%</td></tr>
                <tr><td>Noisy Nets</td><td>Parameterized exploration</td><td>+5%</td></tr>
              </tbody>
            </table>

            <h4>Open Questions</h4>
            <ul>
              <li><strong>Continuous Actions:</strong> How can DQN, which relies on a $\max$ over a discrete action set, be effectively scaled to continuous action spaces without discretization? (E.g., via normalizing flows or actor-critic hybrids.)</li>
              <li><strong>Data Efficiency:</strong> Despite its successes, DQN requires millions of environmental steps to learn. Can self-supervised pre-training or better world models reduce this data hunger?</li>
              <li><strong>Uncertainty and Safety:</strong> How can DQN incorporate uncertainty quantification (e.g., via ensembles) for safer decision-making in robotics?</li>
              <li><strong>Non-Visual Inputs:</strong> What architectures beyond CNNs (e.g., graph networks) optimize DQN for structured, non-visual data like graphs or text?</li>
            </ul>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-cogs"></i>
            6.6 Quantitative Analyses and Real-World Applications
          </summary>
          <div class="details-content">
            <h4>Quantitative Analysis</h4>
            <p>While theoretical regret bounds for deep RL are complex and an active area of research (e.g., no tight bounds due to non-convexity), empirically, DQN and its variants demonstrate sub-linear regret in practice on many benchmarks. For instance, in CartPole, regret grows as O(√T) after initial exploration, but sample efficiency remains a challenge: DQN required approximately 200 million frames of gameplay (equivalent to 38 days at human speed) to master the Atari suite. Rainbow DQN improves this to ~100M frames via combined extensions.</p>
            <p>(Description: A plot of cumulative regret vs. steps would show vanilla DQN's sub-linear curve flattening, while ablations (no replay) exhibit linear growth due to stagnation.)</p>
            <h4>Case Study: Data Center Cooling</h4>
            <p>One of the most impactful real-world applications of DQN was at Google's data centers. An RL agent, built on DQN principles, was trained to control over 120 variables in the cooling infrastructure (fans, pumps, etc.). By learning a control policy that optimized for energy efficiency, the agent achieved a 40% reduction in cooling energy, translating to a 15% improvement in overall power usage efficiency (PUE) and saving hundreds of millions of dollars (Lazic et al., 2018).</p>
            <h4>Case Study: Robotic Manipulation</h4>
            <p>In robotics, DQN has been applied to dexterous manipulation tasks, such as OpenAI's Dactyl system for in-hand object reorientation. Using simulation-to-real transfer, DQN learned policies that generalized to physical robots, achieving success rates of 80-90% on complex tasks like rotating a block. However, a counterexample is its struggle in highly sparse-reward robotics environments, where vanilla DQN fails without extensions like HER, leading to zero progress.</p>
            <h4>Case Study: Healthcare Treatment Optimization</h4>
            <p>DQN variants have been used in personalized medicine, such as optimizing sepsis treatment protocols in ICUs. By modeling patient states from electronic health records, DQN learned policies that improved survival rates by 2-4% over clinicians in retrospective studies (Komorowski et al., 2018), demonstrating potential for data-driven healthcare decisions.</p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-forward"></i>
            6.7 Recent Advancements and Future Directions
          </summary>
          <div class="details-content">
            <p>Recent advancements have built on DQN to address its limitations. Transformer-based hybrids, such as Decision Transformer (Chen et al., 2021), integrate DQN with sequence modeling, using attention to handle long-horizon dependencies for tasks like offline RL. Self-supervised pretraining, as in Fan et al. (2020), uses contrastive learning on unlabeled data to reduce DQN's sample needs by 10-50x in vision-based tasks.</p>
            <p>Future directions include combining DQN with world models (e.g., DreamerV3 for planning) and uncertainty-aware methods (e.g., ensemble DQNs for epistemic uncertainty in safety-critical systems). These aim to make DQN more efficient and robust, influencing areas like autonomous driving and drug discovery.</p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale"></i>
            6.8 Comparative Analysis with Other Methods
          </summary>
          <div class="details-content">
            <p>DQN excels in discrete-action, high-dimensional state spaces but differs from other RL paradigms. Below is a pros/cons comparison:</p>
            <table>
              <thead>
                <tr><th>Method</th><th>Pros</th><th>Cons</th><th>Use Cases</th></tr>
              </thead>
              <tbody>
                <tr><td>DQN</td><td>Stable off-policy learning, sample reuse</td><td>Discrete actions only, overestimation</td><td>Games, discrete control</td></tr>
                <tr><td>PPO (Policy Gradient)</td><td>Continuous actions, stable on-policy</td><td>Sample-inefficient, no replay</td><td>Robotics, continuous tasks</td></tr>
                <tr><td>SAC (Actor-Critic)</td><td>Entropy regularization for exploration</td><td>Complex hyperparameters</td><td>Continuous, stochastic environments</td></tr>
              </tbody>
            </table>
            <p>For discrete tasks, DQN often outperforms policy gradients in sample efficiency due to replay, but in continuous spaces, actor-critic methods like SAC are preferred. Hybrids, such as DQN with continuous extensions (e.g., via normalizing flows), are emerging to bridge gaps.</p>
          </div>
        </details>
        
<section id="part7">
        <h2 id="part7-title">
          <i class="fas fa-wand-magic-sparkles"></i>
          Part 7: Advanced DRL Architectures and Algorithms
        </h2>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-chess-queen"></i>
            7.1 Dueling DQN: Decoupling Value and Advantage
          </summary>
          <div class="details-content">
            <p>
              While DQN was a monumental breakthrough, its architecture is somewhat monolithic. It estimates the Q-value for each action as a single, opaque number. However, intuitively, the value of a state-action pair $Q(s,a)$ can be decomposed. In many states, the value of simply <em>being in that state</em> is far more critical than the relative value of each available action. This insight draws from earlier work in RL, such as the advantage actor-critic methods, but Dueling DQN applies it directly to value-based learning.
            </p>
            <p>
              The <strong>Dueling Network Architecture</strong>, introduced in 2016 by Wang et al., is a powerful modification to the DQN architecture that explicitly models this decomposition. It was designed on the insight that separating the estimation of "how good is this state?" from "how much better is this action?" can lead to faster and more robust learning, especially in environments with many similar-valued actions.
            </p>

            <h4>
              The Core Intuition: Value vs. Advantage
            </h4>
            <p>
              Let's formalize the relationship between the Q-function, the state-value function ($V(s)$), and a new quantity, the <strong>advantage function</strong> ($A(s, a)$). The advantage function measures how much better taking action $a$ is compared to the average action possible in state $s$. It is formally defined as:
            </p>
            $$A^\pi(s, a) \doteq Q^\pi(s, a) - V^\pi(s)$$
            <p>Rearranging this, we can express the Q-function as a sum of the state value and the action advantage:</p>
            $$Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)$$
            <p>
              The Dueling DQN architecture is built on the hypothesis that directly modeling both $V(s)$ and $A(s, a)$ separately and then combining them is more effective than estimating $Q(s, a)$ directly. In many situations, the choice of action is irrelevant. For example, when driving on an empty highway, it doesn't matter much whether you steer slightly left or right; the value of the state is high regardless. The Dueling architecture allows the network to learn the state's value ($V(s)$) without having to learn the effect of every action for that state. This decomposition also aids in environments with redundant actions, reducing variance in updates.
            </p>
            
            <h4>
               The Two-Stream Architecture
            </h4>
            <p>
              The Dueling architecture modifies the end of a standard DQN. After the initial convolutional layers that act as a shared feature extractor, the network splits into two separate, fully-connected streams:
            </p>
            <ol>
              <li><strong>The Value Stream</strong>: This stream takes the features from the convolutional base and outputs a <strong>single scalar value</strong>. This is our estimate of the state-value function, $V(s; \theta, \alpha)$.</li>
              <li><strong>The Advantage Stream</strong>: This stream also takes the features from the shared base but outputs a <strong>vector with $|A|$ values</strong>. Each output corresponds to the estimated advantage for taking the corresponding action, $A(s, a; \theta, \beta)$.</li>
            </ol>
            
            <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                    <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                    <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
                </div>
                <div class="mermaid">
                    graph TD
                        Input[State Input] --> ConvBase[Shared CNN Base]
                        
                        ConvBase --> ValueStream_FC[FC Layer] --> ValueStream_Out("V(s)<br/>(Scalar)")
                        ConvBase --> AdvStream_FC[FC Layer] --> AdvStream_Out("A(s,a)<br/>(|A|-dim Vector)")
                        
                        subgraph AggregationLayer["Aggregation"]
                            ValueStream_Out --> Combine
                            AdvStream_Out --> Combine
                        end
                        
                        Combine --> Output["Q(s,a)<br/>(|A|-dim Vector)"]

                        subgraph ValueHead["Value Stream"]
                            ValueStream_FC
                            ValueStream_Out
                        end
                        
                        subgraph AdvantageHead["Advantage Stream"]
                            AdvStream_FC
                            AdvStream_Out
                        end

                        style ValueHead fill:#161930,stroke:#00f7ff
                        style AdvantageHead fill:#161930,stroke:#f000ff
                </div>
            </div>


            <h4 id="part7-dueling-aggregation">
              The Aggregation Layer and the Identifiability Problem
            </h4>
            <p>
              Now we have two outputs, $V(s)$ and a vector of $A(s, a)$ values. How do we combine them to get our final Q-values? The naive approach would be to simply add them together: $Q(s, a) = V(s) + A(s, a)$.
            </p>
            
            <div class="admonition danger">
              <span class="admonition-title">
                <i class="fas fa-exclamation-triangle"></i>
                The Identifiability Problem
              </span>
              <p>
                This naive aggregation has a serious flaw. We are trying to learn two quantities ($V$ and $A$) from a single target signal ($Q$). The network can learn to produce the correct Q-values in a way that is meaningless. For example, given an optimal Q-function, the decomposition into V and A is not unique. If $V(s) = 10$ and $A(s,a) = [1, 5]$, the Q-values are $[11, 15]$. But we get the exact same Q-values if $V(s) = 12$ and $A(s,a) = [-1, 3]$. The network can add an arbitrary constant to $V(s)$ and subtract it from all advantage values, and the resulting Q-values will be unchanged. This is the <strong>identifiability problem</strong>, and this ambiguity can lead to poor performance and instability during training.
              </p>
            </div>
            
            <p>
              To solve this, we must constrain the advantage function. The most common and stable solution is to force the advantage of the chosen action to be zero, or more robustly, to subtract the mean of the advantages from all advantage values. This anchors the scale of the advantages. Another option is subtracting the max advantage, but mean subtraction is more stable as it preserves relative differences.
            </p>
            <p>The corrected aggregation layer formula is:</p>
            $$Q(s, a) = V(s) + \left( A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a' \in \mathcal{A}} A(s, a') \right)$$
            <p>
              This formula ensures that for a given state $s$, the sum of the advantages is always zero, which uniquely pins down the values for $V(s)$ and $A(s, a)$ and resolves the identifiability issue. Proof of identifiability: suppose two decompositions Q = V1 + A1 = V2 + A2 with mean(A1) = mean(A2) = 0. Then V1 - V2 = A2 - A1, but averaging over actions gives V1 - V2 = 0, implying uniqueness.
            </p>
            
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Dueling DQN Implementation</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://github.com/rl-omnibus/advanced-drl" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
import torch
import torch.nn as nn

class DuelingDQN(nn.Module):
    def __init__(self, state_dim, n_actions):
        super(DuelingDQN, self).__init__()
        
        # Shared feature learning layers
        self.feature_layer = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU()
        )
        
        # Value stream
        self.value_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1) # Outputs a single scalar V(s)
        )
        
        # Advantage stream
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions) # Outputs a vector A(s,a) for each action
        )

    def forward(self, x):
        features = self.feature_layer(x)
        
        values = self.value_stream(features)
        advantages = self.advantage_stream(features)
        
        # Combine the streams
        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a')))
        q_values = values + (advantages - advantages.mean(dim=1, keepdim=True))
        
        return q_values

# Example usage for CartPole
state_dim = 4
n_actions = 2
model = DuelingDQN(state_dim, n_actions)
print(model)

# Test with a dummy input batch
dummy_input = torch.randn(32, state_dim) # Batch of 32 states
output = model(dummy_input)
print(f"\nInput shape: {dummy_input.shape}")
print(f"Output shape: {output.shape}") # Should be (32, 2)
                </code></pre>
            </div>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-ghost"></i>
            7.2 Munchausen RL: Bootstrapping with Self-Awareness
          </summary>
          <div class="details-content">
            <h4>
              The Problem: Q-Value Overestimation
            </h4>
            
            <div class="admonition danger">
              <span class="admonition-title">
                <i class="fas fa-arrow-trend-up"></i>
                The Maximization Bias
              </span>
              <p>
                A common and pernicious problem in Q-learning algorithms is <strong>Q-value overestimation</strong>. Because the Bellman update uses a $\max$ operator ($\max_{a'} Q(s', a')$), any small, positive estimation error in the Q-values of the next state can get systematically selected and propagated. This was addressed by Double DQN (covered in <a href="#part6">Part 6</a>), but other forms of bias can remain, especially in stochastic environments.
              </p>
            </div>
            <p>
              <strong>Munchausen Reinforcement Learning (M-RL)</strong>, introduced in 2020 by Vieillard et al., is a recent and powerful technique designed to address this issue while also encouraging better exploration. It does this by "pulling on its own bootstraps"—augmenting the immediate reward with information about the agent's <em>own current policy</em>. This draws from entropy-regularized RL, providing a unified way to stabilize and improve value-based methods.
            </p>

            <h4>
              The Core Mechanism: Augmenting the Reward with the Log-Policy
            </h4>
            <p>
              M-RL's central mechanism is remarkably simple yet profound: it modifies the target value calculation by adding the scaled logarithm of the agent's current policy probability to the immediate reward. This acts as a form of implicit regularization and entropy bonus.
            </p>
            <p>The standard TD Target is: $y_t = r_t + \gamma \max_{a'} Q(s_{t+1}, a')$.</p>
            <p>The <strong>Munchausen augmented reward</strong> for a transition $(s_t, a_t, r_t)$ is:</p>
            $$r'_t = r_t + \alpha \log \pi_{\theta}(a_t|s_t)$$
            <p>
              Here, $\pi_{\theta}(a_t|s_t)$ is the probability of taking the action $a_t$ in state $s_t$ according to the current policy, and $\alpha$ is a small hyperparameter that controls the strength of this regularization. The policy $\pi$ is typically derived from the Q-values using a softmax function: $\pi(a|s) = \text{softmax}(Q(s,a)/\tau)$, where $\tau$ is a temperature parameter controlling exploration.
            </p>
            <p>The full <strong>M-DQN Target Value</strong> is then:</p>
             $$y_t^{M-DQN} = (r_t + \alpha \log \pi_{\theta}(a_t|s_t)) + \gamma \sum_{a' \in \mathcal{A}} \pi_{\theta^-}(a'|s_{t+1}) Q_{\theta^-}(s_{t+1}, a')$$
             
            <details>
              <summary><i class="fas fa-calculator"></i> Derivation from Entropy-Regularized RL</summary>
              <div class="details-content">
                <p>M-RL derives from soft Q-learning, where the optimal policy maximizes expected return plus entropy: $J(\pi) = \mathbb{E}[\sum_t r_t + \alpha H(\pi(\cdot|s_t))]$. The soft Bellman equation is $Q(s,a) = r + \gamma \mathbb{E}_{s'}[V(s')]$, with $V(s) = \alpha \log \sum_a \exp(Q(s,a)/\alpha)$. Taking log and rearranging gives the augmented reward form, showing M-RL as an efficient approximation that adds self-aware penalties to stabilize updates.</p>
              </div>
            </details>
            
            <div class="admonition info">
                <span class="admonition-title">
                  <i class="fas fa-lightbulb"></i>
                  Intuition Behind the Munchausen Term
                </span>
                <p>Since the probability $\pi(a|s)$ is between 0 and 1, its logarithm is always negative or zero. Therefore, the term $\alpha \log \pi$ is a <strong>penalty</strong>.</p>
                <ul>
                  <li>If the agent takes an action that its policy considers very <strong>unlikely</strong> (a large exploration step), $\pi$ is small, so $\log \pi$ is a large negative number, and the penalty is large. This "cools" the TD target, preventing the agent from becoming overconfident based on a single, lucky, but out-of-distribution experience.</li>
                  <li>If the agent takes an action that its policy considers very <strong>likely</strong> (an exploitation step), $\pi$ is close to 1, so $\log \pi$ is close to zero, and the penalty is small.</li>
                </ul>
                <p>This grounds the Q-value updates in the agent's current policy, preventing drastic changes and promoting stability. It encourages the agent to not just find high-reward actions, but to find actions that it can commit to with high probability, effectively adding an entropy bonus for exploration.</p>
            </div>

            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Munchausen-DQN Target Calculation</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://github.com/rl-omnibus/advanced-drl" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
import torch
import torch.nn.functional as F

def calculate_m_dqn_loss(policy_net, target_net, batch, gamma, alpha, tau):
    """
    Calculates the loss for a batch of transitions for Munchausen-DQN.
    Assumes batch is a tuple: (states, actions, next_states, rewards, dones)
    """
    states, actions, next_states, rewards, dones = batch

    # --- Calculate Q-values for the current state ---
    q_values = policy_net(states)
    state_action_values = q_values.gather(1, actions)

    # --- Calculate the Munchausen augmented reward ---
    with torch.no_grad():
        # Get current policy from the online network's Q-values
        current_policy = F.softmax(q_values / tau, dim=1)
        # Get log-probability of the action that was actually taken
        current_action_log_probs = torch.log(current_policy.gather(1, actions))
        
        # Clip for stability
        clipped_log_probs = torch.clamp(current_action_log_probs, min=-20, max=0)

        # Augment the reward
        augmented_rewards = rewards + alpha * clipped_log_probs
    
    # --- Calculate the soft V-value of the next state ---
    with torch.no_grad():
        next_q_values = target_net(next_states)
        next_policy = F.softmax(next_q_values / tau, dim=1)
        next_log_policy = torch.log(next_policy)
        
        # V_soft(s') = sum_a' [ pi(a'|s') * (Q(s',a') - tau * log pi(a'|s')) ]
        # This is the expected value under the soft policy
        soft_next_v = (next_policy * (next_q_values - tau * next_log_policy)).sum(dim=1).unsqueeze(-1)
        
        # If the state is terminal, the next value is 0
        soft_next_v = soft_next_v * (1 - dones.float())

        # Calculate the final target value
        target_values = augmented_rewards + gamma * soft_next_v

    # --- Calculate Loss (with optional temperature scaling) ---
    loss = F.smooth_l1_loss(state_action_values, target_values)  # Huber loss for stability
    return loss
                </code></pre>
            </div>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-vial"></i>
            7.3 Empirical Insights and Ablation Studies
          </summary>
          <div class="details-content">
            <p>Dueling DQN was evaluated on the Atari suite, showing 5-10% score improvements over vanilla DQN, with faster convergence in action-redundant games. Ablations confirmed the mean subtraction's role: without it, identifiability issues caused 20-30% performance drops due to unstable advantages.</p>
            <p>M-RL, tested on Atari and MuJoCo, achieved state-of-the-art results, e.g., outperforming Rainbow DQN by reducing overestimation in noisy environments. Ablations showed the log-policy term essential, with removal leading to divergence in high-variance tasks.</p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-forward"></i>
            7.4 Limitations, Extensions, and Open Questions
          </summary>
          <div class="details-content">
            <h4>Limitations and Counterexamples</h4>
            <p>Dueling DQN assumes advantages are mean-zero, which may not hold in asymmetric environments (e.g., one dominant action skews means, leading to underestimation). M-RL's temperature τ requires careful tuning; too high causes excessive exploration, failing in precise-control tasks like robotic assembly.</p>
            <h4>Extensions</h4>
            <p>Combine Dueling with M-RL for decomposed, regularized estimates. Extend to continuous actions via normalizing flows or actor-critics.</p>
            <h4>Open Questions</h4>
            <ul>
              <li>How can dueling architectures incorporate temporal hierarchies for long-horizon planning?</li>
              <li>Can Munchausen's self-regularization be adapted for multi-agent settings with non-stationary policies?</li>
              <li>What are the theoretical convergence guarantees for these advanced variants in non-MDP settings?</li>
            </ul>
          </div>
        </details>
      </section>
      <hr/>
<section id="part8">
        <h2 id="part8-title">
          <i class="fas fa-industry"></i>
          Part 8: Applied Deep Reinforcement Learning
        </h2>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-boxes-stacked"></i>
            8.1 Application: Perishable Inventory Management
          </summary>
          <div class="details-content">
            <p>
              A classic problem in operations research is <strong>perishable inventory management</strong>. The goal is to decide how much inventory of a product to order at each time step to meet uncertain customer demand, while facing the dual challenges of product spoilage and lead times for new orders. Reinforcement Learning is exceptionally well-suited for this kind of stochastic optimization problem, which has been a focus since the earliest days of operations research with models like the Economic Order Quantity (EOQ) in 1913. However, unlike EOQ which assumes constant demand, RL can handle stochasticity, non-stationarity, and complex dynamics, often reducing waste by 20-30% in simulations compared to heuristics.
            </p>

            <h4>
              The MDP Formulation
            </h4>
            
            <p>The full transition dynamics are probabilistic: P(s'|s,a) incorporates stochastic demand D ~ Poisson(λ) or empirical distributions, aging of inventory (i_{t+1,j} = i_{t,j-1} for j>1), arrivals (o_{t+1,1} = a, o_{t+1,j} = o_{t,j-1}), and sales/spoilage. Optimal policies exist in infinite-horizon discounted MDPs, but computing them analytically is intractable for large states; RL approximates via function approximation (see <a href="#part5">Part 5</a>).</p>
            
            <h5>State ($S_t$)</h5>
            <p>The state must capture all information necessary to make an optimal ordering decision. For a perishable product, a sufficient state representation is a vector concatenating on-hand inventory by age and in-transit orders by arrival time:</p>
            $$S_t = [\underbrace{i_{t,1}, i_{t,2}, \dots, i_{t,m-1}}_{\text{On-hand by age}}, \underbrace{o_{t,1}, o_{t,2}, \dots, o_{t,L}}_{\text{In-transit by arrival time}}]$$
            <p>For continuous variants, include demand forecasts or vectorized multi-product states (e.g., for K items, dim = K*(m+L-1)).</p>

            <h5>Action ($A_t$)</h5>
            <p>The action is the quantity of new product to order. While this can be a discrete set (suitable for DQN from <a href="#part6">Part 6</a>), more advanced agents using algorithms like SAC can handle <strong>continuous action spaces</strong> for more fine-grained ordering decisions, with constraints like budget limits (A_t ≤ B - sum(o)).</p>

            <h5>Reward ($R_{t+1}$)</h5>
            <p>The goal is to minimize total costs, so the reward is the <strong>negative of total costs</strong>. The expected shortage cost can be derived from the demand distribution (e.g., Poisson):</p>
            $$R_{t+1} = - \left( c_h \cdot (\text{inventory held}) + c_s \cdot \mathbb{E}[\max(0, D - I)] + c_w \cdot (\text{spoiled items}) + c_o \cdot A_t \right)$$
            <p>Shaping can add bonuses for low variance in stock levels.</p>

            <div class="admonition info">
              <span class="admonition-title">
                <i class="fas fa-chart-bar"></i>
                Connection to Operations Research: Why RL?
              </span>
              <p>Traditional heuristics include the <strong>(s, S) policy</strong>: when inventory drops below reorder point `s`, order up to level `S`. While optimal under certain assumptions, these policies are often outperformed by RL agents in complex, non-stationary environments. An RL agent can learn a highly nuanced, state-dependent policy directly from data without assuming a specific demand distribution. Benchmarks show RL reducing costs by 10-25% over (s,S) in stochastic simulations with variable lead times.</p>
              <table>
                <thead>
                  <tr><th>Method</th><th>Average Cost</th><th>Waste Reduction</th></tr>
                </thead>
                <tbody>
                  <tr><td>(s,S) Heuristic</td><td>450</td><td>Baseline</td></tr>
                  <tr><td>DQN</td><td>380</td><td>15%</td></tr>
                  <tr><td>SAC (Continuous)</td><td>340</td><td>24%</td></tr>
                </tbody>
              </table>
            </div>

            <details>
              <summary><i class="fas fa-exclamation-triangle"></i> Limitations and Counterexamples</summary>
              <div class="details-content">
                <p>RL may overfit to simulated demand; in highly volatile markets (e.g., fashion with trend shifts), naive RL fails without meta-learning, leading to stockouts (counterexample: 50% higher costs vs. adaptive heuristics).</p>
              </div>
            </details>

            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Inventory Management with DQN</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://github.com/rl-omnibus/applied-rl" target="_blank" class="colab-button" title="View on GitHub">
                           <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt

# Full Gym-compatible environment
class PerishableInventoryEnv(gym.Env):
    def __init__(self, max_age=3, lead_time=2, max_order=20, demand_lambda=10):
        self.max_age = max_age
        self.lead_time = lead_time
        self.action_space = gym.spaces.Discrete(5)  # Order 0,5,10,15,20
        self.observation_space = gym.spaces.Box(low=0, high=100, shape=(max_age + lead_time - 1,))
        self.inventory = np.zeros(max_age - 1)  # Ages 1 to m-1
        self.pipeline = np.zeros(lead_time)     # Orders arriving in 1 to L periods
        self.demand_lambda = demand_lambda
        self.holding_cost = 0.5
        self.shortage_cost = 5.0
        self.wastage_cost = 2.0
        self.order_cost = 1.0

    def reset(self, **kwargs):
        self.inventory = np.zeros(len(self.inventory))
        self.pipeline = np.zeros(len(self.pipeline))
        return self._get_state(), {}

    def _get_state(self):
        return np.concatenate([self.inventory, self.pipeline])

    def step(self, action):
        order_quantity = action * (self.action_space.n - 1)  # 0 to max_order
        arrived = self.pipeline[0]
        self.pipeline = np.roll(self.pipeline, -1)
        self.pipeline[-1] = order_quantity
        
        self.inventory = np.roll(self.inventory, -1)
        self.inventory[-1] = arrived
        wasted = self.inventory[-1]  # Oldest age spoils if not sold
        
        demand = np.random.poisson(self.demand_lambda)
        
        total_inventory = np.sum(self.inventory)
        fulfilled = min(demand, total_inventory)
        shortage = demand - fulfilled
        
        remaining = total_inventory - fulfilled
        self.inventory = np.full(len(self.inventory), remaining / len(self.inventory))  # Simplified for demo
        
        holding = self.holding_cost * remaining
        shortage_c = self.shortage_cost * shortage
        wastage_c = self.wastage_cost * wasted
        order_c = self.order_cost * order_quantity
        reward = - (holding + shortage_c + wastage_c + order_c)
        
        return self._get_state(), reward, False, False, {}

# For training, integrate with DQN from Part 6
# Conceptual plotting
episodes = np.arange(1000)
conceptual_rewards = -500 + 300 * (1 - np.exp(-episodes / 200)) + np.random.normal(0, 20, 1000)
plt.figure(figsize=(10,5))
plt.plot(episodes, conceptual_rewards)
plt.title('Conceptual RL Learning Curve for Inventory Management')
plt.xlabel('Episode')
plt.ylabel('Average Reward (Negative Cost)')
plt.grid(True)
plt.show()
                </code></pre>
            </div>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-robot"></i>
            8.2 Application: Robot Path Planning with Potential Fields
          </summary>
          <div class="details-content">
            <p>
              Guiding a robot from a start to a goal in an environment with obstacles is a classic robotics problem, pioneered by methods like Artificial Potential Fields (APF) in Khatib's 1986 work. A key challenge in applying RL is designing a reward function that provides smooth and effective guidance. A sparse reward (+1 at goal, -1 for crash) would make learning prohibitively slow, requiring millions of episodes in cluttered spaces.
            </p>

            <h4>
              The Core Idea: Physics-Based Reward Engineering
            </h4>
            <p>
              The APF method treats the robot as a point particle in a field of forces. The goal exerts an <strong>attractive force</strong>, while obstacles exert a <strong>repulsive force</strong>. The total potential is U = U_att + U_rep, with attractive U_att = (1/2) ξ d_goal^2 (quadratic for smooth pull) and repulsive U_rep = (1/2) η (1/d_obs - 1/ρ)^2 if d_obs < ρ, else 0. The negative gradient $-\nabla U(q)$ gives a force vector for classical control.
            </p>
            <p>For RL, we use the potential function to engineer a dense reward. The change in potential energy from moving $q \to q'$ gives a natural reward signal: $R_t = U(q_t) - U(q_{t+1})$. This reward is positive when the agent moves "downhill" towards lower potential energy. In continuous spaces, use SAC or PPO; for discrete, DQN with grid actions.</p>
            
            <details>
              <summary><i class="fas fa-calculator"></i> Derivation of Local Minima Issue</summary>
              <div class="details-content">
                <p>At equilibrium, $\nabla U = 0$, so F_att + F_rep = 0. In U-shaped obstacles, attractive and repulsive gradients can cancel, creating saddle points where progress stalls (proof: Hessian positive in some directions, negative in others).</p>
              </div>
            </details>
        
            <div class="admonition info">
              <span class="admonition-title">
                <i class="fas fa-lightbulb"></i>
                APF and RL: A Powerful Combination
              </span>
              <p>A classic problem with the pure APF method is that it can get stuck in <strong>local minima</strong> where attractive and repulsive forces balance out. An RL agent, with its inherent exploration strategy (like $\epsilon$-greedy or entropy regularization), has a natural mechanism to "kick" itself out of these local minima and continue searching for the true global optimum path. This is a key advantage of using APF within an RL framework. Recent advancements like learned potentials via GNNs further improve robustness.</p>
            </div>
            
            <details>
              <summary><i class="fas fa-exclamation-triangle"></i> Counterexample: APF Failure in Dynamic Environments</summary>
              <div class="details-content">
                <p>In environments with moving obstacles (e.g., crowded warehouses), static APF traps the robot in local minima; RL with APF escapes via learned exploration but may oscillate if repulsive η is too high, causing jittery paths.</p>
              </div>
            </details>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale-right"></i>
            8.3 Comparative Analysis and Benchmarks
          </summary>
          <div class="details-content">
            <h4>RL vs. Traditional Planners</h4>
            <p>For path planning, traditional algorithms like A* are guaranteed to find the shortest path if one exists. So why use RL?</p>
            <table>
              <thead><tr><th>Aspect</th><th>A* Search</th><th>Reinforcement Learning</th></tr></thead>
              <tbody>
                <tr><td>**Optimality**</td><td>Guaranteed optimal path.</td><td>Finds a good policy, which may be near-optimal.</td></tr>
                <tr><td>**Environment Model**</td><td>Requires a perfect, known map.</td><td>Can learn from interaction without a prior map (model-free).</td></tr>
                <tr><td>**Adaptability**</td><td>Must re-plan from scratch if the environment changes.</td><td>A learned policy can generalize and react to dynamic changes.</td></tr>
                <tr><td>**Computational Cost**</td><td>Can be high for large, complex maps.</td><td>High upfront training cost, but very fast inference (action selection).</td></tr>
              </tbody>
            </table>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-question-circle"></i>
            8.4 Open Challenges in Applied RL
          </summary>
          <div class="details-content">
            <ul>
              <li><strong>Sim-to-Real Transfer:</strong> Policies trained in simulation often fail in the real world due to small differences in physics and sensor readings (the "reality gap"). Techniques like domain randomization are an active area of research to bridge this gap.</li>
              <li><strong>Safety and Ethics:</strong> How can we ensure an RL agent acts safely during exploration and deployment? For inventory, this raises ethical questions about managing critical supplies like medicine or minimizing food waste in global supply chains.</li>
              <li><strong>Scalability to Multi-Agent Systems:</strong> Extending to cooperative inventory (e.g., supply chains) or multi-robot planning requires handling non-stationarity—open for MARL advancements.</li>
            </ul>
          </div>
        </details>
      </section>
      <hr/>
      <!-- ============================================================ -->
      <!-- Part 9: Foundations & Markov Decision Processes -->
      <section id="part9">
        <h2 id="part9-title">
          <i class="fas fa-graduation-cap"></i>
          Part 9: Foundations &amp; Markov Decision Processes
        </h2>

        <!-- Introduction: Paradigms of Learning -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-random"></i>
            9.1 Markov Reward Processes &amp; Value Evaluation
          </summary>
          <div class="details-content">
            <p>
              In reinforcement learning the agent interacts with an environment by choosing actions, observing the next state and receiving rewards.  When the policy is fixed, this interaction reduces to a <strong>Markov reward process (MRP)</strong>.  An MRP is defined by a tuple $(S, P, R, \gamma)$: a set of states $S$, a transition matrix $P(s'|s)$ specifying the probability of moving to $s'$ from $s$, a reward function $R(s)$ giving the expected immediate reward in state $s$, and a discount factor $\gamma\in[0,1)$.  The process is <em>Markovian</em>, meaning the next state depends only on the current state, not on the full history.
            </p>
            <p>
              The goal in an MRP is to compute the <strong>state–value function</strong> $V(s)$, which represents the expected discounted return from state $s$ when following the fixed policy implicitly encoded by $P$ and $R$:
              $$V(s) = \mathbb{E}\Big[ \sum_{t=0}^{\infty} \gamma^t R(S_t) \,\Big|\, S_0 = s \Big].$$
              This value function satisfies the linear <em>Bellman expectation equation</em>
              $$V(s) = R(s) + \gamma \sum_{s' \in S} P(s' \mid s)\,V(s'),$$
              which states that the value of a state equals its immediate reward plus the discounted value of its successor states.  Solving this system of linear equations yields $V$ exactly; for small MRPs this can be done by matrix inversion.
            </p>
            <p>
              Below is a simple Python function to compute $V$ for a finite MRP via iterative policy evaluation:
            </p>
<pre><code class="language-python">import numpy as np

def evaluate_mrp(P, R, gamma=0.9, tol=1e-6):
    """Compute value function for a Markov reward process.

    Args:
        P: transition matrix of shape (n_states, n_states)
        R: reward vector of length n_states
        gamma: discount factor
        tol: convergence tolerance

    Returns:
        Value vector V such that V = R + γ P V.
    """
    n = len(R)
    V = np.zeros(n)
    delta = float('inf')
    while delta &gt; tol:
        delta = 0.0
        for s in range(n):
            v_old = V[s]
            V[s] = R[s] + gamma * np.dot(P[s], V)
            delta = max(delta, abs(v_old - V[s]))
    return V
</code></pre>
            <p>
              This function iteratively applies the Bellman operator until the value function converges within a small tolerance.  For large state spaces, linear algebra solvers or Monte Carlo sampling methods are preferred.
            </p>
          </div>
        </details>

        <!-- Markov Decision Processes -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-sync-alt"></i>
            9.2 Stationary Distributions &amp; Ergodicity
          </summary>
          <div class="details-content">
            <p>
              To analyse long‑run behaviour of Markov processes we study their <strong>stationary distributions</strong>.  Consider a finite Markov chain defined by a transition matrix $P$ over a state set $S$ (with no actions).  A distribution $d^*$ over $S$ is <em>stationary</em> if it is unchanged by one step of the chain: $d^* = d^* P$.  When the chain is <em>irreducible</em> and <em>aperiodic</em> (together these conditions are called <strong>ergodicity</strong>), there exists a unique stationary distribution and the distribution of states converges to it regardless of the initial state.  In other words, for an ergodic chain $\lim_{t\to\infty} P^t(i,j) = d^*_j$ for all states $i,j$.
            </p>
            <p>
              Stationary distributions underpin many RL algorithms.  They allow us to compute long‑term averages and understand mixing times.  In practice, one often needs to compute $d^*$ numerically.  Since $d^*$ is the left principal eigenvector of $P$ with eigenvalue 1, we can find it by solving the linear system $d^* P = d^*$ with the constraint $\sum_{s} d^*(s) = 1$.  The following Python snippet shows how to approximate the stationary distribution via power iteration:
            </p>
<pre><code class="language-python">import numpy as np

def stationary_distribution(P, tol=1e-8, max_iter=100000):
    """Approximate the stationary distribution of a stochastic matrix P.

    Uses the power iteration method on the transpose of P.
    """
    n = P.shape[0]
    # start with a uniform distribution
    d = np.ones(n) / n
    for _ in range(max_iter):
        d_new = d @ P  # one step of the Markov chain
        if np.linalg.norm(d_new - d, ord=1) &lt; tol:
            break
        d = d_new
    return d
</code></pre>
            <p>
              For small matrices one can use eigenvalue solvers, but power iteration is straightforward and works well for ergodic chains.  The figure below illustrates a simple three–state Markov chain with transition probabilities; the thickness of the arrows hints at the stationary distribution's mass.
            </p>
            <div class="highlight-box">
              <p><strong>Illustration.</strong> A Markov chain with three states connected by directed edges.  The probability of moving between states is shown by the relative thickness of the arrows.</p>
              <img src="assets/markov_chain_diagram.png" alt="Diagram of a Markov chain with three states and transition probabilities" style="max-width:100%;height:auto;border-radius:8px;box-shadow:0 4px 8px rgba(0,0,0,0.4);" />
            </div>
            <h4>The Agent–Environment Interaction Loop</h4>
            <p>
              At the heart of an MDP is the repetitive loop of observation, action, reward and state transition.  The following Mermaid diagram summarizes this loop.
            </p>
            <div class="mermaid">
              graph LR
                Agent((Agent)) -- chooses A_t --> Environment((Environment))
                Environment -- observes S_t --> Agent
                Environment -- emits R_{t+1}, S_{t+1} --> Agent
            </div>
            <h4>Transition Dynamics &amp; Markov Chains</h4>
            <p>
              The state transitions of an MDP define a Markov chain when the policy is fixed.  The transition probabilities can be arranged into a matrix $P$, where entry $P_{s s'} = P(S_{t+1}=s' \mid S_t=s)$ and each row sums to one.  A <em>stationary distribution</em> $\pi$ over states satisfies $\pi = \pi P$; such distributions are central when analyzing long‑term behavior or ergodicity.  In partially observable settings (POMDPs), the agent must maintain a belief over hidden states, but the Markov assumption remains an important modeling tool.
            </p>
          </div>
        </details>

        <!-- Convergence of Value Iteration &amp; Contraction Mapping -->
        <details class="fade-in">
          <summary>
            <i class="fas fa-compress-arrows-alt"></i>
            9.3 Convergence of Value Iteration &amp; Contraction Mapping
          </summary>
          <div class="details-content">
            <p>
              Value iteration is a classic dynamic programming algorithm for computing the optimal state–value function $V^*$ of an MDP.  Starting from an arbitrary value function $V_0$, we repeatedly apply the <em>Bellman optimality operator</em>:
            </p>
            <p style="text-align:center;font-size:1.1em;">
              $$V_{k+1}(s) = \max_a \Bigl[R(s,a) + \gamma \sum_{s'} P(s' \mid s,a)\,V_k(s')\Bigr],\quad s\in S.$$
            </p>
            <p>
              This update backs up the optimal future return and, thanks to the discount factor $\gamma &lt; 1$, is a <strong>contraction mapping</strong> with respect to the supremum norm.  In particular, for any two value functions $V$ and $W$,
              \[
                \|T V - T W\|_\infty \le \gamma\,\|V - W\|_\infty,
              \]
              where $T$ denotes the Bellman optimality operator.  The Banach fixed–point theorem then guarantees that repeated application of $T$ converges to a unique fixed point $V^*$ and that the error shrinks geometrically at rate $\gamma$.
            </p>
            <p>
              In practice one initializes $V_0$ arbitrarily (often all zeros) and iterates the update until the maximum change falls below a small threshold.  The resulting value function can then be used to extract an optimal policy by greedily selecting the action that achieves the maximum in the Bellman update.
            </p>
<pre><code class="language-python">import numpy as np


def value_iteration(P, R, gamma=0.9, tol=1e-6):
    """Compute the optimal value function for a finite MDP via value iteration.

    Args:
        P: transition dynamics as a nested list P[s][a][s&#39;] giving P(s&#39;|s,a)
        R: reward function R[s][a] giving expected immediate reward for taking action a in state s
        gamma: discount factor in (0,1)
        tol: convergence tolerance for stopping

    Returns:
        A tuple (V, policy) where V is the optimal state–value vector and policy[s] is the greedy action at s.
    """
    n_states = len(P)
    n_actions = len(P[0])
    V = np.zeros(n_states)
    while True:
        delta = 0.0
        for s in range(n_states):
            v_old = V[s]
            # compute the Bellman backup for state s
            q_sa = [R[s][a] + gamma * sum(P[s][a][s_prime] * V[s_prime] for s_prime in range(n_states))
                    for a in range(n_actions)]
            V[s] = max(q_sa)
            delta = max(delta, abs(v_old - V[s]))
        if delta &lt; tol:
            break
    # derive the greedy policy
    policy = np.zeros(n_states, dtype=int)
    for s in range(n_states):
        q_sa = [R[s][a] + gamma * sum(P[s][a][s_prime] * V[s_prime] for s_prime in range(n_states))
                for a in range(n_actions)]
        policy[s] = int(np.argmax(q_sa))
    return V, policy
</code></pre>
            <p>
              The above implementation illustrates value iteration for small, finite state and action spaces.  It updates each state&#39;s value in place and terminates when successive updates change by less than <code>tol</code>.  For large or continuous MDPs one typically uses function approximation or sampling–based methods such as fitted value iteration or Monte Carlo tree search.
            </p>
            <div class="highlight-box">
              <p><strong>Key insight.</strong>  Because the Bellman operator is a contraction, value iteration is guaranteed to converge to the unique optimal value function regardless of initialization.  This property underlies many modern reinforcement‑learning algorithms, where more complex operators (e.g. policy evaluation, Q–learning) inherit similar contraction properties.</p>
                <div style="text-align: center;">
            <!-- decorative illustration conveying contraction mapping and convergence -->
<img src="assets/contraction_mapping.png"
     alt="Abstract depiction of a contraction mapping with swirling arrows converging toward a central point"
     style="max-width:50%;height:auto;border-radius:12px;box-shadow:0 4px 12px rgba(0,0,0,0.5);" />
          </div>
        </details>
      </section>
      <hr/>
<section id="part10">
        <h2 id="part10-title">
          <i class="fas fa-brain"></i>
          Part 10: Neural Network Policies &amp; Policy Gradient Methods
        </h2>

        <!-- Introduction to Neural Network Policies -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-network-wired"></i>
            10.1 Introduction to Neural Network Policies
          </summary>
          <div class="details-content">
            <p>
              A <strong>Neural Network Policy (NNP)</strong> is a policy represented by a parameterized neural network.  Given a state as input, the network outputs either a deterministic action or a probability distribution over actions.  Unlike handcrafted, rule‑based policies, an NNP can approximate complex, non‑linear mappings from high‑dimensional observations to actions and can improve through experience by gradient‑based optimization.
            </p>
            <p>
              The power of NNPs lies in their ability to <em>learn from data</em>.  In classical control, designers manually engineer control laws (e.g. PID controllers) or policies for specific tasks.  Hard‑coded policies perform well only for the scenarios they were designed for and fail in novel situations.  By contrast, an NNP starts with random parameters and gradually shapes its decision surface to maximize reward in the environment, discovering strategies that may surprise human designers.  For example, an NNP trained on the classic <em>Cart‑Pole</em> environment learns to swing the cart left or right at just the right moment to balance the pole, without any explicit knowledge of physics.
            </p>
            <div class="highlight-box">
              <p><strong>Example: Cart‑Pole.</strong>  The Cart‑Pole environment consists of a cart that moves along a track with a pole hinged to it.  The state is a 4‑dimensional vector capturing cart position, velocity, pole angle and angular velocity.  A simple NNP for this task might be a two‑layer feedforward network mapping these four inputs to a probability of pushing left or right.  Trained with policy gradients, the network learns to oscillate the cart to keep the pole upright, far outperforming a naive “if angle is positive, push left” rule.</p>
            </div>

          </div>
        </details>

        <!-- Deeper dive into the policy gradient theorem and actor–critic variants -->

        <!-- Architecture of Neural Network Policies -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-layer-group"></i>
            10.2 Architecture of Neural Network Policies
          </summary>
          <div class="details-content">
            <p>
              The architecture of an NNP depends on the observation and action spaces.  A typical feedforward policy network comprises three types of layers:
            </p>
            <ul>
              <li><strong>Input Layer.</strong>  Receives the state vector (for example a vector of dimension <em>d</em>) or raw sensory observations such as images.  Convolutional layers are commonly used for images to extract spatial features.</li>
              <li><strong>Hidden Layers.</strong>  One or more fully connected, convolutional or recurrent layers apply non‑linear activation functions like ReLU or tanh to transform features.  Deeper networks can represent more complex decision boundaries but are harder to train.</li>
              <li><strong>Output Layer.</strong>  Produces either a discrete distribution over actions (via softmax) for finite action spaces, or the parameters (such as a mean vector and standard deviation) of a continuous distribution for continuous actions.  For continuous control, the output might use tanh to bound actions and softplus to ensure positive standard deviations.</li>
            </ul>
            <p>
              The following Mermaid diagram illustrates a simple two‑layer policy network for a discrete action space.  The input state is passed through hidden units, and the output layer yields a probability for each action via a softmax.
            </p>
            <div class="mermaid">
              graph TD
                S((State\n$s$)) --> H1[Hidden Layer]
                H1 --> H2[Hidden Layer]
                H2 --> A1((Action 1))
                H2 --> A2((Action 2))
                H2 --> A3((Action 3))
              classDef layer fill:#161930,stroke-width:1px;
            </div>
            <p>
              For continuous actions, the output layer might produce a mean vector and a log‑standard‑deviation vector.  Together these define a multivariate normal distribution with mean \(\mu(s)\) and diagonal covariance \(\mathrm{diag}(\sigma(s)^2)\) from which actions are sampled.</p>
          </div>
        </details>

        <!-- Exploration & Credit Assignment -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-compass"></i>
            10.3 Exploration–Exploitation &amp; Credit Assignment
          </summary>
          <div class="details-content">
            <p>
              Two core challenges in reinforcement learning are <strong>exploration</strong> and <strong>credit assignment</strong>.  Exploration refers to the agent’s need to try unfamiliar actions to discover potentially higher rewards.  A deterministic policy will often get stuck in local optima if it never explores.  Stochastic policies (sampling from πθ(a|s)) naturally encourage exploration.  Additional strategies include epsilon‑greedy action selection, entropy regularization, and adding noise to actions.</p>
            <p>
              Credit assignment asks: which actions were responsible for the eventual reward?  Because rewards may be delayed, it can be difficult to determine how much each action contributed to the final outcome.  Policy gradient methods address this via weighting action log‑probabilities by returns (or advantages) computed over the trajectory.</p>
            <div class="admonition info">
              <span class="admonition-title">
                <i class="fas fa-lightbulb"></i>
                Tip: Encourage exploration
              </span>
              <p>
                Entropy regularization adds a term like –β Σ<sub>a</sub> πθ(a|s) log πθ(a|s) to the objective, encouraging higher entropy (more randomness) and preventing premature convergence to a deterministic policy.  The coefficient β controls the strength of the regularization.
              </p>
            </div>
          </div>
        </details>

        <!-- REINFORCE Algorithm & Variance Reduction -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-robot"></i>
            10.4 REINFORCE &amp; Variance Reduction
          </summary>
          <div class="details-content">
            <p>
              The simplest policy gradient method is the <strong>REINFORCE algorithm</strong> (Williams, 1992).  Given a complete episode trajectory (S₀, A₀, R₁, S₁, … , S<sub>T</sub>) sampled under the current policy πθ, the objective is the expected return J(θ) = E<sub>πθ</sub>[∑<sub>t=0</sub><sup>T−1</sup> γ<sup>t</sup> R<sub>t+1</sub>].  The policy gradient theorem gives:
            </p>
            <!-- Display the policy gradient theorem as a properly rendered LaTeX equation -->
            <p>$$\nabla_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(A_t \mid S_t) \cdot G_t\right]$$</p>
            <p>
              where G<sub>t</sub> = ∑<sub>k=t</sub><sup>T−1</sup> γ<sup>k−t</sup> R<sub>k+1</sub> is the return from time t.  In practice, we approximate the expectation with samples: for each step in the trajectory, we compute G<sub>t</sub> and multiply it by the gradient of the log probability of the taken action.
            </p>
            <div class="formula-box">
              <pre><code>loss = -\sum_{t=0}^{T-1} G_t * \log \pi_\theta(A_t \mid S_t)
loss.backward()
optimizer.step()</code></pre>
            </div>
            <p>
              A major drawback of vanilla REINFORCE is its high variance.  To reduce variance without introducing bias, we subtract a <em>baseline</em> $b(s)$ from the return.  The gradient with a baseline becomes
              $$\nabla_{\theta} J(\theta) = \mathbb{E}\left[\sum_{t} \bigl(G_t - b(S_t)\bigr)\, \nabla_{\theta} \log \pi_{\theta}(A_t \mid S_t)\right]$$
              A common choice for $b(s)$ is an estimate of the state‑value function $V(s)$ learned with a separate critic network.  Using $A_t = G_t - V(S_t)$ (the advantage) yields the <em>actor–critic</em> architecture, which forms the basis for algorithms like A2C and PPO.</p>
          </div>
        </details>

        <!-- Applications & Limitations -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-globe"></i>
            10.5 Applications &amp; Limitations
          </summary>
          <div class="details-content">
            <p>
              Neural network policies have been applied to a broad spectrum of domains.  The table below summarizes a few notable applications.
            </p>
            <table>
              <thead>
                <tr><th>Domain</th><th>Environment/Task</th><th>Algorithm &amp; Highlights</th><th>Impact</th></tr>
              </thead>
              <tbody>
                <tr><td>Gaming</td><td>Atari (Pong, Breakout)</td><td>DQN, A3C, PPO</td><td>Achieved superhuman scores learning from pixels</td></tr>
                <tr><td>Robotics</td><td>BipedalWalker, robotic arms</td><td>DDPG, TRPO, SAC</td><td>Learned locomotion and manipulation from sensor data</td></tr>
                <tr><td>Language Models</td><td>RLHF for ChatGPT</td><td>PPO with reward models</td><td>Aligned LLM outputs with human preferences</td></tr>
                <tr><td>Operations</td><td>Data center cooling, logistics</td><td>Custom PG methods, DQN variants</td><td>Reduced energy use and improved resource allocation</td></tr>
              </tbody>
            </table>
            <h4>Limitations</h4>
            <ul>
              <li><strong>Sample efficiency.</strong>  Policy gradient methods require many interactions with the environment, making them expensive when data is scarce.</li>
              <li><strong>High variance.</strong>  Even with baselines, the gradient estimates can be noisy, requiring careful tuning of learning rates and variance reduction techniques.</li>
              <li><strong>Sensitivity to architecture.</strong>  The choice of network depth, activation functions and initialization affects performance; poor choices can lead to vanishing gradients or unstable training.</li>
            </ul>
          </div>
        </details>

      </section>

        <!-- Extended Policy Gradient Algorithms -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-rocket"></i>
            10.6 Advanced Policy Gradient Algorithms &amp; Interactive Demo
          </summary>
          <div class="details-content">
            <p>
              Building on the basics of policy gradients, modern algorithms introduce <strong>actor–critic</strong> architectures and sophisticated update rules. Two popular examples are <strong>Advantage Actor–Critic (A2C)</strong>, which learns a value function to reduce gradient variance, and <strong>Proximal Policy Optimization (PPO)</strong>, which uses a clipped objective to maintain a trust region around the current policy and improve stability. In A2C, the actor (policy network) and critic (value network) are updated simultaneously; the critic minimizes a regression loss to approximate V(s), while the actor maximizes the expected advantage.
            </p>
            <p>
              The PPO objective can be written as:
            </p>
            <p>
              \[
                L^{\text{PPO}}(\theta) = \mathbb{E}_t\left[\min\big(r_t(\theta) A_t,\ \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t\big)\right]
              \]
            </p>
            <p>
              where \(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\) is the importance sampling ratio and \(\epsilon\) is a small constant (e.g., 0.2) controlling the clip range. This objective discourages large policy updates, leading to more stable learning.
            </p>
            Below is a <em>simulated</em> training comparison of REINFORCE, A2C and PPO on a simple task. The interactive chart lets you zoom and hover to inspect individual episodes.
            </p>
            <div id="pg-advanced-chart" style="width:100%;height:420px;"></div>
            <p>
              Here's a simplified PyTorch-style pseudocode for PPO. It highlights the alternating policy and value updates and the computation of the clipped surrogate loss:
            </p>
            <pre><code># Simplified PPO loop
env = gym.make('CartPole-v1')
policy = ActorCriticNetwork(...)
optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)

for iteration in range(num_iterations):
    trajectories = collect_trajectories(env, policy)
    returns, advantages = compute_returns_and_advantages(trajectories, gamma=0.99, lam=0.95)
    for epoch in range(K):  # K epochs of minibatch updates
        for batch in mini_batches(trajectories):
            logprobs_old = policy.get_logprobs(batch.states, batch.actions)
            ratio = (policy.get_logprobs(batch.states, batch.actions) - logprobs_old).exp()
            surrogate1 = ratio * batch.advantages
            surrogate2 = torch.clamp(ratio, 1 - eps, 1 + eps) * batch.advantages
            policy_loss = -torch.min(surrogate1, surrogate2).mean()
            value_loss = F.mse_loss(policy.get_values(batch.states), batch.returns)
            loss = policy_loss + 0.5 * value_loss - 0.01 * policy.entropy(batch.states).mean()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()</code></pre>
          </div>
        </details>


        <details open class="fade-in">
          <summary>
            <i class="fas fa-brain"></i>
            10.7 Deeper Dive into Policy Gradient &amp; Actor–Critic Variants
          </summary>
          <div class="details-content">
            <p>
              The foundation of policy gradient methods is the <strong>policy gradient theorem</strong>. For a parameterized stochastic policy \(\pi_\theta(a \mid s)\), the objective function
              is the expected return \(J(\theta) = \mathbb{E}_{\pi_\theta}[G_0]\).  The theorem shows that its gradient is
            </p>
            <p>
            \[
              \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\big[ \nabla_\theta \log \pi_\theta(a \mid s)\, Q^{\pi_\theta}(s,a) \big],
            \]
            </p>
            <p>
              where \(Q^{\pi_\theta}(s,a)\) is the action–value function.  This expression provides an unbiased gradient estimate from trajectory samples and forms the basis of the <strong>REINFORCE</strong> algorithm. To reduce variance, a baseline (often a state-value estimate \(V^\pi(s)\)) is subtracted from the return.
            </p>
            <p>
              <strong>REINFORCE:</strong> The simplest policy gradient algorithm uses Monte Carlo returns as an estimate of \(Q^{\pi_\theta}(s,a)\).  For each trajectory, it performs updates of the form \( \theta \leftarrow \theta + \alpha \sum_t \nabla_\theta \log \pi_\theta(a_t \mid s_t)\, (G_t - b(s_t)) \), where \(b(s_t)\) is a baseline.
            </p>
            <p>
              <strong>A3C and A2C:</strong> Asynchronous Advantage Actor–Critic (A3C) leverages multiple parallel actors and critics to decorrelate experience and speed up training.  Each actor learns its own policy \(\pi_{\theta'}\) and periodically synchronizes with global parameters \(\theta\).  Advantage Actor–Critic (A2C) is a synchronous variant that waits for all actors to finish an update before applying gradients.  Both algorithms use an advantage estimate \(A(s,a) = Q(s,a) - V(s)\) to reduce variance and share a critic network for \(V(s)\).
            </p>
            <p>
              <strong>Deterministic Policy Gradient and DDPG:</strong> For continuous action spaces, the deterministic policy gradient theorem expresses the gradient as
              \( \nabla_\theta J(\theta) = \mathbb{E}_{s\sim\rho^{\mu}} [ \nabla_a Q^\mu(s,a) \nabla_\theta \mu_\theta(s) ] \).  Deep Deterministic Policy Gradient (DDPG) extends this to deep networks by combining a deterministic actor with a critic and using experience replay and target networks.  Twin Delayed DDPG (TD3) further reduces over‑estimation by learning two critics and using the smaller value for the target while delaying actor updates.
            </p>
            <p>
              <strong>Entropy‑Regularized Methods:</strong> Soft Actor–Critic (SAC) introduces an entropy term to encourage exploration and maximize \(J(\pi) = \mathbb{E}_t [ r_t + \alpha\, H(\pi(\cdot\mid s_t)) ]\), where \(H\) is the entropy and \(\alpha\) controls the exploration temperature.  This objective encourages stochastic policies and improves stability and sample efficiency compared to on‑policy methods.
            </p>
            <p>
              These variants illustrate the rich landscape of policy gradient methods, from the simple REINFORCE algorithm to synchronous and asynchronous actor–critic methods, deterministic gradients for continuous control and entropy‑regularized approaches like SAC.  For a comprehensive survey of policy gradient algorithms, see the detailed review by Lilian Weng.
            </p>
          </div>
        </details>

      <!-- ============================================================ -->
      <!-- Part 11: Deep Q-Learning & Value-Based Methods -->
      <section id="part11">
        <h2 id="part11-title">
          <i class="fas fa-gamepad"></i>
          Part&nbsp;11: Deep Q‑Learning &amp; Value‑Based Methods
        </h2>

        <!-- Genesis & Key Innovations -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-bolt"></i>
            11.1 Genesis &amp; Key Innovations
          </summary>
          <div class="details-content">
            <p>
              Deep Q‑Learning (DQN) extends classic Q‑learning by replacing the tabular Q‑value table with a deep neural network that approximates the action‑value function, enabling agents to learn directly from high‑dimensional inputs like images.  The original DQN algorithm famously mastered dozens of Atari games from raw pixels by addressing two major sources of instability: the correlation of sequential training samples and the “moving target” problem in bootstrapped updates.  <strong>Experience replay</strong> stores past transitions (s, a, r, s') in a replay buffer and draws random mini‑batches for updates, breaking temporal correlations.  A separate <strong>target network</strong> with frozen parameters is used to compute the TD target for several steps before being periodically synchronized with the online network, stabilizing the learning objective.  Together these innovations allowed DQN to train deep function approximators reliably.
            </p>
            <p>
              Despite its success, standard DQN still suffers from <em>over‑estimation bias</em> due to using the same network for action selection and evaluation.  The <strong>Double DQN</strong> variant mitigates this by using the online network to choose the action maximizing Q(s',a') while evaluating that action with the target network.  This simple decoupling greatly improves value estimates and policy performance.  
            </p>
            <div class="highlight-box">
              <p><strong>DQN Architecture.</strong> The schematic below depicts a value‑based agent with a replay buffer and two networks.  The online network learns a mapping Qθ(s,a) and periodically copies its weights to the target network Q<sup>θ‑</sup>.  Mini‑batches of transitions are sampled from the replay buffer to update θ.</p>
              <div class="mermaid">
                graph LR
                  S0((State s)) --> Online{{"Online\\nQ-Network"}}
                  Replay{{"Replay\\nBuffer"}} -- sample batch --> Online
                  Online -- θ ← θ − α ∇ℒ --> Online
                  Online -. copy every τ steps .-> Target{{"Target\\nQ-Network"}}
                  Target -- Q<sup>θ‑</sup>(s',a') --> Online
                  Online --> A((Action a))
              </div>
              <p class="caption">Diagram: Experience replay and target network in DQN.</p>
            </div>
            <!-- decorative illustration capturing the essence of deep Q‑network algorithms -->
          </div>
        </details>

        <!-- DQN Variants & Extensions -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-layer-group"></i>
            11.2 Variants &amp; Extensions
          </summary>
          <div class="details-content">
            <p>
              Following the original DQN, many variants have been proposed to address its limitations and improve performance.  Key extensions include:
            </p>
            <ul>
              <li><strong>Double DQN.</strong> Decouples action selection and evaluation to reduce over‑estimation bias.</li>
              <li><strong>Dueling DQN.</strong> Splits the network’s output into two streams estimating the state value V(s) and the advantage A(s,a).  The outputs are recombined as Q(s,a) = V(s) + [A(s,a) − (1/|A|) ∑<sub>a'</sub> A(s,a')], allowing the network to learn which states are valuable independent of the chosen action.</li>
              <li><strong>Prioritized Experience Replay.</strong> Samples transitions with probability proportional to their TD error, focusing learning on surprising experiences.</li>
              <li><strong>Distributional DQN.</strong> Models the full return distribution rather than just its expectation, enabling agents to reason about risk.</li>
              <li><strong>Rainbow.</strong> Combines the above advances (double, dueling, prioritized replay, multi‑step targets, distributional RL and noisy networks) into a single algorithm with state‑of‑the‑art performance.</li>
            </ul>
            <p>
              These improvements highlight a recurring design principle: decoupling critical components (e.g., selection vs. evaluation) and enriching the learning signal (e.g., using distributions or advantage functions) produce more stable and data‑efficient agents.
            </p>
          </div>
        </details>

        <!-- Comparative Analysis & Applications -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale"></i>
            11.3 Comparative Analysis &amp; Applications
          </summary>
          <div class="details-content">
            <p>
              Value‑based methods like DQN learn an action‑value function and derive a policy via greedy action selection.  They excel in discrete action spaces and environments with sparse rewards, such as classic control benchmarks and many Atari games.  Compared to policy‑gradient approaches, value‑based agents are often more sample‑efficient because bootstrapping reuses experience, but they can be harder to apply to continuous control and suffer from value estimation bias.  The table below summarizes typical settings:
            </p>
            <table>
              <thead>
                <tr><th>Method</th><th>Action Space</th><th>Strengths</th><th>Challenges</th></tr>
              </thead>
              <tbody>
                <tr><td>DQN / Double DQN</td><td>Discrete</td><td>Sample efficient; strong performance on Atari</td><td>Over‑estimation bias; unstable targets</td></tr>
                <tr><td>Rainbow DQN</td><td>Discrete</td><td>State‑of‑the‑art; combines multiple improvements</td><td>More complex; higher computational cost</td></tr>
                <tr><td>Policy Gradient / PPO</td><td>Discrete/Continuous</td><td>Smoothly handles continuous actions; unbiased gradient estimates</td><td>High variance; lower sample efficiency</td></tr>
              </tbody>
            </table>
            <p>
              In practice, DQN and its variants have been deployed in video games, dialogue management, recommendation systems and other sequential decision problems where the state representation is high‑dimensional but the action set is modest.
            </p>
          </div>
        </details>
      </section>

      <!-- ============================================================ -->
      <!-- Part 12: Actor–Critic Methods & Policy Optimization -->
      <section id="part12">
        <h2 id="part12-title">
          <i class="fas fa-user-friends"></i>
          Part&nbsp;12: Actor–Critic Methods &amp; Policy Optimization
        </h2>

        <!-- Actor–Critic Framework -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-users"></i>
            12.1 Actor–Critic Framework
          </summary>
          <div class="details-content">
            <p>
              While policy gradients like REINFORCE learn purely from returns, variance can be high because they ignore structure in the value function.  <strong>Actor–critic</strong> algorithms mitigate this by learning two networks: an <em>actor</em> (policy) πθ(a | s) and a <em>critic</em> (value) V_w(s) or Q_w(s,a).  The critic provides a baseline estimate (or advantage) that reduces the variance of the policy gradient.  Updates alternate between evaluating the current policy using the critic and improving the policy in the direction suggested by the critic’s advantage estimates.  This yields much more stable learning.
            </p>
            <p>
              Actor–critic algorithms come in many flavors.  <em>Advantage Actor‑Critic (A2C/A3C)</em> computes the advantage function A(s,a)=Q(s,a)−V(s) on‑policy and updates both actor and critic in parallel.  <em>Trust Region Policy Optimization (TRPO)</em> and <em>Proximal Policy Optimization (PPO)</em> constrain the policy update to stay within a safe region, preventing overly large gradient steps.  Off‑policy methods like <em>Deep Deterministic Policy Gradient (DDPG)</em>, <em>Twin Delayed DDPG (TD3)</em> and <em>Soft Actor‑Critic (SAC)</em> combine deterministic actors with Q‑value critics and replay buffers, achieving excellent performance on continuous control tasks.  
            </p>
            <div class="highlight-box">
              <p><strong>Actor–Critic Diagram.</strong> The illustration below shows the interplay between actor and critic.  The actor samples an action from its policy; the critic evaluates this action via the value or advantage function and provides a signal to update the actor.</p>
              <div class="mermaid">
                graph LR
                  subgraph Actor-Critic
                    S((State s)) --> Actor[Actor πθ]
                    Actor --> A((Action a))
                    A --> Env((Environment))
                    Env --> R((Reward r))
                    Env --> S2((Next State s'))
                    S2 --> Critic[Critic V_w or Q_w]
                    Critic --> delta((Advantage / TD error))
                    delta --> Actor
                  end
              </div>
              <p class="caption">Diagram: Actor–critic learning loop.</p>
            </div>
          </div>
        </details>

        <!-- Deterministic & Continuous Control -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-wave-square"></i>
            12.2 Deterministic &amp; Continuous Control
          </summary>
          <div class="details-content">
            <p>
              For continuous action spaces, deterministic policy gradient (DPG) methods replace the stochastic policy with a deterministic function μθ(s).  The actor outputs a single action instead of a distribution, and the critic learns Q_w(s,a).  <em>Deep Deterministic Policy Gradient (DDPG)</em> extends DQN with an actor network and uses target networks and experience replay for both actor and critic.  <em>Twin Delayed DDPG (TD3)</em> further reduces over‑estimation by learning two critics and using the smaller value for the target, while delaying actor updates.  <em>Soft Actor‑Critic (SAC)</em> introduces an entropy term to encourage exploration, maximizing J(π) = ∑<sub>t</sub> E[r_t + α H(π(·|s_t))] with temperature parameter α controlling exploration.  
            </p>
            <p>
              These methods highlight the diversity of policy optimization algorithms: from strictly on‑policy, stochastic methods like A3C to off‑policy, deterministic variants like TD3 and entropy‑regularized approaches like SAC.  Choosing among them involves trading off stability, sample efficiency and complexity.
            </p>
          </div>
        </details>

        <!-- Actor–Critic vs Value-Based -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-exchange-alt"></i>
            12.3 Actor–Critic vs Value‑Based Approaches
          </summary>
          <div class="details-content">
            <p>
              Value‑based and actor–critic methods occupy complementary niches.  Value‑based algorithms like DQN learn Q‑values and derive policies by taking the arg‑max, which works well for discrete actions but becomes unwieldy in continuous spaces.  Actor–critic methods directly optimize a parameterized policy and can handle continuous actions gracefully.  However, policy optimization typically requires careful step‑size control and suffers from higher variance.  Hybrid methods like <em>Q‑Actor‑Critic</em> combine the benefits: the actor chooses actions, while the critic learns Q‑values, enabling off‑policy updates with function approximation.
            </p>
          </div>
        </details>

        <!-- Limitations & Challenges -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-exclamation-triangle"></i>
            12.4 Limitations &amp; Open Challenges
          </summary>
          <div class="details-content">
            <p>
              Despite their sophistication, actor–critic methods are not a panacea.  They remain sensitive to hyperparameters like learning rates, discount factors and entropy coefficients.  Off‑policy actor–critic variants can struggle with overestimation and require techniques like target networks and double critics to stabilize learning.  In continuous control tasks, exploration noise and reward scale must be carefully tuned.  The search for more robust, sample‑efficient and theoretically grounded policy optimization algorithms remains a vibrant area of research.
            </p>
          </div>
        </details>
      </section>

      <!-- ============================================================ -->
      <!-- Part 13: Frontiers & Emerging Directions -->
      <section id="part13">
        <h2 id="part13-title">
          <i class="fas fa-rocket"></i>
          Part&nbsp;13: Frontiers &amp; Emerging Directions
        </h2>

        <!-- Offline & Model‑Based RL -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-database"></i>
            13.1 Offline &amp; Model‑Based RL
          </summary>
          <div class="details-content">
            <p>
              Many practical problems limit the agent’s ability to interact with the real environment, motivating <strong>offline (batch) reinforcement learning</strong>.  Offline RL aims to learn effective policies from a fixed dataset of recorded interactions.  A central challenge is <em>distributional shift</em>: the learned policy may propose actions rarely or never seen in the dataset, leading to erroneous value estimates.  Algorithms like Conservative Q‑Learning (CQL) penalize Q‑values on out‑of‑distribution actions, encouraging safe generalization.  <strong>Model‑based RL</strong> learns a dynamics model P_hat(s'|s,a) and reward model R_hat(s,a) so that the agent can plan in imagination.  MuZero, for instance, learns an abstract model to perform look‑ahead search without explicitly reconstructing observations.  Model‑based methods can dramatically improve sample efficiency but require accurate models and careful regularization to avoid compounding model errors.
            </p>
          </div>
        </details>

        <!-- Multi‑Agent & Social RL -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-people-arrows"></i>
            13.2 Multi‑Agent &amp; Social RL
          </summary>
          <div class="details-content">
            <p>
              Real‑world systems often involve multiple learning agents that interact, cooperate or compete.  <strong>Multi‑Agent Reinforcement Learning (MARL)</strong> studies how agents can learn in such non‑stationary environments.  Challenges include the moving‑target problem (other agents change their policies), credit assignment within a team reward and communication.  MARL techniques underpin achievements like AlphaStar in StarCraft and coordinated robot swarms.  Recent work explores centralized critics with decentralized actors, opponent modeling and mean‑field approximations to scale to many agents.
            </p>
          </div>
        </details>

        <!-- RL from Human Feedback & Alignment -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-hands-helping"></i>
            13.3 RL from Human Feedback &amp; Alignment
          </summary>
          <div class="details-content">
            <p>
              <strong>Reinforcement learning from human feedback (RLHF)</strong> aligns an agent’s behavior with human preferences by learning a reward model from annotated comparisons and then fine‑tuning the agent using policy optimization.  The process typically comprises three stages: (1) <em>Supervised fine‑tuning</em> on high‑quality demonstrations to provide a strong initial policy; (2) <em>Reward model training</em>, where humans rank multiple outputs and a model learns to predict which is preferred; and (3) <em>Policy optimization</em>, often via Proximal Policy Optimization (PPO), using the reward model as the reward signal.  RLHF has enabled the alignment of large language models like ChatGPT and Sparrow, producing safer and more helpful responses.  However, it requires substantial annotation effort and may inherit biases from the human feedback dataset.  
            </p>
          </div>
        </details>

        <!-- RL for Reasoning & Cognitive Frontiers -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-brain"></i>
            13.4 RL for Reasoning &amp; Cognitive Frontiers
          </summary>
          <div class="details-content">
            <p>
              Beyond control and games, researchers are exploring RL as a tool for <em>reasoning and language</em>.  By treating the steps of a mathematical proof or the lines of code generation as actions in a long‑horizon environment, RL can be used to optimize the reasoning process.  Recent methods incorporate <em>process supervision</em>, rewarding intermediate reasoning steps rather than just the final answer, leading to models that better generalize to new problems.  Adapting RL algorithms to the enormous action spaces of language models and deterministic environments remains an open challenge, but successes in chain‑of‑thought reasoning and program synthesis indicate a promising frontier.
            </p>
          </div>
        </details>

        <!-- Conclusion & Vision -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-flag-checkered"></i>
            13.5 Conclusion &amp; Vision
          </summary>
          <div class="details-content">
            <p>
              Reinforcement learning has evolved from simple Q‑tables to deep neural architectures, actor–critic frameworks and sophisticated alignment protocols.  As the field moves toward richer settings—offline learning, multi‑agent systems, human preferences and cognitive reasoning—the core principle remains unchanged: agents learn by interacting with environments and optimizing for long‑term reward.  Understanding the strengths and limitations of each algorithm family equips practitioners to choose the right tool for the task and inspires new research directions toward more general, efficient and aligned artificial intelligence.
            </p>
          </div>
        </details>
      </section>

      <!-- ============================================================ -->
      <!-- Part 14: Policy Gradient Deep Dive & Advanced Variants -->
      <section id="part14">
        <h2 id="part14-title">
          <i class="fas fa-brain"></i>
          Part&nbsp;14: Policy Gradient Deep Dive &amp; Advanced Variants
        </h2>

        <!-- Motivation for Policy Gradient Methods -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-lightbulb"></i>
            14.1 Why Policy Gradient Methods?
          </summary>
          <div class="details-content">
            <p>
              Policy gradient algorithms directly optimize a parameterized policy instead of
              learning a value function and deriving a policy from it.  This makes them
              especially well‑suited for problems with continuous action spaces and
              intrinsically stochastic optimal policies.  As highlighted in recent
              surveys, these methods can operate natively in both
              discrete and continuous settings, produce smoother search landscapes by
              learning stochastic policies, and allow the policy
              to evolve more gradually during training, which
              often improves stability compared to value‑based approaches.
            </p>
            <p>
              Another advantage is the ability to incorporate entropy or Kullback–Leibler
              regularization directly into the objective, encouraging exploration and
              preventing premature convergence.  However, naive implementations like
              REINFORCE suffer from high variance and sample inefficiency.  The
              following subsections introduce more sophisticated variants that address
              these challenges.
            </p>
            <!-- decorative illustration depicting policy gradient methods -->
            <!-- Local decorative illustration of policy gradient methods. The original placeholder referenced a missing file. 
                 We now point to a locally generated abstract image capturing the concept of gradient ascent through swirling curves and particles. -->
          </div>
        </details>

        <!-- A3C and Asynchronous Advantage Actor–Critic -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-users"></i>
            14.2 A3C &amp; the Power of Parallel Actors
          </summary>
          <div class="details-content">
            <p>
              The Asynchronous Advantage Actor–Critic (A3C) algorithm builds on two
              key ideas: estimating advantages and running multiple actors in
              parallel.  As detailed in the literature,
              A3C learns an <em>advantage</em> function by combining state‑value and
              action‑value estimates, reducing the variance of policy gradient
              estimates.  Simultaneously, it launches several actors interacting with
              their own copies of the environment.  These actors accumulate
              gradients locally and periodically synchronize with global parameters,
              decorrelating samples and stabilizing learning.
            </p>
            <p>
              In practice, A3C’s asynchronous updates enable efficient use of
              multi‑core processors and improve exploration by letting each actor
              follow slightly different policies.  This algorithm inspired
              synchronous variants like A2C and served as a precursor to more
              advanced methods such as TRPO and PPO.
            </p>
          </div>
        </details>

        <!-- Trust Region Policy Optimization (TRPO) -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-shield-alt"></i>
            14.3 Trust Region &amp; Mirror Learning
          </summary>
          <div class="details-content">
            <p>
              Changing a policy too drastically between updates can destabilize
              training.  Trust Region Policy Optimization (TRPO) introduces a
              <em>trust region</em> constraint on the Kullback–Leibler (KL) divergence
              between the current policy and the updated policy.
              By constraining the KL divergence, TRPO ensures that each update
              remains within a safe region, providing theoretical guarantees of
              monotonic improvement.  The optimization
              is solved approximately via conjugate gradient and backtracking
              line search.
            </p>
            <p>
              TRPO can be interpreted through the lens of <em>mirror descent</em> or
              <em>mirror learning</em>, where the policy update is performed in a
              dual space induced by the KL divergence.  This perspective connects
              TRPO to a broader family of algorithms such as natural policy
              gradients and proximal methods.  While TRPO enjoys strong
              theoretical properties, it can be computationally intensive due to
              second‑order optimization.
            </p>
            <p>
              A simplified pseudocode sketch is shown below:
            </p>
            <pre><code># Simplified TRPO outline
collect trajectories under current policy π
estimate advantages A(s,a) using a value baseline
compute policy gradient g = ∑ ∇_θ log π(a|s) A(s,a)
estimate the Fisher information matrix F via KL divergence
solve F x = g for search direction x using conjugate gradient
line search step size λ to satisfy KL constraint
update parameters: θ ← θ + λ x
update value function via regression
</code></pre>
          </div>
        </details>

        <!-- Interactive Comparison of PG Algorithms -->
        <details open class="fade-in">
          <summary>
            <i class="fas fa-chart-line"></i>
            14.4 Interactive Comparison: REINFORCE vs A3C vs TRPO vs PPO
          </summary>
          <div class="details-content">
            <p>
              The interactive chart below illustrates hypothetical learning curves for four
              policy gradient algorithms on a continuous control task.  While the
              curves are simulated for visualization, they demonstrate how
              on‑policy algorithms like REINFORCE may exhibit high variance, A3C
              converges faster due to variance reduction, TRPO enjoys smoother
              improvements thanks to trust region constraints, and PPO strikes a
              balance with clipped objectives.  You can hover over the curves to
              inspect rewards over training iterations.
            </p>
            <div id="pg-comparison-chart" style="width:100%;height:420px;"></div>
          </div>
        </details>

        <!-- Advantage Functions &amp; Generalized Advantage Estimation -->
        <details class="fade-in">
          <summary>
            <i class="fas fa-balance-scale"></i>
            14.5 Advantage Functions &amp; Generalized Advantage Estimation
          </summary>
          <div class="details-content">
            <p>
              The <strong>advantage function</strong> quantifies how much better taking a particular action in a given state is compared to following the policy as usual.  Formally, it is defined as
              \[A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s).\]
              A positive advantage indicates that action <em>a</em> yields higher return than expected, while a negative advantage suggests the action is worse than the baseline value.  Using advantages rather than raw returns reduces the variance of policy gradient estimates and allows us to decrease the log‑probability of actions that perform poorly.
            </p>
            <p>
              In practice we cannot compute $Q^{\pi}$ and $V^{\pi}$ exactly, so we estimate them.  A simple estimator is the <em>temporal‑difference (TD) error</em> at time $t$, defined as
              \[\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t).\]
              This one‑step TD error is an unbiased estimator of $A^{\pi}(s_t,a_t)$ when the value function $V$ is accurate.  Multi‑step TD errors can further reduce variance by bootstrapping over $n$ steps.
            </p>
            <p>
              <strong>Generalized Advantage Estimation (GAE)</strong> introduces an exponential weighting of these multi‑step TD errors controlled by a parameter $\lambda\in[0,1]$.  Let $\delta_t$ denote the one‑step TD error.  The GAE for time $t$ is
            </p>
            <p style="text-align:center;font-size:1.1em;">
              $$\hat{A}^{\text{GAE}}_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l \, \delta_{t+l},$$
            </p>
            <p>
              where $(\gamma\lambda)^l$ geometrically weights errors from future timesteps.  When $\lambda=0$, GAE reduces to the one‑step TD error; when $\lambda\to 1$, it approaches the Monte Carlo return.  Adjusting $\lambda$ trades variance for bias—higher values use longer returns and have lower variance but more bias.
            </p>
            <p>
              The following Python function computes GAE for a single episode given sequences of rewards and estimated state values:
            </p>
<pre><code class="language-python">import numpy as np

def compute_gae(rewards, values, gamma=0.99, lam=0.95):
    """Compute Generalized Advantage Estimation (GAE) for a trajectory.

    Args:
        rewards (list of float): sequence of rewards r_t
        values (list of float): predicted state values V(s_t) for each timestep (length len(rewards) + 1)
        gamma (float): discount factor
        lam (float): GAE λ parameter controlling bias–variance trade‑off

    Returns:
        list of float: advantage estimates for each timestep
    """
    T = len(rewards)
    advantages = np.zeros(T)
    gae = 0.0
    for t in reversed(range(T)):
        delta = rewards[t] + gamma * values[t+1] - values[t]
        gae = delta + gamma * lam * gae
        advantages[t] = gae
    return advantages
</code></pre>
            <p>
              This implementation processes the trajectory backwards, accumulating an exponentially decayed sum of TD errors.  GAE has become a standard component of modern actor–critic algorithms such as A3C, PPO and TRPO.
            </p>
            <div class="highlight-box">
              <p><strong>Summary.</strong>  The advantage function measures relative performance and underpins variance reduction in policy gradients.  Generalized Advantage Estimation extends this idea by blending short‑ and long‑term TD errors with a tunable parameter $\lambda$, balancing bias and variance and improving sample efficiency.</p>
            </div>
            <!-- decorative graphic of actor and critic working together -->
            <img src="assets/actor_critic_diagram.png" alt="Illustration of actor and critic neural networks exchanging information in a reinforcement learning algorithm" style="max-width:100%;height:auto;border-radius:12px;box-shadow:0 4px 12px rgba(0,0,0,0.5);margin:1rem 0;" />
          </div>
        </details>

      </section>

      <button id="back-to-top" class="control-button" title="Back to Top"><i class="fas fa-arrow-up"></i></button>


 <!-- ============================================================ -->
  <!-- Part 15: Advanced Policy Gradient Techniques & Exploratory Tools -->
  <section id="part15">
    <h2 id="part15-title">
      <i class="fas fa-wave-square"></i>
      Part&nbsp;15: Advanced Policy Gradient Techniques &amp; Exploratory Tools
    </h2>

    <!-- Variance reduction and entropy regularization section -->
    <details open class="fade-in">
      <summary>
        <i class="fas fa-flask"></i>
        15.1 Variance Reduction &amp; Entropy Regularization
      </summary>
      <div class="details-content">
        <p>
          Policy gradient methods are powerful but can suffer from high variance in gradient estimates. To address this, practitioners introduce
          <strong>baselines</strong> or build <em>actor–critic</em> architectures. A baseline subtracts a control variate from the return without adding bias,
          reducing variance. The optimal baseline corresponds to the true state‑value function, but in practice approximations
          suffice. Actor–critic algorithms combine a value‑function critic with a policy actor to learn more stable gradients.
        </p>
        <p>
          Another important regularization technique is <strong>entropy maximization</strong>. Adding an entropy term to the objective encourages stochastic
          policies and smooths the optimization landscape. Empirical studies show that higher policy entropy can connect
          local optima and support larger learning rates. Algorithms like Soft Actor–Critic (SAC) leverage this principle by
          optimizing both expected return and policy entropy.
        </p>
        <p>
          Together, variance reduction and entropy regularization underpin modern policy gradient algorithms such as A2C/A3C, TRPO, PPO, SAC, and TD3.
          These methods strike a balance between efficient learning and robust exploration.
        </p>
      </div>
    </details>

    <!-- Historical milestones section with interactive timeline -->
    <details open class="fade-in">
      <summary>
        <i class="fas fa-history"></i>
        15.2 Historical Milestones in Reinforcement Learning
      </summary>
      <div class="details-content">
        <p>
          Reinforcement learning has evolved through a series of breakthroughs spanning more than half a century. Dynamic programming was developed by
          Richard&nbsp;Bellman in the 1950s, establishing the foundation for optimal control via the Bellman equation. Early
          RL work emerged in the 1980s with temporal‑difference learning and the definition of the reinforcement learning framework by Sutton and Barto.
          In 1989 Watkins introduced Q‑learning, enabling off‑policy value‑based learning. In 1992 Williams proposed the REINFORCE algorithm, the first
          widely used policy gradient method. The deep learning era began in 2013 with DeepMind’s DQN architecture, leading to superhuman Atari play.
          AlphaGo’s victory over Lee&nbsp;Sedol in 2016 showcased deep RL combined with search, and subsequent developments like A3C, TRPO, PPO, TD3 and
          SAC have further advanced the field. Most recently, reinforcement learning from human feedback (RLHF) has enabled aligning large language
          models with human preferences.
        </p>
        <div id="rl-timeline" style="width:100%;height:300px;"></div>
      </div>
    </details>

    <!-- Interactive Gaussian policy explorer -->
    <details open class="fade-in">
      <summary>
        <i class="fas fa-sliders-h"></i>
        15.3 Interactive Gaussian Policy Explorer
      </summary>
      <div class="details-content">
        <p>
          Explore how the mean and standard deviation parameters influence a Gaussian policy’s probability density. Adjust the sliders below to change the
          policy parameters and observe how the distribution shifts and spreads. This tool illustrates the role of entropy in policy optimization: higher
          variance (entropy) encourages exploration, while lower variance concentrates probability mass around the current best action.
        </p>
        <div style="margin:0.5rem 0;">
          <label for="mean-slider">Mean: </label>
          <input type="range" id="mean-slider" min="-3" max="3" step="0.1" value="0" style="width:200px;" />
          <span id="mean-value">0</span>
          &nbsp;&nbsp;
          <label for="std-slider">Standard Deviation: </label>
          <input type="range" id="std-slider" min="0.1" max="2" step="0.1" value="1" style="width:200px;" />
          <span id="std-value">1.0</span>
        </div>
        <div id="policy-dist" style="width:100%;height:300px;"></div>
      </div>
    </details>
  </section>

  <!-- ============================================================ -->
  <!-- Part 16: Modern Actor–Critic Algorithms & Interactive Demos -->
  <section id="part16">
    <h2 id="part16-title">
      <i class="fas fa-robot"></i>
      Part&nbsp;16: Modern Actor–Critic Algorithms &amp; Interactive Demos
    </h2>

    <!-- 16.1 Deep Deterministic Policy Gradient (DDPG) -->
    <details open class="fade-in">
      <summary>
        <i class="fas fa-feather-alt"></i>
        16.1 Deep Deterministic Policy Gradient (DDPG)
      </summary>
      <div class="details-content">
        <p>
          Deep Deterministic Policy Gradient (DDPG) is an off‑policy actor–critic algorithm designed for continuous action spaces. It simultaneously learns a Q‑function and a deterministic policy using the deterministic policy gradient theorem. DDPG leverages a replay buffer and separate target networks to stabilize training and uses exploration noise to encourage adequate coverage of the action space. In practice, DDPG is regarded as deep Q‑learning for continuous actions and is only applicable to continuous control tasks.
        </p>
        <p>
          A baseline DDPG step samples a transition from a replay buffer, updates the critic by minimizing a Bellman error, and updates the actor by maximizing the critic’s Q‑value. Adding noise to the deterministic policy during action selection is essential for effective exploration.
        </p>
        <p>
          <em>PyTorch Implementation Sketch</em>:
        </p>
<pre><code class="language-python">def ddpg_step(state, noise_scale):
    # deterministic action from actor network
    action = actor(state)
    # add exploration noise to encourage exploration
    noisy_action = action + noise_scale * torch.randn_like(action)
    next_state, reward, done, info = env.step(noisy_action)
    replay_buffer.add(state, action, reward, next_state, done)
    # sample a batch from the buffer
    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
    # compute target Q using target networks
    with torch.no_grad():
        target_actions = actor_target(next_states)
        target_q = critic_target(next_states, target_actions)
        y = rewards + gamma * (1 - dones) * target_q
    # update critic by minimizing mean‑squared error
    critic_loss = F.mse_loss(critic(states, actions), y)
    critic_optimizer.zero_grad()
    critic_loss.backward()
    critic_optimizer.step()
    # update actor via deterministic policy gradient
    actor_loss = -critic(states, actor(states)).mean()
    actor_optimizer.zero_grad()
    actor_loss.backward()
    actor_optimizer.step()
    # softly update target networks (omitted for brevity)
</code></pre>
        <p>
          <strong>Interactive demonstration:</strong> adjust the noise scale slider to observe how deterministic actions are perturbed by additive noise during exploration.
        </p>
        <div style="margin:0.5rem 0;">
          <label for="ddpg-noise-slider">Noise scale: </label>
          <input type="range" id="ddpg-noise-slider" min="0" max="2" step="0.1" value="0.5" style="width:250px;"/>
          <span id="ddpg-noise-value">0.5</span>
        </div>
        <div id="ddpg-demo" style="width:100%;height:300px;"></div>
      </div>
    </details>

    <!-- 16.2 Twin Delayed DDPG (TD3) -->
    <details open class="fade-in">
      <summary>
        <i class="fas fa-layer-group"></i>
        16.2 Twin Delayed Deep Deterministic Policy Gradient (TD3)
      </summary>
      <div class="details-content">
        <p>
          While DDPG can achieve strong performance, it is brittle and often suffers from overestimated Q‑values and sensitivity to hyperparameters. Twin Delayed DDPG (TD3) addresses these issues by introducing three core tricks: learning two Q‑functions and using the smaller value to form the target (clipped double‑Q learning), delaying policy updates relative to critic updates, and smoothing the target policy by adding clipped noise to target actions. Together, these innovations dramatically reduce overestimation bias and stabilize learning.
        </p>
        <p>
          TD3 retains the off‑policy actor–critic structure of DDPG but empirically yields much more reliable performance on continuous control tasks. Its implementation extends the DDPG pseudocode by computing two critic losses and taking the minimum to form the target, and by delaying policy and target network updates.
        </p>
        <p>
          <em>PyTorch Implementation Sketch</em>:
        </p>
<pre><code class="language-python"># sample batch from replay buffer
states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
# target policy smoothing: add clipped noise to target actions
with torch.no_grad():
    noise = (torch.randn_like(actions) * policy_noise).clamp(-noise_clip, noise_clip)
    next_actions = (actor_target(next_states) + noise).clamp(action_low, action_high)
    target_q1 = critic1_target(next_states, next_actions)
    target_q2 = critic2_target(next_states, next_actions)
    target_q = torch.min(target_q1, target_q2)
    y = rewards + gamma * (1 - dones) * target_q
# update both critics
loss1 = F.mse_loss(critic1(states, actions), y)
loss2 = F.mse_loss(critic2(states, actions), y)
critic1_optimizer.zero_grad(); loss1.backward(); critic1_optimizer.step()
critic2_optimizer.zero_grad(); loss2.backward(); critic2_optimizer.step()
# delay policy updates
if step % policy_delay == 0:
    actor_loss = -critic1(states, actor(states)).mean()
    actor_optimizer.zero_grad(); actor_loss.backward(); actor_optimizer.step()
    # soft update target networks (not shown)
</code></pre>
        <p>
          <strong>Interactive demonstration:</strong> adjust the noise amplitude slider to see how employing two Q‑functions and taking their minimum attenuates estimation variability compared to single‑critic estimates.
        </p>
        <div style="margin:0.5rem 0;">
          <label for="td3-noise-slider">Noise amplitude: </label>
          <input type="range" id="td3-noise-slider" min="0" max="2" step="0.1" value="0.5" style="width:250px;"/>
          <span id="td3-noise-value">0.5</span>
        </div>
        <div id="td3-demo" style="width:100%;height:300px;"></div>
      </div>
    </details>

    <!-- 16.3 Proximal Policy Optimization (PPO) -->
    <details open class="fade-in">
      <summary>
        <i class="fas fa-expand-arrows-alt"></i>
        16.3 Proximal Policy Optimization (PPO)
      </summary>
      <div class="details-content">
        <p>
          Proximal Policy Optimization (PPO) tackles the challenge of performing large policy updates without collapsing performance. Like Trust Region Policy Optimization (TRPO), it seeks to maximize improvement while keeping the new policy close to the old one, but it does so using simple first‑order methods and clipping or penalty terms. PPO‑Penalty incorporates a KL‑divergence penalty into the objective, whereas PPO‑Clip uses a clipped surrogate objective to restrict the probability ratio between the new and old policies within \((1-\epsilon, 1+\epsilon)\). These techniques make PPO robust and easy to implement. The algorithm performs well across discrete and continuous action spaces and supports parallel implementations.
        </p>
        <p>
          <em>Implementation Sketch</em>:
        </p>
<pre><code class="language-python"># compute importance sampling ratios between new and old policies
ratios = torch.exp(new_log_probs - old_log_probs)
# clip ratios within [1 - eps_clip, 1 + eps_clip]
clipped = torch.clamp(ratios, 1 - eps_clip, 1 + eps_clip)
# advantage estimates (precomputed)
surrogate1 = ratios * advantages
surrogate2 = clipped * advantages
loss = -torch.min(surrogate1, surrogate2).mean()
optimizer.zero_grad()
loss.backward()
optimizer.step()
</code></pre>
        <p>
          <strong>Interactive demonstration:</strong> adjust the clipping parameter \(\epsilon\) to see how the clipped probability ratio compares to the raw ratio. The clipping window restricts ratios to lie within <code>[1-\epsilon,1+\epsilon]</code>.
        </p>
        <div style="margin:0.5rem 0;">
          <label for="ppo-epsilon-slider">Clipping parameter &epsilon;: </label>
          <input type="range" id="ppo-epsilon-slider" min="0.05" max="0.5" step="0.05" value="0.2" style="width:250px;"/>
          <span id="ppo-epsilon-value">0.20</span>
        </div>
        <div id="ppo-demo" style="width:100%;height:300px;"></div>
      </div>
    </details>

    <!-- 16.4 Soft Actor–Critic (SAC) -->
    <details open class="fade-in">
      <summary>
        <i class="fas fa-fire"></i>
        16.4 Soft Actor–Critic (SAC)
      </summary>
      <div class="details-content">
        <p>
          Soft Actor–Critic (SAC) bridges the gap between deterministic actor–critic methods and stochastic policy optimization. It optimizes a stochastic policy in an off‑policy manner and introduces entropy regularization, encouraging exploration by maximizing a trade‑off between expected return and policy entropy. Increasing entropy promotes exploration and prevents premature convergence to sub‑optimal policies. SAC uses two critics with clipped double‑Q learning like TD3 and thus benefits from reduced overestimation error while retaining the advantages of stochastic policies.
        </p>
        <p>
          <em>Implementation Sketch</em>:
        </p>
<pre><code class="language-python"># sample batch from replay buffer
states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
# compute target value V using clipped double-Q and entropy
with torch.no_grad():
    next_action, next_log_prob = actor.sample(next_states)
    target_q1 = critic1_target(next_states, next_action)
    target_q2 = critic2_target(next_states, next_action)
    target_v = torch.min(target_q1, target_q2) - alpha * next_log_prob
    y = rewards + gamma * (1 - dones) * target_v
# update critics
loss1 = F.mse_loss(critic1(states, actions), y)
loss2 = F.mse_loss(critic2(states, actions), y)
critic1_optimizer.zero_grad(); loss1.backward(); critic1_optimizer.step()
critic2_optimizer.zero_grad(); loss2.backward(); critic2_optimizer.step()
# update policy: maximize expected Q + entropy bonus
action_new, log_prob = actor.sample(states)
actor_loss = (alpha * log_prob - critic1(states, action_new)).mean()
actor_optimizer.zero_grad(); actor_loss.backward(); actor_optimizer.step()
</code></pre>
        <p>
          <strong>Interactive demonstration:</strong> adjust the entropy coefficient \(\alpha\) to see how the SAC objective (return plus entropy bonus) changes relative to the vanilla return. Higher values of \(\alpha\) place more emphasis on entropy, encouraging exploration.
        </p>
        <div style="margin:0.5rem 0;">
          <label for="sac-alpha-slider">Entropy coefficient &alpha;: </label>
          <input type="range" id="sac-alpha-slider" min="0" max="2" step="0.1" value="1" style="width:250px;"/>
          <span id="sac-alpha-value">1.0</span>
        </div>
        <div id="sac-demo" style="width:100%;height:300px;"></div>
      </div>
    </details>
  </section>

    </main>
    
    <footer>
      <div class="footer-content">
        <div class="feedback-container">
          <span>This document is still developing. Your feedback and contributions are welcome.</span>
          <div class="footer-socials">
            <a href="mailto:taherifarnam@gmail.com" aria-label="Email" title="Email me">
              <i class="fas fa-envelope"></i>
            </a>
            <a href="https://github.com/AmirrezaFarnamTaheri" target="_blank" rel="noopener noreferrer" aria-label="GitHub" title="Find me on GitHub">
              <i class="fab fa-github"></i>
            </a>
          </div>
        </div>
        <div class="footer-meta">
          <span>Version 1.0.0</span> | <span>
          <i class="fas fa-copyright"></i> Authored and designed by Amirreza "Farnam" Taheri.
        </span>| <span id="last-updated">Last Updated: July 21, 2025</span> </span>
        </div>
      </div>
    </footer>

  </div>


  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // --- THEME MANAGEMENT ---
      const THEMES = [
          { id: 'cyberpunk', name: 'Cyberpunk' },
          { id: 'synthwave', name: '80s Synthwave' },
          { id: 'matrix', name: 'Matrix' },
          { id: 'blueprint', name: 'Blueprint' },
          { id: 'nord', name: 'Nord' },
          { id: 'dracula', name: 'Dracula' },
          { id: 'monokai', name: 'Monokai Pro' },
          { id: 'github-dark', name: 'GitHub Dark' },
          { id: 'gruvbox-dark', name: 'Gruvbox Dark' },
          { id: 'solarized-dark', name: 'Solarized Dark' },
          { id: 'solarized-light', name: 'Solarized Light' },
          { id: 'academic-light', name: 'Academic Light' },
      ];
      const themeToggleBtnContainer = document.getElementById('theme-toggle-btn-container');
      const themeSwitcher = document.getElementById('theme-switcher');
      let currentTheme = localStorage.getItem('theme') || 'cyberpunk';

      function setTheme(themeId) {
          document.documentElement.setAttribute('data-theme', themeId);
          localStorage.setItem('theme', themeId);
          currentTheme = themeId;
          
          themeSwitcher.querySelectorAll('.theme-option').forEach(btn => {
              btn.classList.toggle('active', btn.dataset.theme === themeId);
          });

          const isDark = !['solarized-light', 'academic-light'].includes(themeId);
          mermaid.initialize({ startOnLoad: false, theme: isDark ? 'dark' : 'default' });
          mermaid.run();
      }

      function populateThemeSwitcher() {
          themeSwitcher.innerHTML = '';
          THEMES.forEach(theme => {
              const button = document.createElement('button');
              button.className = 'theme-option';
              button.textContent = theme.name;
              button.dataset.theme = theme.id;
              if (theme.id === currentTheme) button.classList.add('active');
              button.addEventListener('click', () => {
                  setTheme(theme.id);
                  themeToggleBtnContainer.classList.remove('active');
              });
              themeSwitcher.appendChild(button);
          });
      }
      
      themeToggleBtnContainer.addEventListener('click', (e) => {
        e.stopPropagation();
        themeToggleBtnContainer.classList.toggle('active');
      });

      document.addEventListener('click', (e) => {
        if (!themeToggleBtnContainer.contains(e.target)) {
            themeToggleBtnContainer.classList.remove('active');
        }
      });
      
      populateThemeSwitcher();
      setTheme(currentTheme);

      // --- TOC & NAVIGATION ---
      const sidebarNav = document.getElementById('sidebar-nav');
      const mainContent = document.getElementById('main-content');
      const container = document.getElementById('container');
      const tocToggleBtn = document.getElementById('toc-toggle-btn');
      const backToTopBtn = document.getElementById('back-to-top');

      function buildTOC() {
          if (!sidebarNav || !mainContent) return;
          const headings = mainContent.querySelectorAll('h2, h3, h4');
          const toc = document.createElement('ul');
          let currentH2List, currentH3List;

          headings.forEach(h => {
              if (!h.id) h.id = h.textContent.trim().toLowerCase().replace(/\s+/g, '-');
              const level = parseInt(h.tagName.substring(1), 10);
              const li = document.createElement('li');
              const a = document.createElement('a');
              a.href = `#${h.id}`;
              a.textContent = h.textContent.trim();
              li.appendChild(a);

              if (level === 2) {
                  toc.appendChild(li);
                  currentH2List = document.createElement('ul');
                  li.appendChild(currentH2List);
                  currentH3List = null;
              } else if (level === 3 && currentH2List) {
                  currentH2List.appendChild(li);
                  currentH3List = document.createElement('ul');
                  li.appendChild(currentH3List);
              } else if (level === 4 && currentH3List) {
                  currentH3List.appendChild(li);
              }
          });
          sidebarNav.innerHTML = '';
          sidebarNav.appendChild(toc);
      }

      function highlightTOC() {
        const sections = Array.from(mainContent.querySelectorAll('h2, h3, h4'));
        const links = sidebarNav.querySelectorAll('a');
        const observer = new IntersectionObserver(entries => {
            let visibleSections = entries.filter(e => e.isIntersecting).map(e => e.target);
            if (visibleSections.length > 0) {
                visibleSections.sort((a,b) => a.offsetTop - b.offsetTop);
                const id = visibleSections[0].id;
                links.forEach(link => {
                    link.classList.remove('active');
                    if (link.getAttribute('href') === `#${id}`) {
                        link.classList.add('active');
                    }
                });
            }
        }, { rootMargin: '0px 0px -80% 0px', threshold: 0 });

        sections.forEach(s => observer.observe(s));
      }
      
      if(tocToggleBtn) {
        tocToggleBtn.addEventListener('click', () => container.classList.toggle('toc-hidden'));
      }
      
      // --- PROGRESS BAR & BACK TO TOP ---
      const progressBar = document.getElementById('progress-bar');
      function updateProgressBar() {
        const scrollableHeight = document.documentElement.scrollHeight - window.innerHeight;
        const scrolled = window.scrollY;
        progressBar.style.width = scrollableHeight > 0 ? `${(scrolled / scrollableHeight) * 100}%` : '0%';
        if (backToTopBtn) backToTopBtn.classList.toggle('visible', scrolled > 300);
      }
      if (backToTopBtn) {
        backToTopBtn.addEventListener('click', () => window.scrollTo({ top: 0, behavior: 'smooth' }));
      }
      window.addEventListener('scroll', updateProgressBar);
      updateProgressBar();
      
      // --- CODE BLOCK ACTIONS ---
      document.querySelectorAll('.copy-code-btn').forEach(button => {
          button.addEventListener('click', async () => {
              const code = button.closest('.code-container').querySelector('pre code').innerText;
              try {
                  await navigator.clipboard.writeText(code);
                  const originalText = button.innerHTML;
                  button.innerHTML = '<i class="fas fa-check"></i> Copied!';
                  setTimeout(() => { button.innerHTML = originalText; }, 2000);
              } catch (err) { console.error('Failed to copy text: ', err); }
          });
      });
      
      // --- VISUALIZATION TOOLBAR ---
      document.querySelectorAll('.visualization-container').forEach(container => {
        const visualElement = container.querySelector('.mermaid, svg, img');
        if (!visualElement) return;

        let scale = 1, panning = false, pointX = 0, pointY = 0, start = { x: 0, y: 0 };
        function setTransform() { visualElement.style.transform = `translate(${pointX}px, ${pointY}px) scale(${scale})`; }

        visualElement.addEventListener('mousedown', e => { e.preventDefault(); panning = true; start = { x: e.clientX - pointX, y: e.clientY - pointY }; });
        visualElement.addEventListener('mouseup', () => { panning = false; });
        visualElement.addEventListener('mouseleave', () => { panning = false; });
        visualElement.addEventListener('mousemove', e => { if (!panning) return; pointX = e.clientX - start.x; pointY = e.clientY - start.y; setTransform(); });

        container.querySelector('[data-action="zoom-in"]').addEventListener('click', () => { scale *= 1.2; setTransform(); });
        container.querySelector('[data-action="zoom-out"]').addEventListener('click', () => { scale /= 1.2; setTransform(); });
        container.querySelector('[data-action="reset"]').addEventListener('click', () => { scale = 1; pointX = 0; pointY = 0; visualElement.style.transform = ''; });
        container.querySelector('[data-action="fullscreen"]').addEventListener('click', () => { if (container.requestFullscreen) container.requestFullscreen(); });
      });

      // --- SEARCH ---
      const searchInput = document.getElementById('search-input');
      const content = document.getElementById('main-content');
      let markInstance = null;
      searchInput.addEventListener('input', () => {
        const query = searchInput.value.trim();
        if (markInstance) markInstance.unmark();
        if (query.length > 2) {
            import('https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js').then(() => {
                markInstance = new Mark(content);
                markInstance.mark(query, {
                    "separateWordSearch": false,
                });
            });
        }
      });

      // --- INITIALIZE ALL ---
      buildTOC();
      highlightTOC();
    });
  </script>

  <script>
    document.addEventListener('DOMContentLoaded', function() {
      if (typeof Plotly !== 'undefined') {
        const episodes = Array.from({length: 100}, (_, i) => i + 1);
        const reinforce = episodes.map(ep => Math.log(ep + 1) * 20 + (Math.random() - 0.5) * 10);
        const a2c = episodes.map(ep => Math.log(ep + 1) * 25 + (Math.random() - 0.5) * 8);
        const ppo = episodes.map(ep => Math.log(ep + 1) * 30 + (Math.random() - 0.5) * 6);
        const data = [
          { x: episodes, y: reinforce, name: 'REINFORCE', mode: 'lines', line: { color: '#e64a19' } },
          { x: episodes, y: a2c, name: 'A2C', mode: 'lines', line: { color: '#43a047' } },
          { x: episodes, y: ppo, name: 'PPO', mode: 'lines', line: { color: '#1e88e5' } }
        ];
        const layout = {
          title: 'Simulated Policy Gradient Training',
          xaxis: { title: 'Episode' },
          yaxis: { title: 'Cumulative Reward' },
          legend: { orientation: 'h' }
        };
        Plotly.newPlot('pg-advanced-chart', data, layout);
      }
    });
  </script>

  <!-- Script for interactive multi‑armed bandit playground -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const chartDiv = document.getElementById('bandit-chart');
      if (!chartDiv) return;
      // Hidden true means for three arms – values between 0 and 1
      const trueMeans = [0.3, 0.6, 0.8];
      let estimates = [0, 0, 0];
      let counts = [0, 0, 0];
      // create initial bar (estimated) and line (true mean) traces
      const traceEst = {
        x: ['Arm 1', 'Arm 2', 'Arm 3'],
        y: estimates.slice(),
        name: 'Estimated Mean',
        type: 'bar',
        marker: { color: '#1f77b4' }
      };
      const traceTrue = {
        x: ['Arm 1', 'Arm 2', 'Arm 3'],
        y: trueMeans.slice(),
        name: 'True Mean',
        type: 'scatter',
        mode: 'lines',
        line: { color: '#d62728', dash: 'dash' }
      };
      Plotly.newPlot(chartDiv, [traceEst, traceTrue], {
        title: 'Multi‑Armed Bandit Estimates',
        barmode: 'group',
        yaxis: { range: [0, 1] },
        legend: { orientation: 'h' }
      });
      // update epsilon display when slider changes
      const epsRange = document.getElementById('epsilon-range');
      const epsValueSpan = document.getElementById('epsilon-value');
      epsRange?.addEventListener('input', () => {
        epsValueSpan.textContent = Number(epsRange.value).toFixed(2);
      });
      const messageEl = document.getElementById('bandit-message');
      document.getElementById('bandit-step-btn')?.addEventListener('click', () => {
        const eps = parseFloat(epsRange.value);
        let armIndex;
        // choose random or exploit based on epsilon
        if (Math.random() < eps || counts.every(c => c === 0)) {
          armIndex = Math.floor(Math.random() * trueMeans.length);
        } else {
          // choose arm with highest estimated mean
          let maxVal = Math.max(...estimates);
          armIndex = estimates.findIndex(v => v === maxVal);
        }
        // sample reward from normal distribution with small noise
        const mu = trueMeans[armIndex];
        const u1 = Math.random();
        const u2 = Math.random();
        const z = Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
        let reward = mu + z * 0.1; // noise with std dev 0.1
        reward = Math.max(0, Math.min(1, reward));
        // update counts and estimates
        counts[armIndex] += 1;
        estimates[armIndex] += (reward - estimates[armIndex]) / counts[armIndex];
        // update chart: update first trace (bar) with new estimates array
        Plotly.restyle(chartDiv, { y: [estimates] }, [0]);
        messageEl.textContent = `Pulled arm ${armIndex + 1}, received reward ${reward.toFixed(2)}`;
      });
    });
  </script>


<script>
// Toggle content tier
(function(){
  const buttons = document.querySelectorAll('.tier-option');
  buttons.forEach(btn => {
    btn.addEventListener('click', () => {
      buttons.forEach(b => b.classList.remove('active'));
      btn.classList.add('active');
      document.documentElement.setAttribute('data-content-tier', btn.getAttribute('data-tier'));
    });
  });
})();
</script>

  <!-- Script for interactive PG comparison chart -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const chartDiv = document.getElementById('pg-comparison-chart');
      if (!chartDiv) return;
      // Generate x-axis values (episodes)
      const xValues = Array.from({ length: 50 }, (_, i) => i + 1);
      // Helper to simulate a learning curve starting at a given value with noise
      function simulateCurve(start, noise) {
        let val = start;
        const arr = [];
        for (let i = 0; i < xValues.length; i++) {
          val += (Math.random() * noise - noise / 2);
          val = Math.max(0, val);
          arr.push(Number(val.toFixed(3)));
        }
        return arr;
      }
      const reinforce = simulateCurve(1.0, 0.3);
      const a3c = simulateCurve(2.0, 0.2);
      const trpo = simulateCurve(2.5, 0.15);
      const ppo = simulateCurve(2.3, 0.18);
      const data = [
        { x: xValues, y: reinforce, name: 'REINFORCE', type: 'scatter', mode: 'lines', line: { color: '#E24A33' } },
        { x: xValues, y: a3c, name: 'A3C', type: 'scatter', mode: 'lines', line: { color: '#348ABD' } },
        { x: xValues, y: trpo, name: 'TRPO', type: 'scatter', mode: 'lines', line: { color: '#988ED5' } },
        { x: xValues, y: ppo, name: 'PPO', type: 'scatter', mode: 'lines', line: { color: '#777777' } }
      ];
      const layout = {
        title: 'Simulated Policy Gradient Learning Curves',
        xaxis: { title: 'Training Episode' },
        yaxis: { title: 'Average Return' },
        legend: { orientation: 'h' }
      };
      Plotly.newPlot(chartDiv, data, layout);
    });
  </script>

 

  <!-- Script for RL timeline and Gaussian policy explorer -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // Build timeline data
      // Data for RL timeline: years and descriptive events
      const timelineYears = ['1957','1989','1992','1998','2013','2015','2016','2017','2018','2019','2020','2023'];
      const timelineEvents = [
        'Bellman introduces dynamic programming',
        'Q-learning (Watkins)',
        'REINFORCE (Williams)',
        'Actor–Critic & TD methods',
        'Deep Q-Network (DQN)',
        'A3C algorithm',
        'AlphaGo defeats Lee Sedol',
        'TRPO & PPO advancements',
        'Twin Delayed DDPG (TD3)',
        'Soft Actor–Critic (SAC)',
        'Reinforcement learning from human feedback (RLHF)',
        'Transformer-based RL advancements'
      ];
      // Alternate marker heights to reduce label overlap
      const timelineY = timelineYears.map((_, i) => (i % 2));
      const timelineData = {
        x: timelineYears,
        y: timelineY,
        mode: 'markers',
        type: 'scatter',
        hovertext: timelineEvents,
        hoverinfo: 'text',
        marker: { size: 8, color: '#00BFFF' }
      };
      const timelineLayout = {
        title: 'Historical Milestones in Reinforcement Learning',
        xaxis: { title: 'Year', type: 'category' },
        yaxis: { visible: false, range: [-0.5, 1.5] },
        margin: { l: 40, r: 20, t: 60, b: 40 }
      };
      const timelineDiv = document.getElementById('rl-timeline');
      if (timelineDiv) {
        Plotly.newPlot(timelineDiv, [timelineData], timelineLayout);
      }

      // Gaussian policy distribution update function
      function updateDistribution() {
        const meanSlider = document.getElementById('mean-slider');
        const stdSlider = document.getElementById('std-slider');
        if (!meanSlider || !stdSlider) return;
        const mean = parseFloat(meanSlider.value);
        const std = parseFloat(stdSlider.value);
        document.getElementById('mean-value').textContent = mean.toFixed(1);
        document.getElementById('std-value').textContent = std.toFixed(1);
        const xVals = [];
        const yVals = [];
        for (let i = -5; i <= 5; i += 0.1) {
          xVals.push(i);
          yVals.push(Math.exp(-0.5 * Math.pow((i - mean) / std, 2)) / (std * Math.sqrt(2 * Math.PI)));
        }
        Plotly.newPlot('policy-dist', [{
          x: xVals,
          y: yVals,
          mode: 'lines',
          line: { color: '#E24A33' }
        }], {
          title: 'Gaussian Policy Density',
          xaxis: { title: 'Action' },
          yaxis: { title: 'Density' },
          showlegend: false,
          margin: { l: 40, r: 20, t: 50, b: 40 }
        });
      }
      // initialize and bind listeners
      updateDistribution();
      const meanSliderEl = document.getElementById('mean-slider');
      const stdSliderEl = document.getElementById('std-slider');
      if (meanSliderEl && stdSliderEl) {
        meanSliderEl.addEventListener('input', updateDistribution);
        stdSliderEl.addEventListener('input', updateDistribution);
      }

      // ========== DDPG Interactive Demo ==========
      function updateDDPG() {
        const slider = document.getElementById('ddpg-noise-slider');
        const display = document.getElementById('ddpg-noise-value');
        const chartDiv = document.getElementById('ddpg-demo');
        if (!slider || !display || !chartDiv) return;
        const noise = parseFloat(slider.value);
        display.textContent = noise.toFixed(1);
        const baseActions = [];
        const noisyActions = [];
        const steps = [];
        for (let i = 0; i < 100; i++) {
          steps.push(i);
          const base = Math.sin(i / 10);
          baseActions.push(base);
          // uniform noise in [-1,1] scaled by noise; for demonstration purposes
          const rand = (Math.random() - 0.5) * 2;
          noisyActions.push(base + noise * rand);
        }
        Plotly.newPlot(chartDiv, [
          { x: steps, y: baseActions, name: 'Deterministic Action', type: 'scatter', mode: 'lines', line: { color: '#1f77b4' } },
          { x: steps, y: noisyActions, name: 'Action with Noise', type: 'scatter', mode: 'lines', line: { color: '#ff7f0e' } }
        ], {
          title: 'DDPG: Deterministic vs Noisy Actions',
          xaxis: { title: 'Time Step' },
          yaxis: { title: 'Action' },
          legend: { orientation: 'h' },
          margin: { l: 40, r: 20, t: 50, b: 40 }
        });
      }
      updateDDPG();
      const ddpgSlider = document.getElementById('ddpg-noise-slider');
      if (ddpgSlider) {
        ddpgSlider.addEventListener('input', updateDDPG);
      }

      // ========== TD3 Interactive Demo ==========
      function updateTD3() {
        const slider = document.getElementById('td3-noise-slider');
        const display = document.getElementById('td3-noise-value');
        const chartDiv = document.getElementById('td3-demo');
        if (!slider || !display || !chartDiv) return;
        const noiseAmp = parseFloat(slider.value);
        display.textContent = noiseAmp.toFixed(1);
        const q1 = [];
        const q2 = [];
        const qMin = [];
        const steps = [];
        for (let i = 0; i < 100; i++) {
          steps.push(i);
          const base = Math.sin(i / 10) + 1; // shift up to keep positive
          const rand = (Math.random() - 0.5) * 2;
          q1.push(base + noiseAmp * rand);
          q2.push(base - noiseAmp * rand);
          qMin.push(Math.min(q1[i], q2[i]));
        }
        Plotly.newPlot(chartDiv, [
          { x: steps, y: q1, name: 'Q1', type: 'scatter', mode: 'lines', line: { color: '#2ca02c' } },
          { x: steps, y: q2, name: 'Q2', type: 'scatter', mode: 'lines', line: { color: '#d62728' } },
          { x: steps, y: qMin, name: 'Min(Q1, Q2)', type: 'scatter', mode: 'lines', line: { color: '#9467bd' } }
        ], {
          title: 'TD3: Double-Q and Clipped Estimates',
          xaxis: { title: 'Time Step' },
          yaxis: { title: 'Q-Value' },
          legend: { orientation: 'h' },
          margin: { l: 40, r: 20, t: 50, b: 40 }
        });
      }
      updateTD3();
      const td3Slider = document.getElementById('td3-noise-slider');
      if (td3Slider) {
        td3Slider.addEventListener('input', updateTD3);
      }

      // ========== PPO Interactive Demo ==========
      function updatePPO() {
        const slider = document.getElementById('ppo-epsilon-slider');
        const display = document.getElementById('ppo-epsilon-value');
        const chartDiv = document.getElementById('ppo-demo');
        if (!slider || !display || !chartDiv) return;
        const eps = parseFloat(slider.value);
        display.textContent = eps.toFixed(2);
        const xVals = [];
        const clipped = [];
        for (let r = 0; r <= 2.0; r += 0.02) {
          xVals.push(r);
          // clip ratio symmetrically within [1-eps, 1+eps]
          const clippedVal = Math.max(1 - eps, Math.min(r, 1 + eps));
          clipped.push(clippedVal);
        }
        Plotly.newPlot(chartDiv, [
          { x: xVals, y: xVals, name: 'Ratio', type: 'scatter', mode: 'lines', line: { color: '#17becf' } },
          { x: xVals, y: clipped, name: 'Clipped Ratio', type: 'scatter', mode: 'lines', line: { color: '#e377c2' } }
        ], {
          title: 'PPO Clipping: Ratio vs Clipped Ratio',
          xaxis: { title: 'Probability Ratio (π_θ(a|s)/π_{θ_k}(a|s))' },
          yaxis: { title: 'Value' },
          legend: { orientation: 'h' },
          margin: { l: 40, r: 20, t: 50, b: 40 }
        });
      }
      updatePPO();
      const ppoSlider = document.getElementById('ppo-epsilon-slider');
      if (ppoSlider) {
        ppoSlider.addEventListener('input', updatePPO);
      }

      // ========== SAC Interactive Demo ==========
      const sacReturns = [1.0, 0.5, -0.2, 1.2, 0.7];
      const sacEntropies = [0.8, 0.6, 1.0, 0.7, 0.9];
      function updateSAC() {
        const slider = document.getElementById('sac-alpha-slider');
        const display = document.getElementById('sac-alpha-value');
        const chartDiv = document.getElementById('sac-demo');
        if (!slider || !display || !chartDiv) return;
        const alpha = parseFloat(slider.value);
        display.textContent = alpha.toFixed(1);
        const objectives = sacReturns.map((r, i) => r + alpha * sacEntropies[i]);
        Plotly.newPlot(chartDiv, [
          { x: ['s1','s2','s3','s4','s5'], y: sacReturns, name: 'Return', type: 'bar', marker: { color: '#1f77b4' } },
          { x: ['s1','s2','s3','s4','s5'], y: objectives, name: 'Return + α·Entropy', type: 'bar', marker: { color: '#ff7f0e' } }
        ], {
          title: 'SAC Objective with Entropy Regularization',
          barmode: 'group',
          yaxis: { title: 'Value' },
          legend: { orientation: 'h' },
          margin: { l: 40, r: 20, t: 50, b: 40 }
        });
      }
      updateSAC();
      const sacSlider = document.getElementById('sac-alpha-slider');
      if (sacSlider) {
        sacSlider.addEventListener('input', updateSAC);
      }
    });
  </script>

  <!-- Custom script to collapse all details by default and to add part-level toggle buttons -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Collapse all details on load
      document.querySelectorAll('details').forEach(function(detail) {
        detail.removeAttribute('open');
      });
      // Append a toggle button to each Part heading
      document.querySelectorAll('h2').forEach(function(h2) {
        const text = h2.textContent.trim();
        if (/^Part\s+\d+/.test(text)) {
          const btn = document.createElement('button');
          btn.className = 'part-toggle';
          btn.textContent = 'Toggle All';
          btn.addEventListener('click', function() {
            // Determine whether any details under this part are closed; if so open all, otherwise close all
            let next = h2.nextElementSibling;
            let shouldOpen = false;
            while (next && next.tagName !== 'H2') {
              if (next.tagName === 'DETAILS' && !next.open) {
                shouldOpen = true;
                break;
              }
              next = next.nextElementSibling;
            }
            next = h2.nextElementSibling;
            while (next && next.tagName !== 'H2') {
              if (next.tagName === 'DETAILS') {
                next.open = shouldOpen;
              }
              next = next.nextElementSibling;
            }
          });
          h2.appendChild(btn);
        }
      });
    });
  </script>
</body>
</html>
