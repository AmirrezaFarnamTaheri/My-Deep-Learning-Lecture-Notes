<!DOCTYPE html>
<html lang="en" data-theme="cyberpunk" data-content-tier="practitioner">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="description" content="A Comprehensive, Unified, and In-Depth Guide to Deep Reinforcement Learning, from Foundations to Advanced Applications. Offline single-file edition."/>
  <meta name="keywords" content="Reinforcement Learning, Deep Reinforcement Learning, RL, DRL, DQN, Dueling DQN, Munchausen RL, Reward Shaping, PBRS, Inventory Management, Robotics, Path Planning, Markov Decision Process, MDP, Bellman Equation, Q-Learning, PPO, SAC, RLHF, MLOps, Model-Based RL, Offline RL, Decision Transformer"/>
  <title>Deep Reinforcement Learning: The Omnibus</title>
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Orbitron:wght@700;900&family=Lora:wght@400;600&family=Russo+One&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        macros: {
          // General Math & RL
          L_DQN: "\\mathcal{L}_{\\text{DQN}}",
          grad: "\\nabla_{\\theta}"
        }
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    };
  </script>

  <!-- Script: Utility panels (music, notes, timers, pomodoro) -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // Helper to set up show/hide for utility panels
      function setupUtility(name) {
        const container = document.getElementById(`${name}-container`);
        const toggleBtn = document.getElementById(`${name}-toggle-btn`);
        if (!container || !toggleBtn) return;
        toggleBtn.addEventListener('click', (e) => {
          e.stopPropagation();
          container.classList.toggle('active');
        });
        document.addEventListener('click', (e) => {
          if (!container.contains(e.target)) {
            container.classList.remove('active');
          }
        });
      }
      ['music','notes','timer','pomodoro','glossary','fontsize'].forEach(setupUtility);

      // ----- Glossary search -----
      const glossaryData = {
        "Markov Decision Process": "An MDP defines the environment in reinforcement learning. It consists of states, actions, a transition function P(s'|s,a), a reward function R(s,a) and a discount factor γ which weighs future rewards.",
        "Policy": "A policy π(a|s) maps each state to a probability distribution over actions. The goal of RL is to learn an optimal policy that maximizes expected return.",
        "Value Function": "The state-value function V_π(s) is the expected return starting from state s under policy π, while the action-value function Q_π(s,a) is the expected return starting from state s taking action a and then following π.",
        "Q-Learning": "Q-Learning is an off-policy, model-free algorithm that updates Q(s,a) toward a target consisting of the reward plus the discounted maximum next-state value. It can be expressed as Q(s,a) ← Q(s,a) + α [r + γ max_{a'} Q(s',a') - Q(s,a)].",
        "Bellman Equation": "The Bellman expectation equation expresses the value of a state as the immediate reward plus the discounted value of successor states. The Bellman optimality equation replaces the policy expectation with a maximum over actions.",
        "Double DQN": "Double DQN uses two networks to reduce overestimation by decoupling action selection from value evaluation: the main network selects the action and the target network evaluates it.",
        "Dueling Network": "A dueling architecture decomposes the Q-function into a state-value stream and an advantage stream, which are combined to compute Q(s,a). This improves learning efficiency.",
        "Distributional RL": "Distributional RL models the full distribution of returns Z(s,a) rather than just the expectation Q(s,a). It learns to approximate the distribution and uses metrics like the Wasserstein distance to update distributions.",
        "TRPO": "Trust Region Policy Optimization (TRPO) performs a constrained optimization step to maximize policy improvement while keeping the KL divergence between old and new policies below a threshold, ensuring stable updates.",
        "Centralized Training": "Centralized Training with Decentralized Execution (CTDE) is a framework where agents are trained jointly with shared information and a common critic, then deployed independently using only local observations.",
        "Intrinsic Motivation": "Intrinsic motivation techniques encourage exploration by providing internal rewards based on novelty, surprise, or curiosity, enabling unsupervised skill discovery.",
        "Model-Based RL": "Model-based RL learns an explicit model of the environment’s dynamics P(s'|s,a) and uses it for planning, imagination, or to improve sample efficiency.",
        "Meta RL": "Meta reinforcement learning trains agents to quickly adapt to new tasks by learning learning algorithms or initial parameters that generalize across tasks.",
        "Unsupervised RL": "Unsupervised RL trains agents without an explicit external reward, using intrinsic objectives like predicting future states or maximizing empowerment.",
        "Distributional Shift": "In offline RL, distributional shift occurs because the learned policy induces a different state-action distribution than the one present in the static dataset, leading to inaccurate value estimates for out-of-distribution actions.",
        "Conservative Q-Learning (CQL)": "An offline RL algorithm that mitigates distributional shift by adding a regularization term to the Q-learning objective, which penalizes high Q-values for actions not seen in the dataset, forcing the learned policy to stay close to the data distribution.",
        "World Models": "A model-based RL approach that learns a compressed latent representation of the environment's states and a predictive model of how these latent states evolve over time. This allows for efficient planning and imagination in a compact, learned state space.",
        "Decision Transformer": "An architecture that frames reinforcement learning as a sequence modeling problem. It uses a Transformer to predict future actions based on a history of states, actions, and desired returns, without relying on traditional value-based or policy gradient methods.",
      };
      const glossaryInput = document.getElementById('glossary-search-input');
      const glossaryResults = document.getElementById('glossary-results');
      if (glossaryInput) {
        glossaryInput.addEventListener('input', () => {
          const query = glossaryInput.value.trim().toLowerCase();
          glossaryResults.innerHTML = '';
          if (!query) return;
          Object.keys(glossaryData).forEach(term => {
            if (term.toLowerCase().includes(query)) {
              const entryDiv = document.createElement('div');
              const termEl = document.createElement('strong');
              termEl.textContent = term;
              entryDiv.appendChild(termEl);
              const defEl = document.createElement('p');
              defEl.textContent = glossaryData[term];
              entryDiv.appendChild(defEl);
              glossaryResults.appendChild(entryDiv);
            }
          });
        });
      }

      // ----- Music player -----
      function parseYouTubeId(url) {
        try {
          const u = new URL(url);
          if (u.hostname.includes('youtu.be')) return u.pathname.slice(1);
          if (u.hostname.includes('youtube.com')) return u.searchParams.get('v');
        } catch (err) {
          return null;
        }
        return null;
      }
      const musicLoadBtn = document.getElementById('load-music-btn');
      const musicPlayPauseBtn = document.getElementById('play-pause-btn');
      const musicUrlInput = document.getElementById('youtube-url-input');
      const musicIframe = document.getElementById('youtube-audio');
      let musicPlaying = false;
      if (musicLoadBtn) {
        musicLoadBtn.addEventListener('click', () => {
          const id = parseYouTubeId(musicUrlInput.value.trim());
          if (id) {
            // Set embed without autoplay
            musicIframe.src = `https://www.youtube.com/embed/${id}?enablejsapi=1&rel=0&controls=0`;
            musicIframe.style.display = 'none';
            musicPlaying = false;
            musicPlayPauseBtn.disabled = false;
            musicPlayPauseBtn.textContent = 'Play';
          } else {
            alert('Please enter a valid YouTube URL.');
          }
        });
        musicPlayPauseBtn.addEventListener('click', () => {
          if (!musicIframe.src) return;
          const url = new URL(musicIframe.src);
          if (musicPlaying) {
            url.searchParams.set('autoplay','0');
            musicIframe.src = url.href;
            musicPlaying = false;
            musicPlayPauseBtn.textContent = 'Play';
          } else {
            url.searchParams.set('autoplay','1');
            musicIframe.src = url.href;
            musicPlaying = true;
            musicPlayPauseBtn.textContent = 'Pause';
          }
        });
      }

      // ----- Notes -----
      const notesTextarea = document.getElementById('notes-text');
      const saveNotesBtn = document.getElementById('save-notes-btn');
      const clearNotesBtn = document.getElementById('clear-notes-btn');
      if (notesTextarea) {
        // Load saved notes from localStorage
        const storedNotes = localStorage.getItem('userNotes');
        if (storedNotes) notesTextarea.value = storedNotes;
        saveNotesBtn.addEventListener('click', () => {
          localStorage.setItem('userNotes', notesTextarea.value);
          alert('Notes saved locally.');
        });
        clearNotesBtn.addEventListener('click', () => {
          notesTextarea.value = '';
          localStorage.removeItem('userNotes');
        });
      }

      // ----- Active time -----
      const activeDisplay = document.getElementById('active-time-display');
      const resetActiveBtn = document.getElementById('reset-active-timer-btn');
      let activeSeconds = 0;
      let activeInterval;
      function startActiveTimer() {
        if (activeInterval) clearInterval(activeInterval);
        activeInterval = setInterval(() => {
          if (!document.hidden) {
            activeSeconds += 1;
            const hours = String(Math.floor(activeSeconds / 3600)).padStart(2,'0');
            const minutes = String(Math.floor((activeSeconds % 3600) / 60)).padStart(2,'0');
            const seconds = String(activeSeconds % 60).padStart(2,'0');
            activeDisplay.textContent = `${hours}:${minutes}:${seconds}`;
          }
        }, 1000);
      }
      if (activeDisplay) {
        startActiveTimer();
        resetActiveBtn.addEventListener('click', () => {
          activeSeconds = 0;
          activeDisplay.textContent = '00:00:00';
        });
      }

      // ----- Pomodoro -----
      const pomStateEl = document.getElementById('pomodoro-state');
      const pomDisplayEl = document.getElementById('pomodoro-display');
      const pomStartBtn = document.getElementById('pomodoro-start-btn');
      const pomPauseBtn = document.getElementById('pomodoro-pause-btn');
      const pomResetBtn = document.getElementById('pomodoro-reset-btn');
      const pomSettingsSelect = document.getElementById('pomodoro-settings');
      let pomSettings = { work: 25, break: 5, cycles: 4 };
      let pomSeconds = pomSettings.work * 60;
      let pomCycle = 0;
      let pomPhase = 'work'; // 'work' or 'break'
      let pomInterval;
      function parseSettings(value) {
        const [w,b,c] = value.split('-').map(v => parseInt(v,10));
        return { work: w, break: b, cycles: c };
      }
      function updatePomDisplay() {
        const minutes = String(Math.floor(pomSeconds / 60)).padStart(2,'0');
        const seconds = String(pomSeconds % 60).padStart(2,'0');
        pomDisplayEl.textContent = `${minutes}:${seconds}`;
        pomStateEl.textContent = pomPhase === 'work' ? `Work Cycle ${pomCycle + 1}/${pomSettings.cycles}` : 'Break';
      }
      function startPomodoro() {
        if (pomInterval) return;
        pomInterval = setInterval(() => {
          if (pomSeconds > 0) {
            pomSeconds -= 1;
            updatePomDisplay();
          } else {
            if (pomPhase === 'work') {
              pomPhase = 'break';
              pomSeconds = pomSettings.break * 60;
            } else {
              pomPhase = 'work';
              pomCycle += 1;
              if (pomCycle >= pomSettings.cycles) {
                clearInterval(pomInterval);
                pomInterval = null;
                pomStateEl.textContent = 'Completed';
                pomDisplayEl.textContent = '00:00';
                return;
              }
              pomSeconds = pomSettings.work * 60;
            }
            updatePomDisplay();
          }
        },1000);
      }
      if (pomDisplayEl) {
        updatePomDisplay();
        pomStartBtn.addEventListener('click', () => { startPomodoro(); });
        pomPauseBtn.addEventListener('click', () => {
          if (pomInterval) {
            clearInterval(pomInterval);
            pomInterval = null;
          }
        });
        pomResetBtn.addEventListener('click', () => {
          if (pomInterval) {
            clearInterval(pomInterval);
            pomInterval = null;
          }
          pomCycle = 0;
          pomPhase = 'work';
          pomSettings = parseSettings(pomSettingsSelect.value);
          pomSeconds = pomSettings.work * 60;
          pomStateEl.textContent = 'Ready';
          updatePomDisplay();
        });
        pomSettingsSelect.addEventListener('change', () => {
          pomSettings = parseSettings(pomSettingsSelect.value);
          pomResetBtn.click();
        });
      }
    });
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10.9.0/dist/mermaid.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js"></script>
  <script src="https://cdn.plot.ly/plotly-3.0.1.min.js" charset="utf-8"></script>
  <script type="module">
    import {Runtime, Inspector} from "https://cdn.jsdelivr.net/npm/@observablehq/runtime@5";
    import {Library} from "https://cdn.jsdelivr.net/npm/@observablehq/stdlib@5";
    window.Observable = { Runtime, Inspector, Library };
  </script>

  <style>
    /* --- DESIGN SYSTEM & THEMES --- */
    :root {
      /* Transitions & Base Properties */
      --radius-sm: 4px;
      --radius-md: 8px;
      --radius-lg: 16px;
      --transition-fast: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
      --transition-medium: all 0.3s ease-in-out;
      --shadow-inset: inset 0 2px 4px 0 rgba(0,0,0,0.5);
    }
    
    /* 1. CYBERPUNK (Default Dark) */
    html[data-theme="cyberpunk"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Orbitron', sans-serif;
      --color-bg-primary: #0a0a14;
      --color-bg-secondary: #101222;
      --color-surface: rgba(22, 25, 48, 0.7);
      --color-surface-solid: #161930;
      --color-border: rgba(76, 89, 186, 0.3);
      --color-border-hover: rgba(121, 128, 255, 0.7);
      --color-shadow: rgba(0, 0, 0, 0.5);
      --color-text-primary: #e0e1ff;
      --color-text-secondary: #a0a3d4;
      --color-text-muted: #6a6f9a;
      --color-text-headings: #ffffff;
      --color-accent-primary: #00f7ff;
      --color-accent-secondary: #f000ff;
      --color-accent-success: #00ff9c;
      --color-accent-warning: #ffc700;
      --color-accent-danger: #ff3c5b;
      --shadow-glow-primary: 0 0 12px 0 rgba(0, 247, 255, 0.4);
      --shadow-glow-secondary: 0 0 12px 0 rgba(240, 0, 255, 0.4);
      --backdrop-blur: blur(10px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 2. SOLARIZED DARK */
    html[data-theme="solarized-dark"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #002b36;
      --color-bg-secondary: #073642;
      --color-surface: rgba(7, 54, 66, 0.8);
      --color-surface-solid: #073642;
      --color-border: #2a5f6f;
      --color-border-hover: #586e75;
      --color-shadow: rgba(0, 0, 0, 0.3);
      --color-text-primary: #839496;
      --color-text-secondary: #657b83;
      --color-text-muted: #586e75;
      --color-text-headings: #93a1a1;
      --color-accent-primary: #268bd2;
      --color-accent-secondary: #6c71c4;
      --color-accent-success: #859900;
      --color-accent-warning: #cb4b16;
      --color-accent-danger: #dc322f;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(10px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 3. SOLARIZED LIGHT */
    html[data-theme="solarized-light"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #fdf6e3;
      --color-bg-secondary: #eee8d5;
      --color-surface: rgba(238, 232, 213, 0.8);
      --color-surface-solid: #eee8d5;
      --color-border: #dcd6c4;
      --color-border-hover: #93a1a1;
      --color-shadow: rgba(0, 0, 0, 0.1);
      --color-text-primary: #657b83;
      --color-text-secondary: #839496;
      --color-text-muted: #93a1a1;
      --color-text-headings: #586e75;
      --color-accent-primary: #268bd2;
      --color-accent-secondary: #6c71c4;
      --color-accent-success: #859900;
      --color-accent-warning: #cb4b16;
      --color-accent-danger: #dc322f;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(10px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 4. NORD */
    html[data-theme="nord"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #2E3440;
      --color-bg-secondary: #3B4252;
      --color-surface: rgba(59, 66, 82, 0.8);
      --color-surface-solid: #3B4252;
      --color-border: #4C566A;
      --color-border-hover: #D8DEE9;
      --color-shadow: rgba(0, 0, 0, 0.2);
      --color-text-primary: #D8DEE9;
      --color-text-secondary: #E5E9F0;
      --color-text-muted: #81A1C1;
      --color-text-headings: #ECEFF4;
      --color-accent-primary: #88C0D0;
      --color-accent-secondary: #81A1C1;
      --color-accent-success: #A3BE8C;
      --color-accent-warning: #EBCB8B;
      --color-accent-danger: #BF616A;
      --shadow-glow-primary: 0 0 8px 0 rgba(136, 192, 208, 0.3);
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(8px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 5. DRACULA */
    html[data-theme="dracula"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #282a36;
      --color-bg-secondary: #1e1f29;
      --color-surface: rgba(68, 71, 90, 0.8);
      --color-surface-solid: #44475a;
      --color-border: #6272a4;
      --color-border-hover: #bd93f9;
      --color-shadow: rgba(0, 0, 0, 0.3);
      --color-text-primary: #f8f8f2;
      --color-text-secondary: #bd93f9;
      --color-text-muted: #6272a4;
      --color-text-headings: #f8f8f2;
      --color-accent-primary: #bd93f9;
      --color-accent-secondary: #ff79c6;
      --color-accent-success: #50fa7b;
      --color-accent-warning: #f1fa8c;
      --color-accent-danger: #ff5555;
      --shadow-glow-primary: 0 0 8px 0 rgba(189, 147, 249, 0.4);
      --shadow-glow-secondary: 0 0 8px 0 rgba(255, 121, 198, 0.4);
      --backdrop-blur: blur(8px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 6. GRUVBOX DARK */
    html[data-theme="gruvbox-dark"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #282828;
      --color-bg-secondary: #1d2021;
      --color-surface: rgba(60, 56, 54, 0.8);
      --color-surface-solid: #3c3836;
      --color-border: #504945;
      --color-border-hover: #bdae93;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #ebdbb2;
      --color-text-secondary: #d5c4a1;
      --color-text-muted: #928374;
      --color-text-headings: #fbf1c7;
      --color-accent-primary: #83a598;
      --color-accent-secondary: #d65d0e;
      --color-accent-success: #b8bb26;
      --color-accent-warning: #fe8019;
      --color-accent-danger: #fb4934;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: none;
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 7. MONOKAI PRO */
    html[data-theme="monokai"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #272822;
      --color-bg-secondary: #191a14;
      --color-surface: rgba(62, 61, 50, 0.8);
      --color-surface-solid: #3E3D32;
      --color-border: #555650;
      --color-border-hover: #fcfcfa;
      --color-shadow: rgba(0, 0, 0, 0.5);
      --color-text-primary: #F8F8F2;
      --color-text-secondary: #c2c2bf;
      --color-text-muted: #75715E;
      --color-text-headings: #fcfcfa;
      --color-accent-primary: #66D9EF; /* Cyan */
      --color-accent-secondary: #F92672; /* Pink */
      --color-accent-success: #A6E22E; /* Green */
      --color-accent-warning: #FD971F; /* Orange */
      --color-accent-danger: #F92672;
      --shadow-glow-primary: 0 0 10px 0 rgba(102, 217, 239, 0.5);
      --shadow-glow-secondary: 0 0 10px 0 rgba(249, 38, 114, 0.5);
      --backdrop-blur: blur(5px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 8. GITHUB DARK */
    html[data-theme="github-dark"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #0d1117;
      --color-bg-secondary: #010409;
      --color-surface: rgba(22, 27, 34, 0.8);
      --color-surface-solid: #161b22;
      --color-border: #30363d;
      --color-border-hover: #8b949e;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #c9d1d9;
      --color-text-secondary: #8b949e;
      --color-text-muted: #484f58;
      --color-text-headings: #f0f6fc;
      --color-accent-primary: #58a6ff;
      --color-accent-secondary: #a5d6ff;
      --color-accent-success: #3fb950;
      --color-accent-warning: #d29922;
      --color-accent-danger: #f85149;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(8px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 9. ACADEMIC LIGHT */
    html[data-theme="academic-light"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Lora', serif;
      --color-bg-primary: #fbfaf7;
      --color-bg-secondary: #ffffff;
      --color-surface: rgba(255, 255, 255, 0.8);
      --color-surface-solid: #ffffff;
      --color-border: #e0e0e0;
      --color-border-hover: #b0b0b0;
      --color-shadow: rgba(0, 0, 0, 0.1);
      --color-text-primary: #333333;
      --color-text-secondary: #555555;
      --color-text-muted: #777777;
      --color-text-headings: #000000;
      --color-accent-primary: #00529B;
      --color-accent-secondary: #1E88E5;
      --color-accent-success: #006400;
      --color-accent-warning: #E65100;
      --color-accent-danger: #B71C1C;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: none;
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 10. MATRIX */
    html[data-theme="matrix"] {
      --font-sans: 'JetBrains Mono', monospace;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'JetBrains Mono', monospace;
      --color-bg-primary: #000000;
      --color-bg-secondary: #050505;
      --color-surface: rgba(0, 20, 0, 0.5);
      --color-surface-solid: #001400;
      --color-border: #003B00;
      --color-border-hover: #00FF41;
      --color-shadow: rgba(0, 255, 65, 0.3);
      --color-text-primary: #00FF41;
      --color-text-secondary: #00c732;
      --color-text-muted: #008f23;
      --color-text-headings: #c0ffc0;
      --color-accent-primary: #00FF41;
      --color-accent-secondary: #50fa7b;
      --color-accent-success: #00FF41;
      --color-accent-warning: #c0ffc0;
      --color-accent-danger: #c0ffc0;
      --shadow-glow-primary: 0 0 10px 0 var(--color-accent-primary);
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(2px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }
    
    /* 11. 80S SYNTHWAVE */
    html[data-theme="synthwave"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Russo One', sans-serif;
      --color-bg-primary: #24133f;
      --color-bg-secondary: #1a0e2d;
      --color-surface: rgba(45, 27, 83, 0.7);
      --color-surface-solid: #2d1b53;
      --color-border: rgba(255, 107, 222, 0.3);
      --color-border-hover: #ff6bde;
      --color-shadow: rgba(0, 0, 0, 0.5);
      --color-text-primary: #f0e8ff;
      --color-text-secondary: #c8bde8;
      --color-text-muted: #82799f;
      --color-text-headings: #ffffff;
      --color-accent-primary: #f92a95; /* Hot Pink */
      --color-accent-secondary: #00e8f5; /* Cyan */
      --color-accent-success: #72f1b8;
      --color-accent-warning: #f9a62a;
      --color-accent-danger: #ff4c65;
      --shadow-glow-primary: 0 0 12px 0 rgba(249, 42, 149, 0.6);
      --shadow-glow-secondary: 0 0 12px 0 rgba(0, 232, 245, 0.6);
      --backdrop-blur: blur(8px);
      --bg-image: 
        linear-gradient(rgba(36, 19, 63, 0.9), rgba(36, 19, 63, 0.9)),
        linear-gradient(to bottom, transparent 98%, var(--color-accent-secondary) 100%);
      --bg-size: 100%, 100% 4px;
    }

    /* 12. BLUEPRINT */
    html[data-theme="blueprint"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'JetBrains Mono', monospace;
      --color-bg-primary: #0a234f;
      --color-bg-secondary: #081c40;
      --color-surface: rgba(10, 48, 102, 0.5);
      --color-surface-solid: #0a3066;
      --color-border: #3a75c4;
      --color-border-hover: #a6d5ff;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #a6d5ff;
      --color-text-secondary: #73b3f2;
      --color-text-muted: #4e83ba;
      --color-text-headings: #ffffff;
      --color-accent-primary: #ffffff;
      --color-accent-secondary: #ffe180;
      --color-accent-success: #a6d5ff;
      --color-accent-warning: #ffe180;
      --color-accent-danger: #ffabab;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: none;
      --bg-image: 
        linear-gradient(rgba(58,117,196,0.15) 1px, transparent 1px),
        linear-gradient(90deg, rgba(58,117,196,0.15) 1px, transparent 1px);
      --bg-size: 2rem 2rem;
    }

    /* 13. CATPPUCCIN LATTE (Pastel Light) */
    html[data-theme="catppuccin-latte"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #eff1f5; /* Base */
      --color-bg-secondary: #e6e9ef; /* Mantle */
      --color-surface: rgba(204, 208, 218, 0.5); /* Surface overlay */
      --color-surface-solid: #ccd0da; /* Surface solid */
      --color-border: #9ca0b0; /* Overlay 0 */
      --color-border-hover: #7287fd; /* Lavender accent for hover */
      --color-shadow: rgba(0,0,0,0.1);
      --color-text-primary: #4c4f69; /* Text */
      --color-text-secondary: #6c6f85; /* Subtext 0 */
      --color-text-muted: #9ca0b0; /* Overlay 0 */
      --color-text-headings: #8839ef; /* Mauve heading */
      --color-accent-primary: #04a5e5; /* Sky blue accent */
      --color-accent-secondary: #ea76cb; /* Pink secondary */
      --color-accent-success: #40a02b; /* Green */
      --color-accent-warning: #df8e1d; /* Yellow */
      --color-accent-danger: #d20f39; /* Red */
      --shadow-glow-primary: 0 0 12px 0 rgba(4,165,229,0.3);
      --shadow-glow-secondary: 0 0 12px 0 rgba(234,118,203,0.3);
      --backdrop-blur: blur(6px);
      --bg-image:
        radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 14. CATPPUCCIN MOCHA (Pastel Dark) */
    html[data-theme="catppuccin-mocha"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #1e1e2e; /* Base */
      --color-bg-secondary: #181825; /* Mantle-like dark */
      --color-surface: rgba(30, 30, 46, 0.6);
      --color-surface-solid: #302d41; /* Surface 1-ish */
      --color-border: #575268; /* Overlay 1/2 */
      --color-border-hover: #c6a0f6; /* Lavender accent */
      --color-shadow: rgba(0, 0, 0, 0.6);
      --color-text-primary: #cdd6f4; /* Text for dark */
      --color-text-secondary: #a9b1d6; /* Subtext */
      --color-text-muted: #6e6c7e; /* Surface 2 tinted */
      --color-text-headings: #f5c2e7; /* Pinkish */
      --color-accent-primary: #89dceb; /* Sky/Teal */
      --color-accent-secondary: #f5c2e7; /* Pink */
      --color-accent-success: #a6e3a1; /* Green */
      --color-accent-warning: #fab387; /* Peach */
      --color-accent-danger: #f38ba8; /* Red */
      --shadow-glow-primary: 0 0 12px 0 rgba(137,220,235,0.4);
      --shadow-glow-secondary: 0 0 12px 0 rgba(245,194,231,0.4);
      --backdrop-blur: blur(6px);
      --bg-image:
        radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 15. GRUVBOX LIGHT */
    html[data-theme="gruvbox-light"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #fbf1c7;
      --color-bg-secondary: #f9f5d7;
      --color-surface: rgba(251, 241, 199, 0.8);
      --color-surface-solid: #f3e8c1;
      --color-border: #d5c4a1;
      --color-border-hover: #83a598;
      --color-shadow: rgba(0, 0, 0, 0.1);
      --color-text-primary: #3c3836;
      --color-text-secondary: #504945;
      --color-text-muted: #7c6f64;
      --color-text-headings: #b57614;
      --color-accent-primary: #b16286;
      --color-accent-secondary: #d79921;
      --color-accent-success: #8ec07c;
      --color-accent-warning: #d79921;
      --color-accent-danger: #cc241d;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(6px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 16. AYU DARK */
    html[data-theme="ayu-dark"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #0a0e14;
      --color-bg-secondary: #0d1017;
      --color-surface: rgba(13, 16, 23, 0.7);
      --color-surface-solid: #0f141c;
      --color-border: #1f242c;
      --color-border-hover: #ffb454;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #e6e1cf;
      --color-text-secondary: #b3b1a7;
      --color-text-muted: #4d5566;
      --color-text-headings: #ff9d00;
      --color-accent-primary: #ffb454;
      --color-accent-secondary: #ff8f40;
      --color-accent-success: #a6e3a1;
      --color-accent-warning: #ffa500;
      --color-accent-danger: #f07178;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(6px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 17. PALENIGHT */
    html[data-theme="palenight"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #292d3e;
      --color-bg-secondary: #1e2235;
      --color-surface: rgba(41, 45, 62, 0.6);
      --color-surface-solid: #313552;
      --color-border: #444267;
      --color-border-hover: #82aaff;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #eefaff;
      --color-text-secondary: #c3ddfd;
      --color-text-muted: #7b88c2;
      --color-text-headings: #c792ea;
      --color-accent-primary: #82aaff;
      --color-accent-secondary: #c792ea;
      --color-accent-success: #c3e88d;
      --color-accent-warning: #ffcb6b;
      --color-accent-danger: #ff5370;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(6px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 18. ONE DARK */
    html[data-theme="one-dark"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #282c34;
      --color-bg-secondary: #21252b;
      --color-surface: rgba(40, 44, 52, 0.6);
      --color-surface-solid: #2f343f;
      --color-border: #3a3f4b;
      --color-border-hover: #61afef;
      --color-shadow: rgba(0, 0, 0, 0.4);
      --color-text-primary: #abb2bf;
      --color-text-secondary: #828997;
      --color-text-muted: #5c6370;
      --color-text-headings: #e06c75;
      --color-accent-primary: #61afef;
      --color-accent-secondary: #c678dd;
      --color-accent-success: #98c379;
      --color-accent-warning: #e5c07b;
      --color-accent-danger: #e06c75;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(6px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    
    /* 19. TOKYO NIGHT */
    html[data-theme="tokyo-night"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #1a1b26;
      --color-bg-secondary: #16161e;
      --color-surface: rgba(26, 27, 38, 0.6);
      --color-surface-solid: #2f2f46;
      --color-border: #444c71;
      --color-border-hover: #7aa2f7;
      --color-shadow: rgba(0, 0, 0, 0.5);
      --color-text-primary: #a9b1d6;
      --color-text-secondary: #9aa5ce;
      --color-text-muted: #565f89;
      --color-text-headings: #bb9af7;
      --color-accent-primary: #7aa2f7;
      --color-accent-secondary: #bb9af7;
      --color-accent-success: #9ece6a;
      --color-accent-warning: #e0af68;
      --color-accent-danger: #f7768e;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(6px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 20. MATERIAL LIGHT */
    html[data-theme="material-light"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #fafafa;
      --color-bg-secondary: #f5f5f5;
      --color-surface: rgba(250, 250, 250, 0.8);
      --color-surface-solid: #ffffff;
      --color-border: #e0e0e0;
      --color-border-hover: #2196f3;
      --color-shadow: rgba(0, 0, 0, 0.1);
      --color-text-primary: #212121;
      --color-text-secondary: #424242;
      --color-text-muted: #757575;
      --color-text-headings: #3f51b5;
      --color-accent-primary: #2196f3;
      --color-accent-secondary: #f44336;
      --color-accent-success: #4caf50;
      --color-accent-warning: #ff9800;
      --color-accent-danger: #f44336;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(6px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 21. SOLARIZED EARTH */
    html[data-theme="solarized-earth"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #fdf6e3;
      --color-bg-secondary: #eee8d5;
      --color-surface: rgba(253, 246, 227, 0.8);
      --color-surface-solid: #eee8d5;
      --color-border: #93a1a1;
      --color-border-hover: #b58900;
      --color-shadow: rgba(0, 0, 0, 0.1);
      --color-text-primary: #657b83;
      --color-text-secondary: #586e75;
      --color-text-muted: #93a1a1;
      --color-text-headings: #b58900;
      --color-accent-primary: #d33682;
      --color-accent-secondary: #cb4b16;
      --color-accent-success: #859900;
      --color-accent-warning: #b58900;
      --color-accent-danger: #dc322f;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(6px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

    /* 22. NEON NIGHT */
    html[data-theme="neon-night"] {
      --font-sans: 'Inter', sans-serif;
      --font-mono: 'JetBrains Mono', monospace;
      --font-display: 'Inter', sans-serif;
      --color-bg-primary: #0d0221;
      --color-bg-secondary: #100328;
      --color-surface: rgba(26, 6, 74, 0.7);
      --color-surface-solid: #2f0a52;
      --color-border: #450f7a;
      --color-border-hover: #ae3ec9;
      --color-shadow: rgba(0, 0, 0, 0.5);
      --color-text-primary: #e0e0ff;
      --color-text-secondary: #c7c7ff;
      --color-text-muted: #7f7aaa;
      --color-text-headings: #ff79c6;
      --color-accent-primary: #8be9fd;
      --color-accent-secondary: #bd93f9;
      --color-accent-success: #50fa7b;
      --color-accent-warning: #f1fa8c;
      --color-accent-danger: #ff5555;
      --shadow-glow-primary: none;
      --shadow-glow-secondary: none;
      --backdrop-blur: blur(6px);
      --bg-image: radial-gradient(circle at 1px 1px, var(--color-border) 1px, transparent 0);
      --bg-size: 2rem 2rem;
    }

/* --- RESET & BASE STYLES --- */
    *, *::before, *::after { 
      box-sizing: border-box; 
    }
    
    html {
      scroll-padding-top: 80px; /* Header height */
      scroll-behavior: smooth;
      font-size: 16px;
    }
    
    body {
      margin: 0;
      font-family: var(--font-sans);
      background-color: var(--color-bg-primary);
      color: var(--color-text-primary);
      line-height: 1.7;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
      background-image: var(--bg-image);
      background-size: var(--bg-size);
      transition: background-color 0.3s, color 0.3s;
    }

    /* --- THEMATIC SCROLLBAR --- */
    html {
        scrollbar-width: thin;
        scrollbar-color: var(--color-border) var(--color-bg-secondary);
    }
    ::-webkit-scrollbar {
        width: 10px;
        height: 10px;
    }
    ::-webkit-scrollbar-track {
        background: var(--color-bg-secondary);
    }
    ::-webkit-scrollbar-thumb {
        background-color: var(--color-border);
        border-radius: 5px;
        border: 2px solid var(--color-bg-secondary);
    }
    ::-webkit-scrollbar-thumb:hover {
        background-color: var(--color-border-hover);
    }
    
    /* --- TYPOGRAPHY --- */
    h1, h2, h3, h4, h5, h6 {
      font-family: var(--font-display);
      font-weight: 700;
      line-height: 1.3;
      margin: 2.5rem 0 1.5rem 0;
      color: var(--color-text-headings);
      letter-spacing: 1px;
      text-shadow: 0 0 5px var(--color-shadow);
    }
    
    h1 { 
      font-size: clamp(2.5rem, 5vw, 3.5rem); 
      font-weight: 900;
      background: linear-gradient(90deg, var(--color-accent-primary), var(--color-accent-secondary), var(--color-accent-primary));
      background-size: 200% auto;
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      animation: gradient-text 10s linear infinite;
    }
    h2 { 
      font-size: clamp(2rem, 4vw, 2.5rem); 
      border-bottom: 2px solid var(--color-border);
      padding-bottom: 0.75rem;
      margin-top: 4rem;
      border-image: linear-gradient(90deg, var(--color-accent-primary), var(--color-accent-secondary)) 1;
    }
    h3 { font-size: 1.75rem; }
    h4 { font-size: 1.25rem; }
    h5 { font-size: 1.125rem; }
    h6 { font-size: 1rem; }
    h1 .fas, h2 .fas, h3 .fas { margin-right: 0.75rem; color: var(--color-accent-primary); }

    p { 
      margin: 0 0 1.5rem 0; 
      color: var(--color-text-secondary);
      max-width: 90ch;
    }
    
    a { 
      color: var(--color-accent-primary);
      text-decoration: none;
      transition: var(--transition-fast);
      font-weight: 500;
    }
    a:hover { 
      filter: brightness(1.2);
      text-shadow: var(--shadow-glow-primary);
    }

    strong { color: var(--color-accent-warning); font-weight: 600; }
    
    /* --- THEMATIC SEPARATOR --- */
    hr {
        border: 0;
        height: 2px;
        margin: 4rem auto;
        width: 50%;
        background-image: linear-gradient(to right, transparent, var(--color-accent-primary), var(--color-accent-secondary), var(--color-accent-primary), transparent);
    }
    
    /* --- LAYOUT & MAIN COMPONENTS --- */
    .container {
      display: grid;
      grid-template-columns: 280px 1fr;
      grid-template-rows: auto 1fr auto;
      grid-template-areas:
        "header header"
        "sidebar main"
        "footer footer";
      min-height: 100vh;
      transition: grid-template-columns 0.3s ease;
    }
    .container.toc-hidden { grid-template-columns: 0px 1fr; }

    header {
      grid-area: header;
      background: var(--color-surface);
      backdrop-filter: var(--backdrop-blur);
      -webkit-backdrop-filter: var(--backdrop-blur);
      border-bottom: 1px solid var(--color-border);
      padding: 1rem 2rem;
      position: sticky;
      top: 0;
      z-index: 1000;
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 1rem;
    }
    header h1 { margin: 0; font-size: 1.5rem; }
    
    #sidebar {
      grid-area: sidebar;
      position: sticky;
      top: 73px; /* Header height */
      height: calc(100vh - 73px);
      background: var(--color-bg-secondary);
      border-right: 1px solid var(--color-border);
      padding: 1.5rem;
      overflow-y: auto;
      transition: all 0.3s ease;
    }
    
    main {
      grid-area: main;
      max-width: 1000px;
      margin: 0 auto;
      padding: 2rem 3rem;
      width: 100%;
      min-width: 0; /* Fix for grid layout shift */
    }
    
    footer {
      grid-area: footer;
      grid-column: 1 / -1; /* Ensure footer spans all columns */
      padding: 2rem;
      background: var(--color-bg-secondary);
      border-top: 1px solid var(--color-border);
      color: var(--color-text-muted);
      font-size: 0.875rem;
    }
    .footer-content {
      max-width: 1000px;
      margin: 0 auto;
      text-align: center; 
    }
    .feedback-container {
      display: flex;
      justify-content: center;
      align-items: center;
      flex-wrap: wrap;
      gap: 1rem;
      margin: 1.5rem 0;
    }
    .feedback-container p { margin: 0; }
    .footer-socials { display: inline-flex; }
    .footer-socials a {
      color: var(--color-text-muted);
      font-size: 1.3rem;
      margin: 0 0.5rem;
      text-decoration: none;
    }
    .footer-socials a:hover { color: var(--color-accent-primary); }
    .footer-meta {
      font-size: 0.875rem;
      border-top: 1px solid var(--color-border);
      padding-top: 1rem;
      margin-top: 1.5rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    /* --- UI CONTROLS (BUTTONS, ETC) --- */
    .control-button {
      background: transparent;
      border: 1px solid var(--color-border);
      color: var(--color-text-secondary);
      font-size: 1.2rem;
      cursor: pointer;
      width: 44px;
      height: 44px;
      border-radius: var(--radius-md);
      display: flex;
      align-items: center;
      justify-content: center;
      transition: var(--transition-fast);
      position: relative;
    }
    .control-button:hover {
      color: var(--color-accent-primary);
      border-color: var(--color-accent-primary);
      box-shadow: var(--shadow-glow-primary);
      transform: translateY(-2px);
    }
    
    #search-container {
      flex-grow: 1;
      max-width: 400px;
      position: relative;
    }
    #search-input {
      width: 100%;
      background: var(--color-bg-primary);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-md);
      padding: 0.5rem 1rem 0.5rem 2.5rem;
      color: var(--color-text-primary);
      font-family: var(--font-sans);
      transition: var(--transition-fast);
    }
    #search-input:focus {
      outline: none;
      border-color: var(--color-accent-primary);
      box-shadow: var(--shadow-glow-primary);
    }
    #search-container .fa-search {
      position: absolute;
      left: 1rem;
      top: 50%;
      transform: translateY(-50%);
      color: var(--color-text-muted);
    }
    
    /* Theme Switcher */
    #theme-switcher {
      position: absolute;
      top: calc(100% + 10px);
      right: 0;
      background: var(--color-surface-solid);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-md);
      padding: 0.5rem;
      box-shadow: 0 10px 20px var(--color-shadow);
      z-index: 1001;
      opacity: 0;
      visibility: hidden;
      transform: translateY(10px);
      transition: var(--transition-medium);
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
      gap: 0.25rem;
      width: 260px;
      max-height: 300px;
      overflow-y: auto;
    }
    #theme-toggle-btn-container.active #theme-switcher {
        opacity: 1;
        visibility: visible;
        transform: translateY(0);
    }
    .theme-option {
      padding: 0.5rem 1rem;
      border-radius: var(--radius-sm);
      cursor: pointer;
      transition: var(--transition-fast);
      text-align: left;
      font-size: 0.9rem;
      color: var(--color-text-secondary);
      background: none;
      border: none;
      width: 100%;
    }
    .theme-option:hover, .theme-option.active {
      background: var(--color-surface);
      color: var(--color-accent-primary);
    }

  /* Redesigned part-toggle button to match control-button style */
  h2 > button.part-toggle {
    background: transparent;
    border: 1px solid var(--color-border);
    color: var(--color-text-secondary);
    font-size: 1rem; /* Adjust size to fit h2 */
    cursor: pointer;
    width: 36px; /* Smaller than header buttons */
    height: 36px;
    border-radius: var(--radius-md);
    display: inline-flex;
    align-items: center;
    justify-content: center;
    transition: var(--transition-fast);
    position: relative;
    vertical-align: middle;
    margin-left: 1rem;
  }
  h2 > button.part-toggle:hover {
      color: var(--color-accent-primary);
      border-color: var(--color-accent-primary);
      box-shadow: var(--shadow-glow-primary);
      transform: translateY(-2px);
  }

  /* Utility panel containers */
  .utility-container {
    position: relative;
  }
  .utility-panel {
    position: absolute;
    top: calc(100% + 10px);
    right: 0;
    background: var(--color-surface-solid);
    border: 1px solid var(--color-border);
    border-radius: var(--radius-md);
    padding: 0.75rem;
    box-shadow: 0 10px 20px var(--color-shadow);
    z-index: 1001;
    opacity: 0;
    visibility: hidden;
    transform: translateY(10px);
    transition: var(--transition-medium);
    width: 260px;
    max-width: 90vw;
  }
  .utility-container.active .utility-panel {
    opacity: 1;
    visibility: visible;
    transform: translateY(0);
  }
  .utility-panel h3 {
    margin-top: 0;
    margin-bottom: 0.5rem;
  }
  #notes-panel textarea {
    width: 100%;
    height: 160px;
    padding: 0.5rem;
    border: 1px solid var(--color-border);
    border-radius: var(--radius-md);
    background: var(--color-bg-primary);
    color: var(--color-text-primary);
    font-family: var(--font-mono);
    resize: vertical;
  }
  .notes-controls, .pomodoro-controls {
    display: flex;
    gap: 0.5rem;
    margin-top: 0.5rem;
  }
  .utility-panel button {
    padding: 0.35rem 0.75rem;
    border: 1px solid var(--color-border);
    background: var(--color-surface);
    border-radius: var(--radius-sm);
    cursor: pointer;
    color: var(--color-text-primary);
    transition: var(--transition-fast);
    font-size: 0.85rem;
  }
  .utility-panel button:hover {
    background: var(--color-border-hover);
    color: var(--color-text-headings);
  }
  #pomodoro-settings {
    width: 100%;
    margin-top: 0.5rem;
    padding: 0.35rem;
    border-radius: var(--radius-sm);
    border: 1px solid var(--color-border);
    background: var(--color-bg-primary);
    color: var(--color-text-primary);
  }
  #active-time-display, #pomodoro-display {
    font-weight: bold;
  }
    
    /* Tiered Content Toggle */
    #tier-switcher {
        display: flex;
        gap: 0.5rem;
        background: var(--color-surface-solid);
        padding: 0.25rem;
        border-radius: var(--radius-md);
        border: 1px solid var(--color-border);
    }
    .tier-option {
        padding: 0.3rem 0.8rem;
        border-radius: var(--radius-sm);
        cursor: pointer;
        transition: var(--transition-fast);
        font-size: 0.8rem;
        color: var(--color-text-secondary);
        background: none;
        border: none;
        font-family: var(--font-mono);
    }
    .tier-option:hover, .tier-option.active {
        background: var(--color-surface);
        color: var(--color-accent-primary);
        box-shadow: var(--shadow-glow-primary);
    }


    /* --- SIDEBAR NAVIGATION --- */
    #sidebar-nav { list-style: none; padding: 0; margin: 0; }
    #sidebar-nav ul { padding-left: 1rem; list-style: none; border-left: 1px solid var(--color-border); }
    #sidebar-nav li a {
      display: block;
      padding: 0.4rem 0.75rem;
      border-radius: var(--radius-sm);
      color: var(--color-text-secondary);
      font-weight: 500;
      font-size: 0.9rem;
      transition: var(--transition-fast);
      position: relative;
    }
    #sidebar-nav > li > a { font-weight: 700; color: var(--color-text-headings); font-size: 1rem; }
    #sidebar-nav li a:hover, #sidebar-nav li a.active {
      background: var(--color-surface);
      color: var(--color-accent-primary);
      transform: translateX(4px);
    }
    #sidebar-nav li a.active::before {
      content: '';
      position: absolute;
      left: -1rem;
      top: 0;
      bottom: 0;
      width: 2px;
      background-color: var(--color-accent-primary);
      box-shadow: var(--shadow-glow-primary);
    }

    /* --- CODE & MATH --- */
    code, pre { font-family: var(--font-mono); font-size: 0.9em; }
    :not(pre) > code {
      background: var(--color-surface);
      color: var(--color-accent-warning);
      padding: 0.2rem 0.4rem;
      border-radius: var(--radius-sm);
      border: 1px solid var(--color-border);
    }
    .code-container {
        margin: 1.5rem 0;
        border-radius: var(--radius-md);
        border: 1px solid var(--color-border);
        overflow: hidden;
        background: var(--color-surface);
    }
    pre {
      padding: 0;
      margin: 0;
      overflow: hidden;
      box-shadow: var(--shadow-inset);
      position: relative;
    }
    pre > code {
      display: block;
      padding: 1.5rem;
      overflow-x: auto;
    }
    .code-block-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      background-color: var(--color-surface-solid);
      padding: 0.5rem 1rem;
      border-bottom: 1px solid var(--color-border);
    }
    .code-block-header span { font-family: var(--font-mono); color: var(--color-text-muted); text-transform: uppercase; font-size: 0.8rem; }
    .code-block-header .buttons { display: flex; gap: 0.5rem; }
    .code-block-header button, .code-block-header a {
      font-size: 0.8rem;
      padding: 0.25rem 0.75rem;
      background: var(--color-surface);
      color: var(--color-text-secondary);
      border: 1px solid var(--color-border);
      border-radius: var(--radius-sm);
      display: flex;
      align-items: center;
      gap: 0.5rem;
      cursor: pointer;
      transition: var(--transition-fast);
      line-height: 1;
      font-family: var(--font-sans);
      text-decoration: none;
    }
    .code-block-header button:hover, .code-block-header a:hover { 
        background: var(--color-surface-solid);
        color: var(--color-text-primary);
        border-color: var(--color-border-hover);
        transform: translateY(-1px); 
    }
    .code-variants summary {
        background: none;
        padding: 0.5rem 1rem;
        font-size: 0.9rem;
        color: var(--color-text-muted);
    }
    .code-variants .details-content {
        padding: 0;
        border-top: 1px solid var(--color-border);
    }
    
    /* --- INTERACTIVE & UI ELEMENTS --- */
    details {
      border: 1px solid var(--color-border);
      border-radius: var(--radius-md);
      margin: 1.5rem 0;
      background: var(--color-surface);
      backdrop-filter: var(--backdrop-blur);
      -webkit-backdrop-filter: var(--backdrop-blur);
      transition: var(--transition-medium);
      overflow: hidden;
    }
    details:hover { box-shadow: 0 0 15px var(--color-shadow); }
    summary {
      cursor: pointer;
      font-weight: 600;
      padding: 1.25rem 1.5rem;
      display: flex;
      align-items: center;
      justify-content: space-between;
      color: var(--color-text-headings);
      background: rgba(0,0,0,0.1);
    }
    summary:hover { background: rgba(0,0,0,0.2); }
    summary::after {
      content: '▼';
      font-size: 0.875rem;
      color: var(--color-text-muted);
      transition: transform 0.3s ease, color 0.3s ease;
    }
    details[open] > summary::after { transform: rotate(180deg); color: var(--color-accent-primary); }
    .details-content { padding: 0.5rem 1.5rem 1.5rem 1.5rem; }
    
    /* Tiered Content Visibility */
    html[data-content-tier="beginner"] .deep-dive,
    html[data-content-tier="beginner"] .research-tier { display: none; }
    html[data-content-tier="practitioner"] .research-tier { display: none; }
    .deep-dive summary, .research-tier summary {
        font-style: italic;
    }

    /* Tooltips */
    .tooltip {
      position: relative;
      cursor: help;
      border-bottom: 1px dotted var(--color-accent-primary);
    }
    .tooltip .tooltip-text {
      visibility: hidden;
      width: 220px;
      background-color: var(--color-surface-solid);
      color: var(--color-text-primary);
      text-align: center;
      border-radius: var(--radius-md);
      padding: 8px;
      position: absolute;
      z-index: 10;
      bottom: 125%;
      left: 50%;
      margin-left: -110px; /* Use half of the width value */
      opacity: 0;
      transition: opacity 0.3s;
      border: 1px solid var(--color-border);
    }
    .tooltip:hover .tooltip-text {
      visibility: visible;
      opacity: 1;
    }
    
    /* --- ADMONITIONS --- */
    .admonition {
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-left: 4px solid;
      border-radius: var(--radius-md);
      background: var(--color-surface);
      position: relative;
    }
    .admonition-title {
      font-weight: 700;
      margin-bottom: 0.5rem;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      font-family: var(--font-display);
    }
    .admonition.info { border-color: var(--color-accent-primary); }
    .admonition.info .admonition-title { color: var(--color-accent-primary); }
    .admonition.danger { border-color: var(--color-accent-danger); }
    .admonition.danger .admonition-title { color: var(--color-accent-danger); }
    .admonition.tip { border-color: var(--color-accent-success); }
    .admonition.tip .admonition-title { color: var(--color-accent-success); }
    .admonition.warning { border-color: var(--color-accent-warning); }
    .admonition.warning .admonition-title { color: var(--color-accent-warning); }
    /* Mermaid Theme Styling */
.mermaid .agent-class { fill: var(--color-accent-primary); stroke: var(--color-bg-primary); }
.mermaid .env-class { fill: var(--color-accent-secondary); stroke: var(--color-bg-primary); }
.mermaid .equation-element { fill: var(--color-accent-primary); stroke: var(--color-bg-primary); }
.mermaid .expectation-node { fill: var(--color-accent-secondary); stroke: var(--color-bg-primary); }
.mermaid .online-class { fill: var(--color-accent-primary); stroke: var(--color-bg-primary); }
.mermaid .target-class { fill: var(--color-accent-secondary); stroke: var(--color-bg-primary); }
.mermaid .buffer-class { fill: var(--color-accent-success); }
.mermaid .network-class { fill: var(--color-accent-primary); }
.mermaid .value-class { fill: var(--color-surface); stroke: var(--color-accent-primary); }
.mermaid .advantage-class { fill: var(--color-surface); stroke: var(--color-accent-secondary); }
.mermaid .primary-method { fill: var(--color-accent-primary); }
.mermaid .advanced-method { fill: var(--color-accent-success); }
.mermaid .actor-class { fill: var(--color-accent-secondary); }
.mermaid .critic-class { fill: var(--color-accent-primary); }
.mermaid .global-class { fill: var(--color-accent-primary); stroke-width: 3px; }
.mermaid .target-class { fill: var(--color-accent-secondary); }
.mermaid .state-class { fill: var(--color-accent-primary); }
.mermaid .failure-class { fill: var(--color-accent-danger); color: white; }
.mermaid .input-class { fill: var(--color-surface); stroke: var(--color-accent-secondary); }
.mermaid .diffusion-class { fill: var(--color-accent-secondary); }
.mermaid .confounder-class { fill: var(--color-accent-danger); color: white; }
.mermaid .real-class { fill: var(--color-surface); stroke: var(--color-accent-primary); }
.mermaid .imagined-class { fill: var(--color-surface); stroke: var(--color-accent-secondary); }
.mermaid .controller-class { fill: var(--color-surface); stroke: var(--color-accent-secondary); }
.mermaid .error-class { fill: var(--color-accent-danger); color: white; }
.mermaid .model-class { fill: var(--color-accent-primary); }
.mermaid .centralized-class { fill: var(--color-surface); stroke: var(--color-accent-primary); }
.mermaid .decentralized-class { fill: var(--color-surface); stroke: var(--color-accent-secondary); }
.mermaid .meta-class { fill: var(--color-accent-primary); stroke-width: 3px; }
.mermaid .manager-class { fill: var(--color-surface); stroke: var(--color-accent-primary); }
.mermaid .worker-class { fill: var(--color-surface); stroke: var(--color-accent-secondary); }
    /* --- TABLES --- */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 2rem 0;
      background: var(--color-surface);
      border-radius: var(--radius-md);
      overflow: hidden;
      border: 1px solid var(--color-border);
    }
    th, td {
      padding: 0.75rem 1rem;
      text-align: left;
      border-bottom: 1px solid var(--color-border);
    }
    th {
      background: var(--color-surface-solid);
      font-weight: 600;
      color: var(--color-text-headings);
      font-family: var(--font-mono);
    }
    tr:last-child td { border-bottom: none; }
    tr:hover { background: var(--color-surface-solid); }

    /* --- VISUALIZATIONS & TOOLBAR --- */
    .visualization-container {
        position: relative;
        margin: 1.5rem 0;
        border: 1px solid var(--color-border);
        border-radius: var(--radius-md);
        background: var(--color-surface);
        overflow: hidden;
        padding: 1rem;
    }
    .visualization-container > .mermaid,
    .visualization-container > svg,
    .visualization-container > img {
        display: block;
        width: 100%;
        height: auto;
        cursor: grab;
        transition: transform 0.2s ease-out;
    }
    .visualization-container > .mermaid:active,
    .visualization-container > svg:active,
    .visualization-container > img:active {
        cursor: grabbing;
    }
    .vis-toolbar {
        position: absolute;
        top: 1rem;
        right: 1rem;
        z-index: 10;
        display: flex;
        gap: 0.5rem;
        background: var(--color-surface-solid);
        padding: 0.5rem;
        border-radius: var(--radius-md);
        border: 1px solid var(--color-border);
        opacity: 0;
        visibility: hidden;
        transform: translateY(-10px);
        transition: var(--transition-medium);
    }
    .visualization-container:hover .vis-toolbar {
        opacity: 1;
        visibility: visible;
        transform: translateY(0);
    }
    .vis-toolbar button {
        background: var(--color-surface);
        border: 1px solid var(--color-border);
        color: var(--color-text-secondary);
        width: 32px;
        height: 32px;
        border-radius: var(--radius-sm);
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 0.9rem;
        transition: var(--transition-fast);
    }
    .vis-toolbar button:hover {
        color: var(--color-accent-primary);
        border-color: var(--color-accent-primary);
        box-shadow: var(--shadow-glow-primary);
    }

    /* --- INTERACTIVE LAB --- */
    .interactive-lab {
        border: 1px solid var(--color-border);
        border-radius: var(--radius-lg);
        background: var(--color-bg-secondary);
        padding: 1.5rem;
        margin-top: 2rem;
    }
    .lab-header {
        font-family: var(--font-display);
        color: var(--color-accent-secondary);
        font-size: 1.5rem;
        margin-bottom: 1rem;
        display: flex;
        align-items: center;
    }
    .lab-header .fas {
        margin-right: 0.75rem;
    }
    .lab-controls {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-bottom: 1.5rem;
        align-items: center;
    }
    .lab-control {
        display: flex;
        flex-direction: column;
        gap: 0.5rem;
    }
    .lab-control label {
        font-family: var(--font-mono);
        font-size: 0.9rem;
        color: var(--color-text-muted);
    }
    .lab-control input[type="range"] {
        width: 200px;
    }
    
    /* --- OTHER UI --- */
    #back-to-top {
        position: fixed;
        bottom: 2rem;
        right: 2rem;
        z-index: 100;
        opacity: 0;
        visibility: hidden;
        transform: translateY(20px);
        transition: var(--transition-medium);
    }
    #back-to-top.visible {
        opacity: 1;
        visibility: visible;
        transform: translateY(0);
    }
    
    mark {
        background-color: var(--color-accent-warning);
        color: var(--color-bg-primary);
        padding: 2px 4px;
        border-radius: var(--radius-sm);
    }
    
    /* --- PROGRESS BAR & ANIMATIONS --- */
    .progress-bar {
      position: fixed;
      top: 0;
      left: 0;
      height: 3px;
      background: linear-gradient(90deg, var(--color-accent-primary), var(--color-accent-secondary));
      box-shadow: var(--shadow-glow-primary);
      z-index: 9999;
      transition: width 0.1s linear;
    }
    @keyframes gradient-text { 
      0% { background-position: 0% 50%; } 
      50% { background-position: 100% 50%; } 
      100% { background-position: 0% 50%; } 
    }
    @keyframes scanline {
      0% { background-position: 0 0; }
      100% { background-position: 0 100%; }
    }
    html[data-theme="synthwave"] body::after {
        content: '';
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background-image: linear-gradient(to bottom, transparent 98%, var(--color-accent-secondary) 100%);
        background-size: 100% 4px;
        z-index: -1;
        pointer-events: none;
        animation: scanline 8s linear infinite;
    }
    
    .fade-in {
      animation: fadeIn 0.5s ease-out forwards;
    }
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(15px); }
      to { opacity: 1; transform: translateY(0); }
    }
    
    /* --- RESPONSIVENESS --- */
    @media (max-width: 1024px) {
      .container {
        grid-template-columns: 1fr;
        grid-template-areas:
          "header"
          "main"
          "footer";
      }
      #sidebar { display: none; }
      #toc-toggle-btn { display: none; }
      main { padding: 1.5rem; }
      header { padding: 1rem; }
      #search-container { max-width: 250px; }
    }
    @media (max-width: 768px) {
      h1 { font-size: 2rem; }
      h2 { font-size: 1.75rem; }
      main { padding: 1rem; }
      header { flex-wrap: wrap; justify-content: center; gap: 0.5rem; row-gap: 1rem; }
      header > h1 { order: -2; width: 100%; text-align: center;}
      #search-container { order: 3; width: 100%;}
      #toc-toggle-btn, #theme-toggle-btn-container { order: -1; }
      #tier-switcher { order: 2; }
    }
  
    /* Font size control buttons */
    .font-size-controls button {
      padding: 0.3rem 0.6rem;
      border: 1px solid var(--color-border);
      background: var(--color-surface);
      color: var(--color-text-primary);
      border-radius: var(--radius-sm);
      cursor: pointer;
      font-size: 0.9rem;
    }
    .font-size-controls button:hover {
      background: var(--color-border-hover);
    }
</style>
<style>
@media (max-width: 768px) {
  #main-content, #sidebar, #container, .details-content {
    width: 100% !important;
    overflow-x: auto;
  }
  input[type=range] {
    width: 100% !important;
  }
  pre, code {
    overflow-x: auto;
    word-wrap: normal;
  }
}
span.tooltip[data-tooltip] {
  position: relative;
  cursor: help;
  border-bottom: 1px dotted var(--color-accent-primary);
}
span.tooltip[data-tooltip]:before,
span.tooltip[data-tooltip]:after {
  content: "";
  position: absolute;
  visibility: hidden;
  opacity: 0;
  transition: opacity 0.2s ease-in-out;
  z-index: 10;
  pointer-events: none;
}
span.tooltip[data-tooltip]:before {
  content: attr(data-tooltip);
  background: var(--color-surface-solid);
  color: var(--color-text-primary);
  padding: 0.5rem 1rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--color-border);
  font-size: 0.85rem;
  white-space: nowrap;
  bottom: 125%;
  left: 50%;
  transform: translateX(-50%);
}
span.tooltip[data-tooltip]:hover:before,
span.tooltip[data-tooltip]:hover:after {
  visibility: visible;
  opacity: 1;
}
</style>
</head>

<body>
  <div class="progress-bar" id="progress-bar"></div>
  
  <div class="container" id="container">
    <header>
      <button id="toc-toggle-btn" class="control-button" title="Toggle Table of Contents">
        <i class="fas fa-bars"></i>
      </button>
      
      <div id="tier-switcher">
        <button class="tier-option" data-tier="beginner" title="Show content for beginners">Beginner</button>
        <button class="tier-option active" data-tier="practitioner" title="Show content for practitioners">Practitioner</button>
        <button class="tier-option" data-tier="researcher" title="Show content for researchers">Researcher</button>
      </div>
      
      <div id="search-container">
        <i class="fas fa-search"></i>
        <input type="search" id="search-input" placeholder="Search document...">
      </div>

      <div id="theme-toggle-btn-container" style="position: relative;">
        <button class="control-button" id="theme-toggle-btn" title="Change Theme">
            <i class="fas fa-palette"></i>
        </button>
        <div id="theme-switcher">
            </div>
      </div>

      <!-- Music player control -->
      <div id="music-container" class="utility-container">
        <button class="control-button" id="music-toggle-btn" title="Music Player">
          <i class="fas fa-music"></i>
        </button>
        <div id="music-player-panel" class="utility-panel">
          <label for="youtube-url-input">YouTube URL:</label>
          <input type="text" id="youtube-url-input" placeholder="Enter YouTube link..." style="width:100%;margin-top:0.5rem;padding:0.35rem;border:1px solid var(--color-border);border-radius:var(--radius-sm);background:var(--color-bg-primary);color:var(--color-text-primary);"/>
          <div class="notes-controls" style="margin-top:0.5rem;">
            <button id="load-music-btn">Load</button>
            <button id="play-pause-btn" disabled>Play</button>
          </div>
          <iframe id="youtube-audio" width="0" height="0" style="display:none;"></iframe>
        </div>
      </div>

      <!-- Note taking control -->
      <div id="notes-container" class="utility-container">
        <button class="control-button" id="notes-toggle-btn" title="Notes">
          <i class="fas fa-sticky-note"></i>
        </button>
        <div id="notes-panel" class="utility-panel">
          <label for="notes-text">Notes:</label>
          <textarea id="notes-text" rows="10"></textarea>
          <div class="notes-controls">
            <button id="save-notes-btn">Save</button>
            <button id="clear-notes-btn">Clear</button>
          </div>
        </div>
      </div>

      <!-- Active time display -->
      <div id="timer-container" class="utility-container">
        <button class="control-button" id="timer-toggle-btn" title="Active Time">
          <i class="fas fa-clock"></i>
        </button>
        <div id="timer-panel" class="utility-panel">
          <p style="margin-top:0;">Total Active Time: <span id="active-time-display">00:00:00</span></p>
          <button id="reset-active-timer-btn">Reset</button>
        </div>
      </div>

      <!-- Pomodoro timer -->
      <div id="pomodoro-container" class="utility-container">
        <button class="control-button" id="pomodoro-toggle-btn" title="Pomodoro Timer">
          <i class="fas fa-hourglass-half"></i>
        </button>
        <div id="pomodoro-panel" class="utility-panel">
          <p style="margin-top:0;">Pomodoro Session: <span id="pomodoro-state">Ready</span></p>
          <p>Time Remaining: <span id="pomodoro-display">25:00</span></p>
          <div class="pomodoro-controls">
            <button id="pomodoro-start-btn">Start</button>
            <button id="pomodoro-pause-btn">Pause</button>
            <button id="pomodoro-reset-btn">Reset</button>
          </div>
          <select id="pomodoro-settings">
            <option value="25-5-4">25/5 (4 cycles)</option>
            <option value="50-10-2">50/10 (2 cycles)</option>
            <option value="90-30-2">90/30 (2 cycles)</option>
          </select>
        </div>
      </div>
      <!-- Glossary control -->
      <div id="glossary-container" class="utility-container">
        <button class="control-button" id="glossary-toggle-btn" title="Glossary">
          <i class="fas fa-book-open"></i>
        </button>
        <div id="glossary-panel" class="utility-panel">
          <label for="glossary-search-input">Search term:</label>
          <!-- Updated placeholder to clarify glossary-specific search -->
          <input type="text" id="glossary-search-input" placeholder="Search the glossary..." style="width:100%;margin-top:0.5rem;padding:0.35rem;border:1px solid var(--color-border);border-radius:var(--radius-sm);background:var(--color-bg-primary);color:var(--color-text-primary);" />
          <div id="glossary-results" style="margin-top:0.5rem; max-height:200px; overflow-y:auto;"></div>
        </div>
      </div>
    
      <!-- Font size control -->
      <div id="fontsize-container" class="utility-container">
        <button class="control-button" id="fontsize-toggle-btn" title="Font Size">
          <i class="fas fa-text-height"></i>
        </button>
        <div id="fontsize-panel" class="utility-panel">
          <h3>Adjust Font Size</h3>
          <div class="font-size-controls" style="display:flex; align-items:center; gap:0.5rem;">
            <button id="font-decrease">A-</button>
            <span id="font-size-display" style="flex:1 1 auto; text-align:center;">16px</span>
            <button id="font-increase">A+</button>
          </div>
        </div>
      </div>
</header>

    <aside id="sidebar">
      <nav id="sidebar-nav">
        <p>Loading Table of Contents...</p>
      </nav>
    </aside>

    <main id="main-content">
      <section id="title-section">
        <h1 style="text-align: center; margin-bottom: 2rem;">
            <i class="fas fa-brain"></i>
            Deep Reinforcement Learning
        </h1>
      </section>
      <hr/>
      <section id="part0">
        <h2 id="part0-title">
          <i class="fas fa-compass"></i>
          Part 0: Orientation & Tooling
        </h2>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-hand-sparkles"></i>
            0.1 Welcome & Philosophy
          </summary>
          <div class="details-content">
            <h4>
              <i class="fas fa-rocket"></i>
              Welcome to the Deep RL Omnibus!
            </h4>
            <p>
                This document is a living, comprehensive guide to Deep Reinforcement Learning, designed from the ground up for clarity, depth, and practical application. Unlike static textbooks, the Omnibus treats learning as an active process. Here, theory is not just described but visualized, and algorithms are not just presented but made transparent.
            </p>

            <h5>A Brief History of Reinforcement Learning</h5>
            <p>The ideas behind RL have a rich history, drawing from computer science, psychology, and control theory. Key milestones include Richard Bellman's work on <strong><span class="tooltip" data-tooltip="A method for solving complex problems by breaking them down into simpler sub-problems. In RL, it's used to compute optimal policies in a known MDP.">dynamic programming</span></strong> in the 1950s, which gave us the foundational Bellman equations. This was followed by the development of model-free methods like TD-learning in the 1980s by Sutton and Barto. The modern era of <em>Deep</em> RL began in earnest with DeepMind's 2015 paper demonstrating a DQN agent that could master Atari games from raw pixels, culminating in breakthroughs like AlphaGo's defeat of the world's best Go player in 2016.</p>
            
            <details class="deep-dive" open>
                <summary><i class="fas fa-timeline"></i> Deep Dive: A Visual History of RL Milestones</summary>
                <div class="details-content">
                    <div class="visualization-container">
                    <div class="vis-toolbar">
                        <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                        <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                        <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                        <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
                    </div>

                    <svg id="rl-timeline-svg" viewBox="0 0 800 160" style="width:100%;height:auto;cursor:pointer">
                        <defs>
                        <linearGradient id="tlGradient" x1="0%" y1="0%" x2="100%" y2="0%">
                            <stop offset="0%"  stop-color="var(--color-accent-primary)" stop-opacity=".25"/>
                            <stop offset="50%" stop-color="var(--color-accent-secondary)" stop-opacity=".25"/>
                            <stop offset="100%" stop-color="var(--color-accent-primary)" stop-opacity=".25"/>
                        </linearGradient>
                        </defs>
                        <rect x="0"   y="0" width="266" height="160" fill="url(#tlGradient)"/>
                        <rect x="266" y="0" width="267" height="160" fill="url(#tlGradient)"/>
                        <rect x="533" y="0" width="267" height="160" fill="url(#tlGradient)"/>
                        <circle cx="60"  cy="80" r="8" class="tl-milestone" data-tip="Bellman Equation (1957)"/>
                        <circle cx="120" cy="80" r="8" class="tl-milestone" data-tip="Samuel’s Checkers (1959)"/>
                        <circle cx="200" cy="80" r="8" class="tl-milestone" data-tip="TD(λ) & Sutton/Barto (1988)"/>
                        <circle cx="300" cy="80" r="8" class="tl-milestone" data-tip="TD-Gammon (1992)"/>
                        <circle cx="380" cy="80" r="8" class="tl-milestone" data-tip="Deadly Triad Theorems (1997)"/>
                        <circle cx="460" cy="80" r="8" class="tl-milestone" data-tip="Neural Fitted Q (2005)"/>
                        <circle cx="540" cy="80" r="8" class="tl-milestone" data-tip="DQN on Atari (2015)"/>
                        <circle cx="620" cy="80" r="8" class="tl-milestone" data-tip="AlphaGo (2016)"/>
                        <circle cx="700" cy="80" r="8" class="tl-milestone" data-tip="RLHF & Diffusion (2020s)"/>
                        <text x="60"  y="110" class="tl-label">Bellman</text>
                        <text x="120" y="110" class="tl-label">Samuel</text>
                        <text x="200" y="110" class="tl-label">TD(λ)</text>
                        <text x="300" y="110" class="tl-label">TD-Gammon</text>
                        <text x="380" y="110" class="tl-label">Deadly Triad</text>
                        <text x="460" y="110" class="tl-label">NFQ</text>
                        <text x="540" y="110" class="tl-label">DQN</text>
                        <text x="620" y="110" class="tl-label">AlphaGo</text>
                        <text x="700" y="110" class="tl-label">RLHF</text>
                        <style>
                        #rl-timeline-svg .tl-milestone{fill:var(--color-accent-primary);transition:fill .4s}
                        #rl-timeline-svg .tl-label{font-size:10px;font-family:var(--font-mono);fill:var(--color-text-primary);text-anchor:middle}
                        </style>
                    </svg>

                    <div id="tl-tooltip" style="position:absolute;top:0;left:0;background:var(--color-surface-solid);border:1px solid var(--color-border);padding:4px 8px;border-radius:4px;font-size:12px;pointer-events:none;opacity:0;transition:opacity .2s"></div>
                    </div>
                </div>

                <script>
                (function(){
                    const svg   = document.getElementById('rl-timeline-svg');
                    const tips  = {60:'Bellman Equation (1957)',120:'Samuel’s Checkers (1959)',200:'TD(λ) & Sutton/Barto (1988)',
                                300:'TD-Gammon (1992)',380:'Deadly Triad Theorems (1997)',460:'Neural Fitted Q (2005)',
                                540:'DQN on Atari (2015)',620:'AlphaGo (2016)',700:'RLHF & Diffusion (2020s)'};
                    const tooltip = document.getElementById('tl-tooltip');
                    let hue = 0;
                    setInterval(()=>{hue=(hue+1)%360;svg.querySelectorAll('.tl-milestone').forEach(c=>c.style.fill=`hsl(${hue},100%,50%)`);},50);

                    svg.addEventListener('mousemove', e=>{
                    const el = e.target;
                    if (el.classList.contains('tl-milestone')) {
                        tooltip.textContent = tips[el.getAttribute('cx')];
                        tooltip.style.left = (e.pageX+10)+'px';
                        tooltip.style.top  = (e.pageY-20)+'px';
                        tooltip.style.opacity = 1;
                    }
                    });
                    svg.addEventListener('mouseleave',()=>tooltip.style.opacity=0);
                })();
                </script>
            </details>            
            
            <div class="admonition info">
              <span class="admonition-title">
                <i class="fas fa-lightbulb"></i>
                Our Learning Philosophy
              </span>
              <ul>
                <li><strong>First-Principles Thinking:</strong> We deconstruct complex algorithms into their core mathematical and intuitive components. You won't just learn <em>what</em> works, but <em>why</em> it works.</li>
                <li><strong>Visual Intuition:</strong> Abstract concepts are made tangible through animated diagrams, interactive plots, and clear visualizations. We believe seeing a concept in action is as important as reading its definition.</li>
                <li><strong>Progressive Disclosure:</strong> Information unfolds naturally. We start with high-level intuition, move to the formal mathematics, and then dive into implementation details, allowing you to learn at your preferred level of depth.</li>
                <li><strong>Practicality First:</strong> While rigorously mathematical, our focus is on building practical understanding. All code is annotated with implementation notes, common pitfalls, and best practices.</li>
              </ul>
            </div>

            <h4>
              <i class="fas fa-users"></i>
              Who This Is For & How to Navigate
            </h4>
            <p>This guide is crafted for a diverse audience. You can use the toggles in the header to filter content based on your learning goals. The main text follows the <strong>Practitioner Path</strong>, while collapsible blocks offer deeper dives for researchers or simplified explanations for beginners.</p>

            <div class="visualization-container">
            <div class="vis-toolbar">
                <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
            </div>

            <svg id="persona-path-svg" viewBox="0 0 600 120" style="width:100%">
                <defs>
                <linearGradient id="personaGrad" x1="0%" y1="0%" x2="100%" y2="0%">
                    <stop offset="0%"  stop-color="var(--color-accent-success)"/>
                    <stop offset="50%" stop-color="var(--color-accent-primary)"/>
                    <stop offset="100%" stop-color="var(--color-accent-secondary)"/>
                </linearGradient>
                </defs>
                <path id="pFlow" d="M 50 60 Q 150 20 300 60 T 550 60"
                    stroke="url(#personaGrad)" stroke-width="3" fill="none">
                <animate attributeName="stroke-dasharray" values="0 600;600 0" dur="3s" repeatCount="indefinite"/>
                </path>
                <g data-tier="beginner">
                <circle cx="50"  cy="60" r="22" fill="var(--color-accent-success)"/>
                <text x="50" y="66" text-anchor="middle" font-size="18">🎓</text>
                </g>
                <g data-tier="practitioner">
                <circle cx="300" cy="60" r="22" fill="var(--color-accent-primary)"/>
                <text x="300" y="66" text-anchor="middle" font-size="18">💻</text>
                </g>
                <g data-tier="researcher">
                <circle cx="550" cy="60" r="22" fill="var(--color-accent-secondary)"/>
                <text x="550" y="66" text-anchor="middle" font-size="18">🔬</text>
                </g>
                <text x="50"  y="100" class="p-label">Beginner</text>
                <text x="300" y="100" class="p-label">Practitioner</text>
                <text x="550" y="100" class="p-label">Researcher</text>
                <style>
                #persona-path-svg .p-label{font-size:12px;font-family:var(--font-sans);fill:var(--color-text-primary);text-anchor:middle}
                </style>
            </svg>
            </div>
            <script>
            (function(){
            const path = document.getElementById('pFlow');
            let hue = 0;
            setInterval(()=>{hue=(hue+2)%360;path.setAttribute('stroke',`hsl(${hue},100%,50%)`);},60);
            })();
            </script>            
            <details class="research-tier">
                <summary><i class="fas fa-calculator"></i> Crash Course in Probability & MDP Notation</summary>
                <div class="details-content">
                    <p>A rigorous understanding of RL requires familiarity with the language of probability theory.</p>
                    <ul>
                        <li><strong>Probability Space:</strong> An MDP is defined over a probability space $(\Omega, \mathcal{F}, P)$, where $\Omega$ is the set of all possible outcomes (e.g., all possible state-action-reward trajectories), $\mathcal{F}$ is a <span class="tooltip" data-tooltip="A collection of subsets of Ω that is closed under complements and countable unions. It defines the set of events we can assign probabilities to.">$\sigma$-algebra</span> of events, and $P$ is a probability measure.</li>
                        <li><strong>Random Variables:</strong> Quantities like state $S_t$, action $A_t$, and reward $R_t$ are random variables mapping outcomes in $\Omega$ to real values.</li>
                        <li><strong>Conditional Independence:</strong> The Markov Property is a statement of conditional independence. We say random variable $X$ is conditionally independent of $Y$ given $Z$, written $X \perp Y | Z$, if $P(X|Y,Z) = P(X|Z)$. The Markov property for states is $S_{t+1} \perp \{S_0, \dots, S_{t-1}\} | S_t$. This means the current state $S_t$ is a <span class="tooltip" data-tooltip="A function of the data that captures all the relevant information about an unknown parameter. Here, the state is a sufficient statistic of the history.">sufficient statistic</span> of the history.</li>
                        <li><strong>Hidden Markov Models (HMMs):</strong> A POMDP (Partially Observable MDP) can be viewed as an MDP where the true state is hidden, forming an HMM. The agent receives an observation $O_t$ that is probabilistically related to the true state $S_t$.</li>
                    </ul>
                </div>
            </details>

          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-tools"></i>
            0.2 Interactive Environment
          </summary>
          <div class="details-content">
            <p>
              To bridge the gap between theory and practice, the code snippets in this guide are designed to be both readable and functional. While this document is static for security and performance, we provide one-click links to run the code in an interactive <span class="tooltip" data-tooltip="A free, cloud-based Jupyter notebook environment provided by Google.">Google Colab</span> environment.
            </p>
            
            <details class="research-tier">
              <summary><i class="fas fa-server"></i> Deep Dive: Advanced Tooling for Reproducible Research</summary>
              <div class="details-content">
                  <p>For serious projects, moving beyond Colab to a robust local setup is crucial for reproducibility and performance. A professional workflow often includes the following components, which ensure that experiments can be replicated and scaled.</p>
                  <ul>
                      <li><strong>Dependency Management:</strong> Tools like <a href="https://python-poetry.org/" target="_blank">Poetry</a> or <a href="https://docs.conda.io/en/latest/" target="_blank">Conda</a> manage complex dependencies and create isolated environments. Using a `conda-lock` or `poetry.lock` file ensures that anyone can replicate your exact package versions, which is critical as minor version changes in libraries like PyTorch or Gymnasium can alter agent behavior.</li>
                      <li><strong>Containerization:</strong> <a href="https://www.docker.com/" target="_blank">Docker</a> provides the highest level of reproducibility. A `Dockerfile` can package your entire application—code, dependencies, system libraries, and even CUDA versions—into a portable container. This is the industry standard for deploying machine learning models and is essential for cloud-based training.</li>
                      <li><strong>Experiment Tracking:</strong> Services like <a href="https://wandb.ai" target="_blank">Weights & Biases</a> or <a href="https://www.mlflow.org/" target="_blank">MLflow</a> are indispensable for serious RL research. They automatically log hyperparameters, metrics (reward curves, loss), code versions, and even video recordings of agent behavior, allowing for systematic comparison and analysis of different experimental runs.</li>
                      <li><strong>GPU Profiling:</strong> To debug performance bottlenecks, use profilers like NVIDIA's <a href="https://developer.nvidia.com/nsight-systems" target="_blank">Nsight Systems</a> or the built-in <a href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html" target="_blank">PyTorch Profiler</a>. These tools can visualize exactly how much time is spent on data loading, GPU computation, and CPU overhead, helping you optimize your training loop for maximum hardware utilization.</li>
                  </ul>
              </div>
            </details>

            <h4>
              <i class="fas fa-download"></i>
              Required Libraries
            </h4>
            <p>
              The Colab environments are configured with the complete scientific Python stack. The first cell in each notebook will handle the setup, which typically includes:
            </p>
            <ul>
              <li><strong>PyTorch:</strong> The primary deep learning framework used for its flexibility and strong community support.</li>
              <li><strong>NumPy/SciPy:</strong> For numerical and scientific computing, forming the backbone of data manipulation.</li>
              <li><strong>Matplotlib/Seaborn:</strong> For generating static visualizations and plots of training progress.</li>
              <li><strong>Gymnasium:</strong> The standard toolkit for reinforcement learning environments, a community-maintained fork of OpenAI's Gym.</li>
            </ul>
            
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Example Setup Cell in Colab</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://colab.research.google.com/notebooks/intro.ipynb" target="_blank" class="colab-button" title="Run this code in Google Colab">
                            <i class="fas fa-play"></i> Run in Colab
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
# This code block would typically be the first cell in a Colab notebook.
# It handles the installation of required packages and sets up the environment.

# Install necessary libraries quietly
!pip install -q torch numpy matplotlib gymnasium "gymnasium[classic_control,box2d]"

# Import core libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym
import random
from collections import deque, namedtuple

# --- Environment Setup ---
print("🚀 Initializing environment...")

# Set device to GPU if available, otherwise CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Set a seed for reproducibility across all libraries
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    # The following two lines are important for determinism in CUDA operations
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

print("✅ Environment Initialized!")
print(f"PyTorch Version: {torch.__version__}")
print(f"NumPy Version: {np.__version__}")
print(f"Gymnasium Version: {gym.__version__}")
                </code></pre>
            </div>
          </div>
        </details>
        
        <div class="interactive-lab">
            <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Foundational Concepts</div>
            <p>Test your understanding of the core concepts introduced in this section.</p>
            <h5><i class="fas fa-question-circle"></i> Micro-Quiz: The Markov Property</h5>
            <p>Which of the following scenarios best satisfies the <span class="tooltip" data-tooltip="The assumption that the future is independent of the past, given the present. The current state must capture all relevant information.">Markov property</span> from the agent's perspective?</p>
            <ol>
              <li>An agent playing poker where the state is only its own two cards.</li>
              <li>An agent playing chess where the state is the complete board position.</li>
              <li>An agent controlling a robot arm where the state is just the arm's end-effector position (but not its velocity).</li>
            </ol>
            <details>
                <summary>Reveal Answer</summary>
                <div class="details-content">
                  <p><strong>Answer: 2.</strong> The complete chess board position contains all necessary information to determine the possible future moves and outcomes. How the pieces arrived at their current positions is irrelevant for future planning.
                  <br/>
                  <em>Why the others are wrong:</em> In poker (1), the history of betting and folded cards (which are not part of the state) provides crucial information. For the robot arm (3), the velocity is needed to predict its future position, so position alone is not a Markov state.
                  </p>
                </div>
            </details>
        </div>
</section>

      <hr/>
      <section id="part1">
        <h2 id="part1-title">
          <i class="fas fa-chess-board"></i>
          Part 1: The Bedrock of Reinforcement Learning
        </h2>
        
        <details open class="fade-in">
            <summary>
              <i class="fas fa-gamepad"></i>
              1.1 The RL Paradigm: Learning from Interaction
            </summary>
            <div class="details-content">
                <p>
                    At its most fundamental level, Reinforcement Learning (RL) is a computational approach to learning goal-directed behavior through <strong>interaction</strong>. It is a paradigm of learning from the consequences of actions, rather than from explicit instruction. It stands as one of the three primary pillars of machine learning, focusing on an <strong class="tooltip" data-tooltip="The learner and decision-maker in an RL problem.">agent</strong> situated within an <strong class="tooltip" data-tooltip="Everything outside the agent that it interacts with.">environment</strong>, learning to make optimal decisions over time to maximize a cumulative reward.
                </p>
        
                <h4>A Deeper Comparison of Machine Learning Paradigms</h4>
                <p>To fully appreciate what makes RL unique, it's useful to compare it rigorously against its supervised and unsupervised counterparts.</p>
                <table>
                    <thead>
                        <tr>
                            <th>Paradigm</th>
                            <th>Core Idea</th>
                            <th>Type of Data</th>
                            <th>Feedback Mechanism</th>
                            <th>Goal</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Supervised Learning</strong></td>
                            <td>Learning by Imitation</td>
                            <td>Labeled data $(x, y)$</td>
                            <td>Direct, immediate, and corrective. The loss function measures the error between the prediction and the true label.</td>
                            <td>Generalize from the training data to make accurate predictions on new, unseen data (e.g., classification, regression).</td>
                        </tr>
                        <tr>
                            <td><strong>Unsupervised Learning</strong></td>
                            <td>Learning by Observation</td>
                            <td>Unlabeled data $(x)$</td>
                            <td>No external feedback. The learning signal is derived from the intrinsic structure of the data itself (e.g., distance metrics for clustering).</td>
                            <td>Discover hidden patterns, structures, or representations in the data (e.g., clustering, dimensionality reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>Reinforcement Learning</strong></td>
                            <td>Learning by Trial and Error</td>
                            <td>Interaction data $(s, a, r, s')$</td>
                            <td>Evaluative, sparse, and often delayed. A scalar reward signal indicates how "good" an action was, not what the "correct" action was.</td>
                            <td>Learn a behavioral <span class="tooltip" data-tooltip="The agent's strategy or 'brain', which maps states to actions.">policy</span> that maximizes the cumulative reward over time.</td>
                        </tr>
                    </tbody>
                </table>
                
                <h4>The Agent-Environment Interaction Loop in Detail</h4>
                <p>
                    The interaction between agent and environment is a continuous feedback loop that unfolds over discrete time steps $t=0, 1, 2, \dots$. This loop is the fundamental data-generating process in all of RL. Let's break down a single cycle with precision:
                </p>
                <ol>
                    <li><strong>Observation:</strong> At time $t$, the agent observes the state of the environment, $S_t \in \mathcal{S}$.</li>
                    <li><strong>Action Selection:</strong> Based on this observation, the agent's policy, $\pi$, selects an action, $A_t \in \mathcal{A}(S_t)$.</li>
                    <li><strong>Interaction:</strong> The agent executes action $A_t$.</li>
                    <li><strong>Environment Response:</strong> The environment receives action $A_t$ and transitions to a new state, $S_{t+1}$.</li>
                    <li><strong>Feedback:</strong> As a consequence of this transition, the environment provides a scalar reward, $R_{t+1} \in \mathcal{R}$.</li>
                    <li><strong>Learning:</strong> The agent receives $S_{t+1}$ and $R_{t+1}$, forming an "experience tuple" $(S_t, A_t, R_{t+1}, S_{t+1})$. This tuple is used to update the agent's policy, $\pi$, so that in the future, it is more likely to take actions that lead to higher cumulative rewards. The loop then repeats from step 1.</li>
                </ol>
        
                <div class="visualization-container">
                    <div class="vis-toolbar">
                        <button data-action="zoom-in" title="Zoom In"><i class="fas fa-plus"></i></button>
                        <button data-action="zoom-out" title="Zoom Out"><i class="fas fa-minus"></i></button>
                        <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                        <button data-action="fullscreen" title="Fullscreen"><i class="fas fa-expand"></i></button>
                    </div>
                    <div class="mermaid">

graph TD
    subgraph "🤖 Agent"
        Policy["Policy π(a|s)"]
        ValueFunc["Value Func V(s)"]
        LearningAlgo["Learning Algorithm"]
    end

    subgraph "🌍 Environment"
        Dynamics["Dynamics P(s'|s,a)"]
        RewardFunc["Reward Func R(s,a)"]
    end

    Agent -- "Action A_t" --> Environment
    Environment -- "State S_t, Reward R_t" --> Agent

    Policy -- "Selects" --> Action_Out["Action A_t"]
    State_In["State S_t"] -- "Observes" --> Policy
    Action_Out -- "Executes in" --> Dynamics
    Dynamics -- "Transitions to" --> Next_State_Out["Next State S_{t+1}"]
    Dynamics -- "Generates" --> Reward_Out["Reward R_{t+1}"]
    
    Next_State_Out --> State_In
    Reward_Out --> LearningAlgo
    State_In --> LearningAlgo
    Action_Out --> LearningAlgo
    Next_State_Out --> LearningAlgo
    
    LearningAlgo -- "Updates" --> Policy
    LearningAlgo -- "Updates" --> ValueFunc
    ValueFunc -- "Informs" --> Policy

    class Policy,ValueFunc,LearningAlgo agent-class
    class Dynamics,RewardFunc env-class
    class Action_Out,State_In,Reward_Out,Next_State_Out data-flow
                    </div>
                </div>

                <h4>The Temporal Credit Assignment Problem: A Deeper Look</h4>
                <p>
                    The core difficulty that separates RL from other learning paradigms is the <strong><span class="tooltip" data-tooltip="The problem of determining which actions in a sequence are responsible for a particular outcome or reward.">temporal credit assignment problem</span></strong>. The reward signal is often sparse and delayed. An agent might make hundreds of moves in a game of chess, only to receive a single reward signal (+1 for a win, -1 for a loss) at the very end.
                </p>
                <div class="admonition info">
                    <span class="admonition-title"><i class="fas fa-route"></i>Analogy: Navigating a Maze</span>
                    <p>
                        Imagine an agent in a maze. The only reward is +100 for reaching the exit. All other moves give a reward of -1 (a small penalty for taking time).
                    </p>
                    <ul>
                        <li>The agent takes a sequence of 20 actions: Right, Right, Up, ..., Left.</li>
                        <li>On the 20th step, it reaches the exit and receives a reward of +100.</li>
                        <li>The preceding 19 steps all received a reward of -1.</li>
                    </ul>
                    <p>
                        A naive learning algorithm might associate the final "Left" action with the large positive reward and strengthen its tendency to go left. However, the true credit belongs to the entire sequence of 20 correct turns. The very first "Right" turn might have been the most critical decision that put the agent on the correct path. The goal of an RL algorithm is to propagate the final reward backward in time, assigning proper credit to all the decisions that led to the successful outcome. Value functions are the primary mechanism for achieving this propagation of credit.
                    </p>
                </div>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-project-diagram"></i>
              1.2 The Mathematical Framework: Markov Decision Processes (MDPs)
            </summary>
            <div class="details-content">
                <p>
                    To move from the high-level paradigm to concrete algorithms, we must formalize the problem of sequential decision-making under uncertainty. The universal mathematical framework for this in reinforcement learning is the <strong><span class="tooltip" data-tooltip="A mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.">Markov Decision Process (MDP)</span></strong>. An MDP, first explored in depth by Richard Bellman and Ronald Howard in the 1950s and 60s, provides a formal language to describe the interaction between a decision-making agent and its environment.
                </p>
                
                <h4>The Markov Property: "The Future is Independent of the Past Given the Present"</h4>
                <p>
                    The entire framework of MDPs hinges on a single, critical simplifying assumption: the <strong>Markov Property</strong>. A state signal $S_t$ is said to be Markov if and only if it contains all the information from the past that is relevant for predicting the future. Once the current state is known, the full history of how the agent arrived in that state can be discarded without losing any predictive power.
                </p>
                
                <div class="admonition tip">
                    <span class="admonition-title"><i class="fas fa-calculator"></i>Math Breakdown: The Markov Property</span>
                    <p>Let $H_t = (S_0, A_0, R_1, S_1, A_1, \dots, S_t)$ be the history up to time $t$. A state $S_t$ is Markov if and only if:
                    $$P(S_{t+1}=s' | S_t=s, A_t=a) = P(S_{t+1}=s' | H_t=h, A_t=a)$$
                    This equation states that the probability of transitioning to the next state $s'$ depends only on the current state $s$ and action $a$, not on the entire history $h$ that led to state $s$. The state $S_t$ is a <span class="tooltip" data-tooltip="In statistics, a sufficient statistic is a function of the data that contains all the information needed for any inference about a parameter. Here, the state is a sufficient statistic of the history.">sufficient statistic</span> of the past.
                    </p>
                </div>

                <div class="admonition danger">
                    <span class="admonition-title">
                      <i class="fas fa-exclamation-triangle"></i>
                      A Property of the Representation, Not the World
                    </span>
                    <p>It is a common misconception to think of the Markov property as a property of the <em>environment</em>. In reality, almost no real-world environment is truly Markovian. The Markov property is a feature of the <strong>state representation</strong> that we, as designers, provide to the agent. The core challenge of state design is to craft a representation $S_t$ that is a sufficient statistic of the history, thereby making the problem Markovian from the agent's perspective.</p>
                    <ul>
                        <li><strong>Good Representation (Chess):</strong> The current board position is a Markov state. It perfectly captures everything needed to play optimally. How the pieces got to their current squares is irrelevant for future planning.</li>
                        <li><strong>Bad Representation (Driving):</strong> If the state is only the car's <em>position</em>, it's not Markov. To predict the future position, you also need to know the car's current <em>velocity</em> and <em>acceleration</em>. A proper Markov state for a car would be a vector like $[position, velocity, acceleration]$.</li>
                        <li><strong>Deep RL Solution:</strong> For complex tasks like Atari games, a single frame is not Markov (you can't tell a ball's velocity from one picture). DeepMind's solution was to stack the last 4 frames together to form the state, allowing the network to implicitly infer dynamics like velocity and acceleration from the sequence.</li>
                    </ul>
                </div>

                <details class="deep-dive">
                    <summary><i class="fas fa-eye-slash"></i> Deep Dive: Partially Observable MDPs (POMDPs)</summary>
                    <div class="details-content">
                        <p>
                            When the agent cannot observe the full state, the problem is modeled as a <strong>Partially Observable Markov Decision Process (POMDP)</strong>. A POMDP adds two components to the MDP tuple: a set of observations $\Omega$ and an observation function $O(o|s,a)$, which gives the probability of observing $o$ after taking action $a$ and landing in state $s$.
                        </p>
                        <p>
                            To act optimally, the agent must maintain a <strong>belief state</strong>, $b(s_t)$, which is a probability distribution over all possible environment states, conditioned on its history. This belief state is updated using Bayesian filtering. The key insight, established by Sondik and Smallwood in the 1970s, is that this belief state <em>is</em> a Markovian representation of the history. This allows the problem to be reformulated as a (much harder, continuous-state) MDP over the space of beliefs. Recurrent Neural Networks (RNNs) are often used in deep RL to approximate this belief state implicitly.
                        </p>
                    </div>
                </details>
            </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-list-ol"></i>
            1.3 The MDP Tuple: (S, A, P, R, γ) - A Rigorous Unpacking
          </summary>
          <div class="details-content">
              <p>
                  An MDP is formally defined by a 5-element tuple. Mastering these components is essential to understanding any RL algorithm. Let's make this concrete with a simple <strong>Grid World</strong> example: a 3x4 grid where an agent (🤖) tries to reach a goal (🏆) while avoiding a pitfall (🔥). There's also a wall (🧱) the agent cannot pass through.
              </p>
              
              <h4>The Five Pillars of the MDP</h4>
              <p>Each component of the tuple defines a crucial aspect of the learning problem. A deep understanding of each is necessary to correctly model a real-world problem as an MDP.</p>
              
              <h5>1. The State Space ($S$)</h5>
              <p>The state space is the set of all possible situations the agent can be in. A state must be a complete, Markovian description of the world from the agent's perspective.</p>
              <ul>
                  <li><strong>Discrete vs. Continuous:</strong> State spaces can be finite and discrete, like the 11 valid positions in our Grid World, or they can be continuous and high-dimensional. For a robotic arm, the state could be a vector of joint angles and velocities, where each value is a real number. For an agent learning from images, the state is the grid of pixel values, which can be astronomically large (e.g., a 210x160x3 image has over 100,000 dimensions).</li>
                  <li><strong>Feature Engineering:</strong> For complex problems, the raw state may not be ideal for learning. We often apply feature engineering to extract relevant information. For example, in a self-driving car, the raw state might be camera images and LiDAR point clouds. A feature-engineered state might instead include objects' positions, velocities, and predicted trajectories. Deep RL excels at learning these features automatically.</li>
                  <li><strong>Grid World Example:</strong> The state space is the set of all non-wall grid coordinates: $S = \{(0,0), (0,1), (0,2), (0,3), (1,0), (1,2), (1,3), (2,0), (2,1), (2,2), (2,3)\}$. The total number of states is $|S|=11$.</li>
              </ul>

              <h5>2. The Action Space ($A$)</h5>
              <p>The action space is the set of all possible actions the agent can take. Like the state space, it can be discrete or continuous.</p>
              <ul>
                  <li><strong>Discrete Actions:</strong> The agent chooses from a finite set of actions. In our Grid World, $A = \{\text{up, down, left, right}\}$. In Atari games, the actions correspond to joystick movements and button presses.</li>
                  <li><strong>Continuous Actions:</strong> The agent's action is a real-valued vector. For a robot, this could be the torques applied to each motor. For an investment agent, it could be the percentage of the portfolio to allocate to different assets. Algorithms for continuous action spaces (like PPO or SAC) are fundamentally different from those for discrete spaces (like DQN).</li>
              </ul>

              <h5>3. The Transition Model ($P$)</h5>
              <p>The transition model, or dynamics, defines the rules of the environment. It specifies the probability of transitioning to a new state $s'$ given the current state $s$ and the chosen action $a$.</p>
              $$P(s'|s,a) = P(S_{t+1}=s' | S_t=s, A_t=a)$$
              <ul>
                  <li><strong>Deterministic vs. Stochastic:</strong> In a deterministic environment, a given action in a given state always leads to the same next state. In a stochastic environment, there is randomness. Our "slippery" Grid World is stochastic: taking "right" might result in moving right (80% chance), but it could also result in sliding up or down (10% chance each). This models uncertainty in the real world, like a robot's actuators not being perfectly precise.</li>
                  <li><strong>Model-Based vs. Model-Free RL:</strong> A crucial distinction in RL is whether the agent has access to the transition model $P$. In <strong>model-based RL</strong>, the agent knows the dynamics and can use them for planning (e.g., in chess, the agent knows all the rules). In <strong>model-free RL</strong>, the agent does not know $P$ and must learn about the environment's rules purely through trial-and-error. Most deep RL methods are model-free.</li>
              </ul>

              <h5>4. The Reward Function ($R$)</h5>
              <p>The reward function defines the goal of the RL problem. It is a scalar signal, $R(s,a,s')$, that the environment provides after a transition. The agent's objective is to maximize the cumulative sum of these rewards.</p>
              <ul>
                  <li><strong>Sparse vs. Dense Rewards:</strong> A sparse reward is one that is only received in rare, terminal states (e.g., +1 for winning a game, 0 otherwise). This makes learning very difficult due to the temporal credit assignment problem. A dense reward is received at every step, providing more frequent feedback.</li>
                  <li><strong>Reward Engineering and Shaping:</strong> Designing a good reward function is often the hardest part of applying RL. A poorly designed reward can lead to unintended behavior (reward hacking). For example, a cleaning robot rewarded for collecting dust might learn to dump its dustbin and collect the same dust again. <span class="tooltip" data-tooltip="A principled way to add intermediate rewards without changing the optimal policy. It involves adding a term of the form F(s') - F(s), where F is a potential function.">Potential-based reward shaping</span> is a technique to add dense rewards while formally guaranteeing that the optimal policy remains unchanged.</li>
                  <li><strong>Grid World Example:</strong> $R = +1$ for entering the goal 🏆, $R = -1$ for entering the pitfall 🔥, and $R = -0.04$ for all other moves to encourage efficiency.</li>
              </ul>

              <h5>5. The Discount Factor ($\gamma$)</h5>
              <p>The discount factor $\gamma \in [0, 1)$ determines the present value of future rewards. It's a key hyperparameter that controls the agent's foresight.</p>
              <ul>
                  <li><strong>Mathematical Purpose:</strong> For continuing tasks with no end, the sum of rewards could be infinite. The discount factor ensures that the infinite sum of discounted rewards is a finite value, making the problem mathematically tractable. If rewards are bounded by $|R_{max}|$, the total return is guaranteed to be less than or equal to $|R_{max}| / (1-\gamma)$.</li>
                  <li><strong>Behavioral Effect:</strong> A $\gamma$ close to 0 makes the agent "myopic" or "impatient," caring only about immediate rewards. A $\gamma$ close to 1 makes the agent "farsighted" or "patient," valuing future rewards almost as much as immediate ones.</li>
                  <li><strong>Economic Analogy:</strong> $\gamma$ is analogous to an interest rate. A reward of 100 received 10 steps in the future has a present value of $100 \times \gamma^{10}$.</li>
              </ul>
              <div class="visualization-container">
                <div id="gamma-plot" style="width:100%; height:300px;"></div>
              </div>
              <script>
                (function() {
                    const plotDiv = document.getElementById('gamma-plot');
                    if (!plotDiv || typeof Plotly === 'undefined') return;
                    const steps = Array.from({length: 50}, (_, i) => i);
                    const gammas = [0.8, 0.9, 0.95, 0.99];
                    const data = gammas.map(gamma => ({
                        x: steps,
                        y: steps.map(t => Math.pow(gamma, t)),
                        mode: 'lines',
                        name: `γ = ${gamma}`
                    }));
                    const layout = {
                        title: 'The Effect of the Discount Factor (γ)',
                        xaxis: {title: 'Time Steps into the Future (t)'},
                        yaxis: {title: 'Discount (γ^t)'},
                        legend: {orientation: 'h'}
                    };
                    Plotly.newPlot(plotDiv, data, layout);
                })();
              </script>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas-solid fa-bullseye"></i>
              1.4 The Objective: Policies, Returns, and Value Functions
            </summary>
            <div class="details-content">
                <p>
                    With the MDP framework established, we can now formally define the agent's goal and the tools it uses to achieve it. The agent's "brain" is its policy, its goal is to maximize the return, and its "judgment" is its value function.
                </p>

                <h4>The Return ($G_t$): What is the Goal?</h4>
                <p>The agent's goal is to maximize the <strong>Return</strong>, $G_t$, which is the total discounted reward from time step $t$ onward.</p>
                $$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
                <div class="admonition tip">
                    <span class="admonition-title"><i class="fas fa-recycle"></i>The Recursive Nature of the Return</span>
                    <p>This definition has a powerful recursive property that is the foundation of the Bellman equations:</p>
                    $$G_t = R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \dots) = R_{t+1} + \gamma G_{t+1}$$
                    <p>This simple relationship is profound. It shows that the total future reward from now is simply the immediate reward we get, plus the discounted total future reward from the next state. This allows us to break a large, complex problem into smaller, self-similar subproblems—the core idea of dynamic programming.</p>
                </div>

                <h4>The Policy ($\pi$): The Agent's Brain</h4>
                <p>The policy is the agent's behavior—its strategy for choosing actions. The ultimate goal of RL is to find an <strong>optimal policy</strong>, $\pi^*$, which maximizes the expected return from any state.</p>
                <ul>
                    <li><strong>Deterministic Policy:</strong> $a = \pi(s)$. For a given state, the policy always outputs the same action.</li>
                    <li><strong>Stochastic Policy:</strong> $\pi(a|s) = P(A_t=a | S_t=s)$. The policy outputs a probability distribution over actions. Stochastic policies are crucial for exploration and in environments where the optimal strategy is inherently random (like rock-paper-scissors).</li>
                </ul>

                <h4>Value Functions ($V^\pi$ and $Q^\pi$): The Agent's Judgment</h4>
                <p>To improve its policy, the agent needs a way to evaluate it. This is the role of value functions, which estimate the expected return.</p>
                <ul>
                    <li>The <strong>state-value function</strong>, $V^\pi(s)$, is the expected return starting from state $s$ and then following policy $\pi$.
                        $$V^\pi(s) \doteq \mathbb{E}_\pi [G_t | S_t = s]$$
                    </li>
                    <li>The <strong>action-value function</strong>, $Q^\pi(s, a)$, is the expected return after taking action $a$ in state $s$ and then following policy $\pi$.
                        $$Q^\pi(s, a) \doteq \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$$
                    </li>
                </ul>
                <div class="admonition danger">
                    <span class="admonition-title"><i class="fas fa-question-circle"></i>The Key Difference: Why Q-Functions are Crucial for Model-Free Control</span>
                    <p>It might seem redundant to have two types of value functions, but they serve different purposes, especially when the agent doesn't have a model of the environment's dynamics ($P$).</p>
                    <ul>
                        <li>The <strong>State-Value Function $V(s)$</strong> tells you the long-term value of <em>being in a state</em>. It's great for analysis, but to act on it, you need a model. To choose an action, you'd have to perform a one-step lookahead for every possible action $a$, calculate $\sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V(s')]$, and then pick the action that gives the best result. <strong>This requires knowing the transition probabilities $P$.</strong></li>
                        <li>The <strong>Action-Value Function $Q(s, a)$</strong> tells you the long-term value of <em>taking a specific action in a specific state</em>. This is much more direct for control. To choose the best action, you simply find the action with the highest Q-value: $\arg\max_a Q(s, a)$. <strong>You don't need to know anything about the environment's dynamics to make an optimal decision.</strong> This is why model-free algorithms like Q-Learning learn Q-values directly.</li>
                    </ul>
                </div>
            </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-infinity"></i>
            1.5 The Bellman Equations: The Heartbeat of RL
          </summary>
          <div class="details-content">
              <p>
                  The components of the MDP give us the language to describe an RL problem, but they don't tell us how to solve it. The key to unlocking the solution lies in a set of elegant, recursive relationships known as the <strong>Bellman Equations</strong>. Developed by the American mathematician Richard Bellman in the 1950s as part of his work on dynamic programming, these equations are arguably the most important theoretical foundation in all of reinforcement learning.
              </p>
              <p>
                  They provide a way to break down the complex, long-term problem of maximizing a cumulative future reward into a series of simple, one-step subproblems. They connect the value of a state to the values of the states that might follow, creating a web of self-consistent relationships across the entire state space.
              </p>
              <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-map-signs"></i>Analogy: Planning a Cross-Country Trip</span>
                <p>
                    Imagine you are planning the best route from New York to Los Angeles. The "value" of any city on the map is the "goodness" of being in that city, which you can define as the total enjoyment (negative travel time) of the rest of the trip from there.
                </p>
                <p>
                    The Bellman equation provides a simple, local rule for calculating this value: <strong>The value of your current city (say, Chicago) is the immediate enjoyment of driving to the next city (e.g., Denver) plus the discounted value of that next city (Denver).</strong>
                </p>
                <p>
                    You don't need to know the entire path from Denver to Los Angeles to update the value of Chicago; you only need to know the value of Denver itself. By applying this simple, one-step lookahead rule repeatedly to all cities on the map, the values will propagate backward from the destination (Los Angeles, with a value of 0 remaining travel time) until the optimal value for every city, including your starting point in New York, is known. This is the essence of Bellman's insight: solving a global optimization problem through local, iterative updates.
                </p>
              </div>

              <h4>The Two Flavors of Bellman Equations</h4>
              <p>There are two main forms of the Bellman equation, each serving a distinct purpose:</p>
              <ol>
                  <li><strong>The Bellman Expectation Equation:</strong> Used for <strong>evaluation</strong>. It answers the question, "If I follow this specific policy $\pi$, what is the value of each state?"</li>
                  <li><strong>The Bellman Optimality Equation:</strong> Used for <strong>control</strong>. It answers the question, "What is the value of each state if I follow the best possible policy $\pi^*$?"</li>
              </ol>
              <p>We will now unpack each of these in full detail.</p>
          </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-calculator"></i>
              1.6 In-Depth: The Bellman Expectation Equation
            </summary>
            <div class="details-content">
                <p>This equation describes the value function for a <em>given, fixed policy $\pi$</em>. It allows us to calculate the expected return from any state, thereby evaluating how good that policy is. This process is known as <strong>policy evaluation</strong>.</p>
                
                <h5>Step-by-Step Derivation for the State-Value Function $V^\pi(s)$</h5>
                <p>The derivation is a beautiful chain of substitutions that connects the high-level definition of value to a practical, computable recursion. It's crucial to understand each step.</p>
                <ol>
                    <li><strong>Start with the definition of the value function:</strong> The value of state $s$ under policy $\pi$ is the expected return from that state.
                        $$V^\pi(s) \doteq \mathbb{E}_\pi [G_t | S_t = s]$$
                    </li>
                    <li><strong>Substitute the recursive definition of the return:</strong> We know from our earlier definition that $G_t = R_{t+1} + \gamma G_{t+1}$.
                        $$V^\pi(s) = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s]$$
                    </li>
                    <li><strong>Apply the linearity of expectation:</strong> The expectation of a sum is the sum of expectations.
                        $$V^\pi(s) = \mathbb{E}_\pi [R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s]$$
                    </li>
                    <li><strong>Unpack the expectations:</strong> This is the most critical step. We need to average over all possibilities. The agent's policy $\pi(a|s)$ determines the actions, and the environment's dynamics $p(s', r | s, a)$ determine the outcomes.
                        $$V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \mathbb{E}_\pi [G_{t+1} | S_{t+1} = s'] \right]$$
                    </li>
                    <li><strong>Recognize the recursive structure:</strong> Notice that the term $\mathbb{E}_\pi [G_{t+1} | S_{t+1} = s']$ is simply the definition of the value of the <em>next</em> state, $V^\pi(s')$. Substituting this in gives the final form.</li>
                </ol>
                <p>This leads us to the final <strong>Bellman expectation equation for $V^\pi$</strong>:</p>
                $$V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V^\pi(s') \right]$$
                
                <div class="visualization-container">
                    <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                    <div class="mermaid">
graph TD
    S[Current State s] -->|"Policy π(a|s) selects action"| A1(Action a1)
    S -->|"..."| A2(Action a2)
    
    subgraph "One-Step Lookahead"
        A1 -->|"Dynamics P(s'|s,a1)"| S1_prime("Next State s'_1")
        A1 -->|"..."| S2_prime("Next State s'_2")
        A2 -->|"Dynamics P(s'|s,a2)"| S3_prime("Next State s'_3")
    end

    S1_prime -->|"Value Vπ(s'_1)"| V1_prime("r1 + γVπ(s'_1)")
    S2_prime -->|"Value Vπ(s'_2)"| V2_prime("r2 + γVπ(s'_2)")
    S3_prime -->|"Value Vπ(s'_3)"| V3_prime("r3 + γVπ(s'_3)")
    
    V1_prime -->|"Expectation over s'"| Exp1
    V2_prime -->|"..."| Exp1
    V3_prime -->|"Expectation over s'"| Exp2
    
    Exp1 -->|"Expectation over a"| V_s("Vπ(s)")
    Exp2 -->|"..."| V_s

    class S,V_s equation-element
    class Exp1,Exp2 expectation-node                    </div>
                    <p style="text-align:center; font-style:italic;">A "backup diagram" showing how the value of state s is calculated by looking one step ahead at all possible actions and their resulting states.</p>
                </div>
                
                <h5>Concrete Example: Policy Evaluation in Grid World</h5>
                <p>Let's calculate the value for state (0,0) in our Grid World, assuming a simple policy that chooses each of the 4 actions with equal probability ($\pi(a|s) = 0.25$) and a deterministic environment (no slipping). Let $\gamma=0.9$ and assume we have some initial (likely incorrect) value estimates for all states, say $V_0(s) = 0$ for all $s$.</p>
                <p>To compute the new value $V_1(0,0)$, we apply the equation:</p>
                $V_1(0,0) = \sum_{a \in \{\text{up,down,left,right}\}} 0.25 \left[ R(s,a,s') + \gamma V_0(s') \right]$
                <ul>
                    <li><strong>Action 'Up':</strong> Agent stays at (0,0). $s'=(0,0)$. Reward is -0.04. Contribution: $0.25 \times [-0.04 + 0.9 \times V_0(0,0)] = 0.25 \times [-0.04 + 0] = -0.01$.</li>
                    <li><strong>Action 'Down':</strong> Agent moves to (1,0). $s'=(1,0)$. Reward is -0.04. Contribution: $0.25 \times [-0.04 + 0.9 \times V_0(1,0)] = 0.25 \times [-0.04 + 0] = -0.01$.</li>
                    <li><strong>Action 'Left':</strong> Agent stays at (0,0). $s'=(0,0)$. Reward is -0.04. Contribution: $0.25 \times [-0.04 + 0.9 \times V_0(0,0)] = -0.01$.</li>
                    <li><strong>Action 'Right':</strong> Agent moves to (0,1). $s'=(0,1)$. Reward is -0.04. Contribution: $0.25 \times [-0.04 + 0.9 \times V_0(0,1)] = -0.01$.</li>
                </ul>
                <p>So, after one iteration, $V_1(0,0) = -0.01 - 0.01 - 0.01 - 0.01 = -0.04$. We would repeat this for every state in the grid. Then we would repeat the entire process again to calculate $V_2$, $V_3$, and so on, until the values stop changing significantly. This algorithm is called <strong>Iterative Policy Evaluation</strong>.</p>
                
                <div class="code-container">
                    <div class="code-block-header">
                        <span><i class="fab fa-python"></i> Python: Iterative Policy Evaluation</span>
                        <div class="buttons">
                            <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        </div>
                    </div>
                    <pre><code class="language-python">
# Assume a simple GridWorld environment class is defined
# with env.n_states, env.n_actions, and env.P[s][a] which returns [(prob, next_state, reward, done)]

def policy_evaluation(policy, env, gamma=0.99, theta=1e-8):
    """
    Evaluate a given policy by iteratively applying the Bellman expectation equation.
    
    Args:
        policy: A matrix [n_states, n_actions] with probabilities.
        env: The environment object.
        gamma: The discount factor.
        theta: A small threshold to determine convergence.
        
    Returns:
        A vector V of length n_states with the value function.
    """
    V = np.zeros(env.n_states)
    while True:
        delta = 0
        for s in range(env.n_states):
            v_old = V[s]
            new_v = 0
            # Sum over all actions weighted by the policy
            for a, action_prob in enumerate(policy[s]):
                # Sum over all possible outcomes for this action
                for prob, next_state, reward, done in env.P[s][a]:
                    new_v += action_prob * prob * (reward + gamma * V[next_state])
            
            # Update the value for this state
            V[s] = new_v
            delta = max(delta, np.abs(v_old - V[s]))
            
        # Check for convergence
        if delta < theta:
            break
    return V
                    </code></pre>
                </div>
            </div>
        </details>
        <details open class="fade-in">
            <summary>
              <i class="fas fa-star"></i>
              1.7 In-Depth: The Bellman Optimality Equation
            </summary>
            <div class="details-content">
                <p>
                    While the expectation equation is for <strong>evaluation</strong>, our ultimate goal is <strong>control</strong>—finding the best possible policy. The <strong>Bellman Optimality Equation</strong> describes the value function for the optimal policy, $\pi^*$. It is a statement about the condition that the value function for an optimal policy must satisfy, and it forms the basis for algorithms that seek to maximize return.
                </p>
                <p>
                    The fundamental insight is that an optimal policy must satisfy a simple self-consistency condition: if you are following an optimal policy, then whatever your first action is, your subsequent actions from the next state must also be optimal. This principle of optimality is the heart of dynamic programming.
                </p>
                
                <h4>From Expectation to Optimality: The Role of the Max Operator</h4>
                <p>
                    The key difference from the expectation equation is the introduction of a <strong>$\max$ operator</strong>. Instead of averaging over the actions a policy might take, we simply choose the action that leads to the best outcome. This reflects a deterministic, greedy choice at each state to maximize long-term reward.
                </p>
                <p>The optimal state-value function, $V^*(s)$, is defined as the maximum possible expected return from state $s$ over all possible policies:</p>
                $$V^*(s) \doteq \max_{\pi} V^\pi(s)$$
                <p>The Bellman optimality equation provides a recursive definition for this quantity:</p>
                $$V^*(s) = \max_{a \in A} \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V^*(s') \right]$$
                
                <div class="admonition info">
                    <span class="admonition-title"><i class="fas fa-lightbulb"></i>Breaking Down the Bellman Optimality Equation</span>
                    <p>Let's dissect this equation piece by piece:</p>
                    <ul>
                        <li>$V^*(s)$: The value of being in state $s$ if we act optimally from now on.</li>
                        <li>$\max_{a \in A}$: A maximization over all possible actions. This is the crucial difference. We are not averaging; we are choosing the single best action.</li>
                        <li>$\sum_{s', r} p(s', r | s, a)$: An average over all possible outcomes from the environment, given that we have chosen action $a$.</li>
                        <li>$[r + \gamma V^*(s')]$: The value of an outcome: the immediate reward plus the discounted <em>optimal</em> value of the next state.</li>
                    </ul>
                    <p>In words: "The optimal value of a state is the value of the best action you can take from it. The value of taking a specific action is the average over the world's outcomes of the immediate reward plus the discounted optimal value of whatever state you land in next."</p>
                </div>

                <h4>The Optimality Equation for $Q^*(s,a)$</h4>
                <p>The optimality equation for the action-value function, $Q^*(s,a)$, is even more important for model-free learning. It states that the optimal value of taking action $a$ in state $s$ is the expected immediate reward plus the discounted value of the <em>best possible action from the next state</em>.</p>
                $$Q^*(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \max_{a' \in A} Q^*(s', a') \right]$$
                <p>This equation is the foundation of the famous <strong>Q-learning algorithm</strong>. It tells us how to update our estimate of $Q(s,a)$ using the observed reward $r$ and our current estimate of the best value we can get from the next state, $\max_{a'} Q(s', a')$.</p>

                <h4>Solving the Optimality Equation: Value Iteration</h4>
                <p>
                    The Bellman optimality equation gives us a straightforward algorithm for finding the optimal value function, known as <strong>Value Iteration</strong>. It is similar to iterative policy evaluation, but instead of applying a policy-specific backup, we apply the optimality backup at each step, maximizing over actions.
                </p>
                <p>The update rule for Value Iteration is a direct translation of the equation:</p>
                $$V_{k+1}(s) \leftarrow \max_{a \in A} \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V_k(s') \right]$$
                <p>We start with an arbitrary value function $V_0$ (e.g., all zeros) and repeatedly apply this update for all states $s \in S$. Because the Bellman optimality operator is a contraction mapping, this process is guaranteed to converge to the unique optimal value function $V^*$.</p>
                
                <div class="code-container">
                    <div class="code-block-header">
                        <span><i class="fab fa-python"></i> Python: Value Iteration</span>
                        <div class="buttons">
                            <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        </div>
                    </div>
                    <pre><code class="language-python">
# Assume a simple GridWorld environment class is defined
# with env.n_states, env.n_actions, and env.P[s][a] which returns [(prob, next_state, reward, done)]

def value_iteration(env, gamma=0.99, theta=1e-8):
    """
    Solve for the optimal value function using Value Iteration.
    
    Returns:
        A vector V of length n_states with the optimal value function.
    """
    V = np.zeros(env.n_states)
    while True:
        delta = 0
        for s in range(env.n_states):
            v_old = V[s]
            
            # Compute the value for each possible action
            action_values = np.zeros(env.n_actions)
            for a in range(env.n_actions):
                # Sum over all possible outcomes for this action
                for prob, next_state, reward, done in env.P[s][a]:
                    action_values[a] += prob * (reward + gamma * V[next_state])
            
            # Bellman Optimality Update: take the max over actions
            V[s] = np.max(action_values)
            delta = max(delta, np.abs(v_old - V[s]))
            
        # Check for convergence
        if delta < theta:
            break
    return V

def extract_optimal_policy(V_star, env, gamma=0.99):
    """
    Extract a deterministic optimal policy from the optimal value function.
    """
    policy = np.zeros(env.n_states, dtype=int)
    for s in range(env.n_states):
        action_values = np.zeros(env.n_actions)
        for a in range(env.n_actions):
            for prob, next_state, reward, done in env.P[s][a]:
                action_values[a] += prob * (reward + gamma * V_star[next_state])
        policy[s] = np.argmax(action_values)
    return policy

# --- Example Usage ---
# V_star = value_iteration(env)
# optimal_policy = extract_optimal_policy(V_star, env)
                    </code></pre>
                </div>
                
                <div class="admonition tip">
                    <span class="admonition-title">
                      <i class="fas fa-balance-scale"></i>
                      Policy Iteration vs. Value Iteration
                    </span>
                    <p>
                        Value Iteration is one of two fundamental dynamic programming algorithms for solving MDPs. The other is <strong>Policy Iteration</strong>. They differ in how they arrive at the optimal policy.
                    </p>
                    <ul>
                        <li><strong>Policy Iteration</strong> alternates between two steps:
                            <ol>
                                <li><strong>Policy Evaluation:</strong> Given the current policy $\pi$, compute its value function $V^\pi$ (using the Bellman expectation equation, iterated until convergence).</li>
                                <li><strong>Policy Improvement:</strong> Act greedily with respect to $V^\pi$ to create a new, improved policy $\pi'$. That is, for each state, choose the action that maximizes the one-step lookahead value.</li>
                            </ol>
                            This process is repeated until the policy no longer changes.
                        </li>
                        <li><strong>Value Iteration</strong> can be seen as a more streamlined version. It combines the policy improvement step directly into the value update by using the $\max$ operator, effectively performing only one "sweep" of policy evaluation in each iteration.</li>
                    </ul>
                    <p>For most problems, Value Iteration converges faster, though Policy Iteration can sometimes require fewer total iterations if the evaluation step is efficient.</p>
                </div>
            </div>
        </details>
</section>
      <hr/>
      <section id="part2">
        <h2 id="part2-title">
          <i class="fas fa-compass"></i>
          Part 2: The Core Dilemma: Exploration vs. Exploitation
        </h2>
        
        <details open class="fade-in">
            <summary>
              <i class="fas fa-balance-scale"></i>
              2.1 The Fundamental Trade-Off in Decision Making
            </summary>
            <div class="details-content">
                <p>
                    In an ideal world, an agent would possess perfect knowledge of its environment's dynamics ($P$) and reward function ($R$). With this information, it could simply compute the optimal policy $\pi^*$ using methods like Value Iteration. However, in nearly all interesting problems, this information is unknown. The agent must learn it through interaction.
                </p>
                <p>
                    This necessity of learning from experience gives rise to the most fundamental and persistent challenge in online decision-making: the <strong>exploration versus exploitation</strong> dilemma. The agent must continually choose between two competing objectives:
                </p>
                <ul>
                    <li><strong>Exploitation:</strong> To make the best decision given its current, incomplete knowledge. The agent leverages its existing beliefs to select the action that it currently estimates will yield the highest cumulative reward. This is the "safe" bet, maximizing short-term performance.</li>
                    <li><strong>Exploration:</strong> To gather more information to improve its knowledge. The agent intentionally tries actions that it is uncertain about, even if they currently have low estimated values. This is a "risky" investment, potentially sacrificing immediate reward for the chance to discover a new, better long-term strategy.</li>
                </ul>
                <div class="admonition info">
                    <span class="admonition-title"><i class="fas fa-utensils"></i>Analogy: The Restaurant Dilemma, Expanded</span>
                    <p>
                        Let's formalize the restaurant choice problem. Suppose your enjoyment of a meal is rated from 1 to 10.
                    </p>
                    <ul>
                        <li><strong>Restaurant A (Exploit):</strong> Your favorite spot. You've been there 50 times. Your average enjoyment is a solid 8/10, with very low variance. You are highly certain about its quality.</li>
                        <li><strong>Restaurant B (Explore):</strong> A new restaurant. You've never been. Your prior belief about its quality is a uniform distribution over [1, 10]. It could be a 1/10 disaster or a 10/10 life-changing experience. Your uncertainty is maximal.</li>
                    </ul>
                    <p>
                        A purely exploitative agent would go to Restaurant A every single night, guaranteeing a cumulative reward close to $8 \times N$ over $N$ nights. A purely exploratory agent would try a new restaurant every night, leading to a highly variable and likely lower average reward.
                    </p>
                    <p>
                        The optimal strategy is more nuanced. You might explore Restaurant B. If it's a 9/10, you update your belief and now have a new "best" restaurant to exploit. If it's a 3/10, you also update your belief and are now more confident that Restaurant A is the better choice. Every exploratory action reduces uncertainty and refines your world model, leading to better future exploitation. The challenge is deciding <em>when</em> the potential information gain from exploration is worth the risk of a poor immediate outcome.
                    </p>
                </div>
                <p>
                    This is not just an abstract concept; it is a central challenge in many real-world applications, from A/B testing in web design to drug discovery in medicine. Striking the right balance is the key to effective learning.
                </p>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-landmark"></i>
              2.2 The Multi-Armed Bandit: A Formal Model of the Dilemma
            </summary>
            <div class="details-content">
                <p>
                    To study the exploration-exploitation dilemma in its purest form, researchers use a simplified reinforcement learning problem called the <strong>k-armed bandit</strong>. It is an RL problem with only a single, non-changing state, where the agent must repeatedly choose one of $k$ actions (the "arms" of a slot machine) to maximize its total reward over a series of plays.
                </p>
                
                <h4>Formal Definition</h4>
                <p>
                    A k-armed bandit problem is defined by a set of $k$ actions, $\{a_1, \dots, a_k\}$. For each action $a$, there is an unknown probability distribution over rewards. The true value of an action, $q_*(a)$, is its expected reward:
                </p>
                $$q_*(a) \doteq \mathbb{E}[R_t | A_t = a]$$
                <p>
                    At each time step $t=1, 2, \dots, T$, the agent selects an action $A_t$ and receives a reward $R_t$ drawn from the distribution associated with $A_t$. The agent's goal is to devise a sequence of action selections to maximize its total cumulative reward.
                </p>
                <p>Since the true values $q_*(a)$ are unknown, the agent must maintain an <em>estimate</em> for each action, $\hat{Q}_t(a)$. A common way to update this estimate after receiving reward $R_t$ for action $A_t$ is via an incremental sample-average update rule:</p>
                $$\hat{Q}_{t+1}(A_t) = \hat{Q}_t(A_t) + \frac{1}{N_t(A_t)} [R_t - \hat{Q}_t(A_t)]$$
                <p>Where $N_t(A_t)$ is the number of times action $A_t$ has been selected prior to time $t$. This update rule moves the estimate a small amount in the direction of the <strong>prediction error</strong>, $[R_t - \hat{Q}_t(A_t)]$.</p>

                <div class="code-container">
                    <div class="code-block-header">
                        <span><i class="fab fa-python"></i> Python: A Simple Bandit Environment</span>
                        <div class="buttons">
                            <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        </div>
                    </div>
                    <pre><code class="language-python">
import numpy as np

class GaussianBandit:
    """A k-armed bandit environment with Gaussian reward distributions."""
    def __init__(self, k, seed=42):
        self.k = k
        np.random.seed(seed)
        # The true, unknown mean reward for each arm
        self.true_means = np.random.randn(k)
        # All arms have a standard deviation of 1
        self.std_devs = np.ones(k)

    def pull_arm(self, arm_index):
        """
        Select an arm and receive a stochastic reward.
        
        Args:
            arm_index (int): The index of the arm to pull (0 to k-1).
        
        Returns:
            float: The reward sampled from the arm's Gaussian distribution.
        """
        if not 0 <= arm_index < self.k:
            raise ValueError("Invalid arm index.")
        
        mean = self.true_means[arm_index]
        std = self.std_devs[arm_index]
        return np.random.normal(mean, std)

# Example usage:
bandit_env = GaussianBandit(k=10)
print(f"True means of the 10 arms (unknown to the agent):")
print(np.round(bandit_env.true_means, 2))

reward_from_arm_3 = bandit_env.pull_arm(3)
print(f"\nPulled arm 3 and received a reward of: {reward_from_arm_3:.2f}")
                    </code></pre>
                </div>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-chart-line"></i>
              2.3 Regret: A Rigorous Measure of Performance
            </summary>
            <div class="details-content">
                <p>How do we formally and quantitatively measure how well an exploration strategy performs? The key metric is <strong>regret</strong>. The total regret after $T$ steps is the cumulative opportunity loss—the difference between the total reward an omniscient agent could have achieved and the total reward our learning agent actually achieved.</p>
                <p>Let $q_*(a^*) = \max_a q_*(a)$ be the value of the truly optimal action $a^*$. The regret at a single time step $t$ is the difference between the best possible expected reward and the expected reward for the action taken: $\Delta_t = q_*(a^*) - q_*(A_t)$.</p>
                <p>The <strong>total cumulative regret</strong> after $T$ steps is the sum of these single-step expected regrets:</p>
                $$L_T = \mathbb{E}\left[\sum_{t=1}^T (q_*(a^*) - q_*(A_t))\right]$$
                <p>A good learning algorithm should ensure that regret grows as slowly as possible. The goal is to have <strong>sub-linear total regret</strong>, meaning that the average regret per step, $L_T/T$, approaches zero as $T \to \infty$. This indicates the agent is successfully learning and converging on the optimal action.</p>
                
                <div class="visualization-container">
                    <div id="regret-plot" style="width:100%; height:400px;"></div>
                </div>
                <script>
                (function() {
                    const plotDiv = document.getElementById('regret-plot');
                    if (!plotDiv || typeof Plotly === 'undefined') return;
                    const T = Array.from({length: 1000}, (_, i) => i + 1);
                    const linear_regret = T.map(t => 0.5 * t); // Agent gets it wrong 50% of the time
                    const log_regret = T.map(t => 20 * Math.log(t + 1));
                    const sqrt_regret = T.map(t => 5 * Math.sqrt(t));

                    const data = [
                        {x: T, y: linear_regret, mode: 'lines', name: 'Linear Regret O(T) (Failure)', line: {color: 'red'}},
                        {x: T, y: log_regret, mode: 'lines', name: 'Logarithmic Regret O(log T) (Optimal)', line: {color: 'green'}},
                        {x: T, y: sqrt_regret, mode: 'lines', name: 'Sub-linear Regret O(√T) (Good)', line: {color: 'blue'}}
                    ];
                    const layout = {
                        title: 'Growth Rates of Cumulative Regret',
                        xaxis: {title: 'Time Steps (T)'},
                        yaxis: {title: 'Total Cumulative Regret (L_T)'},
                        legend: {orientation: 'h', y: -0.2}
                    };
                    Plotly.newPlot(plotDiv, data, layout);
                })();
                </script>
                
                <p>As the plot illustrates, an algorithm with linear regret is failing to learn effectively, as its per-step performance loss does not decrease. In contrast, algorithms with logarithmic or other sub-linear regret are successfully minimizing their mistakes over time, which is the hallmark of an efficient exploration strategy.</p>
        </details>
</section>
      <section id="part2-strategies">
        <p>Given the challenge of balancing exploration and exploitation, how does an agent actually decide which arm to pull? This section explores several foundational strategies in great detail, from the simplest greedy approaches to more sophisticated methods that use uncertainty to guide exploration. We will analyze their mechanisms, their theoretical underpinnings, their practical failure modes, and their performance through code.</p>
        
        <details open class="fade-in">
            <summary>
              <i class="fas fa-dice-one"></i>
              2.4 Baseline Strategy: The Purely Greedy Approach
            </summary>
            <div class="details-content">
                <p>
                    The most straightforward strategy is to be purely greedy. At every time step, the agent simply exploits its current knowledge without any consideration for exploration. It operates under the assumption that its current estimates are perfect.
                </p>
                <p><strong>The Greedy Action Selection Rule:</strong></p>
                $$A_t \doteq \arg\max_{a} \hat{Q}_t(a)$$
                <p>
                    This means the agent calculates the current estimated value, $\hat{Q}_t(a)$, for all $k$ arms and selects the one with the highest value. While this maximizes the immediate expected reward <em>according to the agent's current beliefs</em>, it is a deeply flawed learning strategy because those beliefs are almost certainly wrong, especially early on.
                </p>
                <div class="admonition danger">
                    <span class="admonition-title"><i class="fas fa-lock"></i>Catastrophic Failure Mode: The Lock-In Problem</span>
                    <p>
                        The greedy approach is susceptible to a catastrophic failure known as "lock-in" or premature convergence. A single unlucky, early experience can permanently prevent the agent from ever discovering the true optimal action.
                    </p>
                    <p>
                        Imagine a 2-armed bandit where Arm 1 has a true mean of $q_*(1) = 0.8$ and Arm 2 has a true mean of $q_*(2) = 0.9$. Arm 2 is objectively better.
                    </p>
                    <ol>
                        <li><strong>Step 1:</strong> The agent has no information, so it tries Arm 1. Due to stochasticity, it receives a lucky high reward of $R_1 = 0.95$. Its estimate becomes $\hat{Q}_1(1) = 0.95$.</li>
                        <li><strong>Step 2:</strong> The agent tries Arm 2. It receives an unlucky low reward of $R_2 = 0.7$. Its estimate becomes $\hat{Q}_2(2) = 0.7$.</li>
                        <li><strong>Step 3 onwards:</strong> The agent now compares its estimates: $\hat{Q}_2(1) = 0.95$ vs. $\hat{Q}_2(2) = 0.7$. Since $0.95 > 0.7$, the greedy agent will <em>always</em> choose Arm 1. It will never again pull Arm 2 to gather more data and correct its flawed initial estimate. It is "locked in" to the suboptimal choice.</li>
                    </ol>
                    <p>
                        As a result, its total regret will grow linearly with time, which is the worst possible outcome for a learning algorithm. This demonstrates that for learning to occur, exploration is not optional—it is essential.
                    </p>
                </div>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-dice"></i>
              2.5 Simple Exploration: The $\epsilon$-Greedy Strategy
            </summary>
            <div class="details-content">
                <p>
                    The <strong>$\epsilon$-greedy</strong> (epsilon-greedy) strategy is the most fundamental and widely used method to introduce exploration. It is a simple modification of the greedy strategy: behave greedily most of the time, but with a small probability $\epsilon$, choose an action completely at random.
                </p>
                <p><strong>The $\epsilon$-Greedy Action Selection Rule:</strong></p>
                $$ A_t = \begin{cases} \arg\max_{a} \hat{Q}_t(a) & \text{with probability } 1 - \epsilon \\ \text{a random action from } \mathcal{A} & \text{with probability } \epsilon \end{cases} $$
                <p>
                    The parameter $\epsilon \in [0, 1]$ is the <strong>exploration rate</strong>. A value of $\epsilon=0.1$ means the agent explores 10% of the time and exploits 90% of the time. This simple mechanism is incredibly powerful because it guarantees that, as $t \to \infty$, every arm will be sampled an infinite number of times. This property, known as <em>infinite exploration</em>, ensures that the law of large numbers will apply to our estimates. Consequently, the Q-value estimates are guaranteed to converge to their true values: $\hat{Q}_t(a) \to q_*(a)$ for all $a$. This solves the lock-in problem of the purely greedy approach.
                </p>
                
                <h4>The Inefficiency of Uniform (Unintelligent) Exploration</h4>
                <p>
                    While $\epsilon$-greedy is guaranteed to eventually find the optimal action, its exploration is "undirected" or "unintelligent." When it decides to explore, it chooses among all non-greedy actions with equal probability. This can be extremely inefficient.
                </p>
                <p>
                    Consider a 10-armed bandit where the true values are:
                </p>
                <p>
                    $q_* = [0.9, 0.85, 0.1, 0.12, 0.05, 0.08, 0.11, 0.09, 0.13, 0.07]$
                </p>
                <p>
                    Here, Arm 1 is optimal, and Arm 2 is very close to optimal. The other eight arms are terrible. The $\epsilon$-greedy agent, during its exploratory steps, will waste $8/9^{th}$ of its exploration budget on the eight terrible arms, even after it has gathered enough evidence to be almost certain they are bad. A smarter strategy would be to focus exploration on the most promising alternative (Arm 2), rather than treating all non-greedy arms as equally worthy of exploration.
                </p>

                <details class="deep-dive">
                    <summary><i class="fas fa-chart-area"></i> Deep Dive: The Critical Role of Initial Values</summary>
                    <div class="details-content">
                        <p>
                            A simple but effective technique to encourage early exploration is known as <strong>"optimism in the face of uncertainty."</strong> Instead of initializing all Q-estimates to zero, we initialize them to a value that is likely higher than any possible true value (e.g., $\hat{Q}_1(a) = 5$ for all $a$ if we know rewards are in $[-1, 1]$).
                        </p>
                        <p>
                            How does this work? When the agent first pulls an arm, say Arm 1, it receives a reward (e.g., $R_1 = 0.5$). Its new estimate, $\hat{Q}_2(1)$, will be a weighted average of the initial 5 and the received 0.5, so it will be less than 5. Now, all other arms still have an optimistic estimate of 5. A greedy agent will therefore be "encouraged" to try every other arm at least once before settling on one. This provides a burst of initial exploration.
                        </p>
                        <p>
                            This technique is powerful but has limitations. It only provides a temporary burst of exploration. Once all arms have been tried a few times, their estimates will drop below the initial optimistic value, and a purely greedy agent can still get locked in. It is most effective when combined with a persistent exploration strategy like $\epsilon$-greedy.
                        </p>
                    </div>
                </details>
                
                <div class="code-container">
                    <div class="code-block-header">
                        <span><i class="fab fa-python"></i> Python: $\epsilon$-Greedy Agent with Decaying Epsilon</span>
                        <div class="buttons">
                            <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        </div>
                    </div>
                    <pre><code class="language-python">
import numpy as np

class EpsilonGreedyAgent:
    """An agent that follows the epsilon-greedy strategy with optional decay."""
    def __init__(self, k, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, initial_q=0.0):
        self.k = k
        self.epsilon = epsilon_start
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # Estimated Q-values for each arm
        self.q_estimates = np.full(k, initial_q, dtype=float)
        # Number of times each arm has been pulled
        self.arm_counts = np.zeros(k, dtype=int)

    def select_action(self):
        """Selects an arm using the epsilon-greedy policy."""
        if np.random.random() < self.epsilon:
            # Explore: choose a random arm
            return np.random.randint(self.k)
        else:
            # Exploit: choose the arm with the highest current Q-estimate
            # We find all occurrences of the max value and choose one randomly to break ties.
            best_value = np.max(self.q_estimates)
            best_actions = np.where(self.q_estimates == best_value)[0]
            return np.random.choice(best_actions)

    def update(self, action, reward):
        """Updates the Q-estimate for the chosen action."""
        self.arm_counts[action] += 1
        # Use the incremental sample-average update rule
        step_size = 1.0 / self.arm_counts[action]
        error = reward - self.q_estimates[action]
        self.q_estimates[action] += step_size * error
        
        # Decay epsilon for the next step
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

# --- Simulation Code ---
def simulate(agent, env, steps=1000):
    """Run a single simulation of an agent in an environment."""
    reward_history = []
    for _ in range(steps):
        action = agent.select_action()
        reward = env.pull_arm(action)
        agent.update(action, reward)
        reward_history.append(reward)
    return np.array(reward_history)

# --- Example Usage ---
# bandit_env = GaussianBandit(k=10)
# agent = EpsilonGreedyAgent(k=10, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995)
# rewards = simulate(agent, bandit_env)
# print(f"Average reward over 1000 steps: {np.mean(rewards):.2f}")
# print(f"Final epsilon value: {agent.epsilon:.3f}")
                    </code></pre>
                </div>
            </div>
        </details>
</section>
        <details open class="fade-in">
            <summary>
              <i class="fas fa-brain"></i>
              2.6 Directed Exploration: Upper-Confidence-Bound (UCB)
            </summary>
            <div class="details-content">
                <p>
                    The core inefficiency of $\epsilon$-greedy is its random, undirected exploration. A more intelligent and principled approach would be to direct exploration towards actions that are either (1) potentially very good, or (2) highly uncertain. The <strong>Upper-Confidence-Bound (UCB)</strong> family of algorithms formalizes this idea using the statistical principle of <strong>"optimism in the face of uncertainty."</strong>
                </p>
                <p>
                    Instead of exploring randomly, a UCB agent calculates an "optimistic" value for each arm at every time step. This value is the sum of the current Q-value estimate and a bonus term that quantifies the uncertainty in that estimate. The agent then behaves greedily with respect to these optimistic values.
                </p>
                
                <h4>The UCB1 Action Selection Rule</h4>
                <p>The most common UCB algorithm, UCB1, selects an action by solving the following optimization problem at each time step $t$:</p>
                $$A_t \doteq \arg\max_{a \in A} \left[ \hat{Q}_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]$$
                
                <div class="admonition info">
                    <span class="admonition-title"><i class="fas fa-calculator"></i>Breaking Down the UCB Formula: Exploitation + Exploration</span>
                    <p>Let's dissect the two components of the UCB value in great detail:</p>
                    <ul>
                        <li>$\hat{Q}_t(a)$: This is the <strong>exploitation</strong> term. It is the current sample-average estimate of the arm's true value. This term pushes the agent to select arms that have performed well in the past. It represents the agent's current "best guess."</li>
                        <li>$c \sqrt{\frac{\ln t}{N_t(a)}}$: This is the <strong>exploration</strong> or <strong>uncertainty bonus</strong> term. It is the heart of UCB's intelligent exploration. It has several crucial properties:
                            <ul>
                                <li><strong>Dependence on $t$:</strong> The term grows with the total number of time steps, $t$ (specifically, with its logarithm, $\ln t$). This ensures that, over time, the agent will eventually re-explore all arms, even those that initially looked bad. As time goes on, the agent becomes more "curious" about arms it hasn't pulled in a while.</li>
                                <li><strong>Dependence on $N_t(a)$:</strong> The term shrinks as the number of times an arm has been pulled, $N_t(a)$, increases. The more we pull an arm, the more data we have, the more certain we are about its true value, and the smaller the uncertainty bonus becomes. This is the mechanism that directs exploration: arms that have been tried less have a higher uncertainty bonus and are thus more likely to be selected.</li>
                                <li><strong>The role of $c$:</strong> The hyperparameter $c > 0$ is the <strong>exploration constant</strong>. It controls the agent's "optimism." A larger $c$ gives more weight to the uncertainty bonus, leading to more exploration. A smaller $c$ makes the agent more reliant on its current estimates, leading to more exploitation. Tuning $c$ is a way to adjust the algorithm to the specific problem's reward variance.</li>
                            </ul>
                        </li>
                    </ul>
                    <p>
                        The UCB algorithm thus elegantly and deterministically balances exploration and exploitation. At each step, it chooses the arm that has the highest potential to be the best, considering both its past performance and the uncertainty in its value estimate.
                    </p>
                </div>

                <details class="deep-dive">
                    <summary><i class="fas-solid fa-square-root-variable"></i> Deep Dive: Theoretical Origins from Concentration Inequalities</summary>
                    <div class="details-content">
                        <p>
                            The specific form of the UCB bonus term is not arbitrary; it is rigorously derived from concentration inequalities from probability theory, specifically <strong>Hoeffding's Inequality</strong>. This provides a strong theoretical justification for its performance and logarithmic regret bounds.
                        </p>
                        <p>
                            Hoeffding's Inequality provides a bound on the probability that the sum of bounded, independent random variables deviates from its expected value. Let $R_{i,1}, \dots, R_{i,n_i}$ be the $n_i$ rewards received from arm $i$, which are i.i.d. draws from a distribution with mean $q_*(i)$ and are bounded in $[0,1]$. Hoeffding's inequality tells us that:
                        </p>
                        $$ P(q_*(i) > \hat{Q}_{n_i}(i) + \epsilon) \le e^{-2n_i\epsilon^2} $$
                        <p>
                            This inequality states that the probability of the true mean being larger than our empirical estimate plus some margin $\epsilon$ is very small, and this probability decreases exponentially with the number of samples $n_i$.
                        </p>
                        <p>
                            The UCB algorithm's strategy is to choose a margin $\epsilon$ that is large enough to be confident that the true value is below the upper bound $\hat{Q}_{n_i}(i) + \epsilon$. We want this probability of being wrong to decrease as we get more data. So, we set the right-hand side to a small probability $p$ that decreases with the total number of steps $t$, for example, $p = t^{-4}$.
                        </p>
                        $$ t^{-4} = e^{-2N_t(a)\epsilon^2} $$
                        <p>
                            Now, we solve for $\epsilon$:
                        </p>
                        $$ \ln(t^{-4}) = -2N_t(a)\epsilon^2 $$
                        $$ -4 \ln t = -2N_t(a)\epsilon^2 $$
                        $$ \epsilon = \sqrt{\frac{4 \ln t}{2N_t(a)}} = \sqrt{\frac{2 \ln t}{N_t(a)}} $$
                        <p>
                            This shows that, with high probability (specifically, $1 - t^{-4}$), the true mean $q_*(a)$ is bounded above by our estimate plus a term that looks exactly like the UCB bonus: $\hat{Q}_t(a) + \sqrt{\frac{2 \ln t}{N_t(a)}}$. The UCB algorithm acts as if this upper bound were the true value, perfectly embodying the principle of optimism. This theoretical grounding is what gives UCB its strong logarithmic regret guarantees, proving it is a highly efficient exploration strategy.
                        </p>
                    </div>
                </details>
                
                <div class="code-container">
                    <div class="code-block-header">
                        <span><i class="fab fa-python"></i> Python: UCB Agent Implementation and Comparison</span>
                        <div class="buttons">
                            <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        </div>
                    </div>
                    <pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

class UCBAgent:
    """An agent that follows the Upper-Confidence-Bound strategy."""
    def __init__(self, k, c=2.0, initial_q=0.0):
        self.k = k
        self.c = c  # The exploration parameter
        self.q_estimates = np.full(k, initial_q, dtype=float)
        self.arm_counts = np.zeros(k, dtype=int)
        self.t = 0 # Total step count

    def select_action(self):
        """Selects an arm using the UCB1 formula."""
        self.t += 1
        
        # First, play each arm once to get an initial estimate. This is crucial to avoid division by zero.
        for arm in range(self.k):
            if self.arm_counts[arm] == 0:
                return arm
        
        # Calculate the UCB values for all arms
        uncertainty_bonus = self.c * np.sqrt(np.log(self.t) / self.arm_counts)
        ucb_values = self.q_estimates + uncertainty_bonus
        
        # Select the arm with the highest UCB value. Break ties randomly.
        max_value = np.max(ucb_values)
        best_actions = np.where(ucb_values == max_value)[0]
        return np.random.choice(best_actions)

    def update(self, action, reward):
        """Updates the Q-estimate for the chosen action."""
        self.arm_counts[action] += 1
        step_size = 1.0 / self.arm_counts[action]
        error = reward - self.q_estimates[action]
        self.q_estimates[action] += step_size * error

# --- Simulation to compare UCB with Epsilon-Greedy ---
def run_experiment(agent_class, env, steps=1000, num_runs=200, **kwargs):
    """Runs multiple simulations and averages the results for robust comparison."""
    all_rewards = np.zeros((num_runs, steps))
    for i in range(num_runs):
        # Re-initialize agent and env for each independent run
        current_env = GaussianBandit(k=env.k, seed=i)
        agent = agent_class(k=current_env.k, **kwargs)
        
        reward_history = []
        for _ in range(steps):
            action = agent.select_action()
            reward = current_env.pull_arm(action)
            agent.update(action, reward)
            reward_history.append(reward)
        all_rewards[i, :] = reward_history
        
    return np.mean(all_rewards, axis=0)

# bandit_env_template = GaussianBandit(k=10)
# egreedy_rewards_avg = run_experiment(EpsilonGreedyAgent, bandit_env_template, epsilon=0.1)
# ucb_rewards_avg = run_experiment(UCBAgent, bandit_env_template, c=2.0)

# plt.figure(figsize=(12, 6))
# plt.plot(egreedy_rewards_avg, label="Avg Reward per Step (Epsilon-Greedy, ε=0.1)")
# plt.plot(ucb_rewards_avg, label="Avg Reward per Step (UCB, c=2.0)")
# plt.title("Performance Comparison: Epsilon-Greedy vs. UCB")
# plt.xlabel("Steps")
# plt.ylabel("Average Reward (Averaged over 200 runs)")
# plt.legend()
# plt.grid(True)
# plt.show()
# # Expected outcome: The UCB plot will typically show a steeper initial learning curve and converge to a higher average reward
# # than epsilon-greedy, demonstrating its superior exploration strategy.
                    </code></pre>
                </div>
            </div>
        </details>
</section>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-bullseye"></i>
            1.4 The Objective I: The Return ($G_t$) and Defining the Goal
          </summary>
          <div class="details-content">
              <p>
                  With the MDP framework established, we can now formally define precisely what the agent is trying to achieve. The agent's goal is not merely to get a high reward on the next step, but to select actions that will lead to the greatest possible <strong>cumulative reward</strong> over its lifetime. This long-term objective is formalized by a concept called the <strong>Return</strong>.
              </p>

              <h4>Episodic vs. Continuing Tasks: Defining the Horizon</h4>
              <p>Before we can define the return, we must distinguish between two fundamental types of tasks in RL, as the definition of "cumulative reward" depends on whether the task has an end.</p>
              <ul>
                <li><strong>Episodic Tasks:</strong> These tasks have a natural end point, called a <strong class="tooltip" data-tooltip="A state that, once entered, terminates the interaction. No further actions can be taken or rewards received.">terminal state</strong>. Each run of the task, from a start state to a terminal state, is called an "episode." For these tasks, the sequence of rewards is finite. Examples include playing a game of chess (ends in checkmate, stalemate, or draw), navigating a maze (ends at the exit), or a robotic pick-and-place task (ends when the object is placed).</li>
                <li><strong>Continuing Tasks:</strong> These tasks have no terminal state and could, in principle, go on forever. For these tasks, a simple sum of rewards could diverge to infinity, making it impossible to compare different strategies. Examples include a stock-trading bot that runs indefinitely, a climate control system maintaining temperature, or an agent managing a power grid.</li>
              </ul>

              <h4>Defining the Discounted Return</h4>
              <p>
                To handle both episodic and continuing tasks with a single, elegant equation, we introduce the concept of discounting. The <strong>Return</strong> at time $t$, denoted $G_t$, is the total discounted reward from time $t$ onward.
              </p>
              $$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
              
              <div class="admonition info">
                  <span class="admonition-title"><i class="fas fa-calculator"></i>Math Breakdown: Unpacking the Return Formula</span>
                  <p>Let's dissect the equation for the return, $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$, piece by piece:</p>
                  <ul>
                      <li>$G_t$: This is the <strong>Return</strong>, the quantity we want our agent to learn to maximize. It represents the total value of all future rewards as seen from the perspective of the current time step $t$.</li>
                      <li>$\sum_{k=0}^{\infty}$: This is a sum over all future time steps. The index $k$ represents how many steps into the future we are looking. $k=0$ is the next step, $k=1$ is two steps away, and so on. For episodic tasks, this sum is finite and terminates when a terminal state is reached.</li>
                      <li>$R_{t+k+1}$: This is the reward received at the future time step $t+k+1$. When $k=0$, it's the immediate reward $R_{t+1}$.</li>
                      <li>$\gamma^k$: This is the <strong>discount factor</strong> $\gamma$ raised to the power of $k$. Since $\gamma \in [0, 1)$, this term gets smaller as $k$ gets larger. It is the mathematical mechanism that makes rewards further in the future less valuable than immediate rewards.</li>
                  </ul>
              </div>

              <div class="admonition tip">
                <span class="admonition-title"><i class="fas fa-recycle"></i>The Profound Recursion of the Return</span>
                <p>The definition of the return has a powerful recursive property that is the absolute foundation of the Bellman equations and nearly all of RL. We can express the return at time $t$ in terms of the return at time $t+1$:</p>
                $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$$
                <p>By factoring out a $\gamma$ from all terms except the first, we get:</p>
                $$G_t = R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \dots)$$
                <p>We can see that the term in the parentheses is simply the definition of the return at the <em>next</em> time step, $G_{t+1}$. This gives us the fundamental relationship:</p>
                $$G_t = R_{t+1} + \gamma G_{t+1}$$
                <p>This simple equation is profound. It shows that the total future reward from now is simply the immediate reward we get, plus the discounted total future reward from the next state. This allows us to break a large, complex, long-horizon problem into a series of smaller, self-similar, one-step subproblems. This is the core idea of dynamic programming and the engine that drives RL algorithms.</p>
              </div>
              
              <div class="code-container">
                  <div class="code-block-header">
                      <span><i class="fab fa-python"></i> Python: Calculating Discounted Returns from a Trajectory</span>
                      <div class="buttons">
                          <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i></button>
                      </div>
                  </div>
                  <pre><code class="language-python">
import numpy as np

def calculate_returns_from_trajectory(rewards: list[float], gamma: float) -> np.ndarray:
    """
    Calculates the discounted return G_t for each time step t in an episode.
    This is typically done by iterating backwards from the end of the episode.
    
    Args:
        rewards: A list of rewards received during the episode [r_1, r_2, ..., r_T].
        gamma: The discount factor.
        
    Returns:
        An array of returns [G_0, G_1, ..., G_{T-1}].
    """
    returns = np.zeros_like(rewards, dtype=float)
    future_return = 0.0
    
    # Iterate backwards from the last reward to the first
    for t in reversed(range(len(rewards))):
        # The return at time t is the immediate reward r_{t+1} plus the discounted future return from t+1
        # G_t = r_{t+1} + gamma * G_{t+1}
        # In our loop, future_return is G_{t+1} from the previous iteration.
        future_return = rewards[t] + gamma * future_return
        returns[t] = future_return
        
    return returns

# --- Example Calculation ---
# An agent moves for 3 steps and then finds the goal.
# rewards = [-1, -1, -1, 10]
# gamma = 0.99
# returns = calculate_returns_from_trajectory(rewards, gamma)
# print("\n=== Return Calculation Example ===")
# print(f"Rewards:     {rewards}")
# print(f"Gamma:       {gamma}")
# print(f"Returns (G_t): {[f'{r:.2f}' for r in returns]}")
# Expected Output:
# G_3 = 10.00
# G_2 = -1 + 0.99 * 10.00 = 8.90
# G_1 = -1 + 0.99 * 8.90  = 7.81
# G_0 = -1 + 0.99 * 7.81  = 6.73
                  </code></pre>
              </div>
          </div>
        </details>
        <details open class="fade-in">
            <summary>
              <i class="fas-solid fa-chart-pie"></i>
              2.7 Bayesian Exploration: Thompson Sampling
            </summary>
            <div class="details-content">
                <p>
                    While UCB uses uncertainty to create a single optimistic estimate, a different and often more powerful approach is to model our uncertainty explicitly using the language of probability. <strong>Thompson Sampling</strong>, also known as posterior sampling, is a Bayesian algorithm that does exactly this. It is one of the oldest heuristics for this problem, first proposed by William R. Thompson in 1933 for use in clinical trials, but its effectiveness and computational efficiency have led to a major resurgence in the modern era.
                </p>
                <p>
                    Instead of maintaining a single point estimate for each arm's value, $\hat{Q}(a)$, Thompson Sampling maintains a full <strong>probability distribution</strong> that represents our evolving belief about each arm's true value. Action selection then becomes a simple, elegant, and highly effective generative process.
                </p>
                
                <h4>The Thompson Sampling Algorithm: A Step-by-Step Guide</h4>
                <p>The algorithm is remarkably simple to state, yet it encapsulates a deep connection between belief and action.</p>
                <ol>
                    <li><strong>Step 1: Maintain a Belief (Posterior Distribution).</strong> For each arm $a$, we maintain a posterior probability distribution $P(\theta_a | D_a)$. This distribution represents our current belief about the arm's true mean reward $\theta_a$ (equivalent to $q_*(a)$), given the data $D_a$ (the history of rewards) we have observed from that arm so far.</li>
                    <li><strong>Step 2: Sample from the Belief (Posterior Sampling).</strong> At the beginning of each time step $t$, we draw one random sample from each arm's current posterior distribution. Let's call these samples $\hat{\theta}_a(t)$.
                        $$ \text{For each arm } a \in \{1, \dots, k\}, \text{ sample } \hat{\theta}_a(t) \sim P(\theta_a | D_a) $$
                    </li>
                    <li><strong>Step 3: Act Greedily on the Samples.</strong> We then choose the arm corresponding to the largest sample. This is the only "greedy" part of the algorithm.
                        $$ A_t = \arg\max_a \hat{\theta}_a(t) $$
                    </li>
                    <li><strong>Step 4: Update the Belief.</strong> After executing action $A_t$ and observing the reward $R_t$, we update the posterior distribution for the chosen arm, $A_t$, using Bayes' rule to incorporate this new piece of evidence.
                        $$ P(\theta_{A_t} | D_{A_t} \cup \{R_t\}) \propto P(R_t | \theta_{A_t}) P(\theta_{A_t} | D_{A_t}) $$
                    </li>
                </ol>
                <p>
                    This approach elegantly and naturally balances exploration and exploitation. An arm with high uncertainty will have a wide posterior distribution, meaning it has a non-trivial chance of producing a high sample, which encourages exploration. An arm with low uncertainty (one that has been pulled many times) will have a narrow posterior, and its samples will be tightly clustered around its estimated mean, leading to exploitation. This has been described as <strong>probability matching</strong>: the probability that an arm is selected is equal to the probability that it is the optimal arm, according to our current beliefs.
                </p>

                <details class="deep-dive">
                    <summary><i class="fas-solid fa-calculator"></i> Deep Dive: The Magic of Conjugate Priors</summary>
                    <div class="details-content">
                        <p>
                            The Bayesian update in Step 4 seems computationally expensive. In its general form, it would require complex numerical integration. However, we can make this step trivial by choosing our prior distribution family carefully. We use <strong>conjugate priors</strong>.
                        </p>
                        <p>
                            A prior distribution is conjugate to a likelihood function if the resulting posterior distribution is in the same mathematical family as the prior. This means the Bayesian update simplifies to a simple, algebraic update of the distribution's parameters.
                        </p>
                        
                        <h5>Case 1: Bernoulli Rewards (e.g., Clicks, Success/Failure)</h5>
                        <p>
                            This is the classic use case. If rewards are binary (0 or 1), the likelihood of observing a reward is given by the Bernoulli distribution. The conjugate prior for the Bernoulli parameter (the probability of success, $\theta$) is the <strong>Beta distribution</strong>.
                        </p>
                        <p>The Beta distribution is defined by two parameters, $\alpha$ and $\beta$, and its PDF is $f(x; \alpha, \beta) \propto x^{\alpha-1}(1-x)^{\beta-1}$. We can interpret $\alpha-1$ as the number of "successes" and $\beta-1$ as the number of "failures" we have seen.
                        </p>
                        <ul>
                            <li><strong>Prior:</strong> We start with a non-informative prior, typically $Beta(1, 1)$, which is a uniform distribution.</li>
                            <li><strong>Update Rule:</strong> If we pull an arm and observe a success (reward=1), we update its posterior by incrementing its $\alpha$ parameter. If we observe a failure (reward=0), we increment its $\beta$ parameter.
                                <ul>
                                    <li>If $R_t=1$: $\alpha_{A_t} \leftarrow \alpha_{A_t} + 1$</li>
                                    <li>If $R_t=0$: $\beta_{A_t} \leftarrow \beta_{A_t} + 1$</li>
                                </ul>
                            </li>
                        </ul>
                        <p>The update is just one addition. It's extremely fast.</p>

                        <div class="visualization-container">
                            <div id="beta-dist-plot" style="width:100%; height:350px;"></div>
                        </div>
                        <script>
                        (function() {
                            const plotDiv = document.getElementById('beta-dist-plot');
                            if (!plotDiv || typeof Plotly === 'undefined') return;
                            
                            function beta_pdf(x, a, b) {
                                // A simple (un-normalized) proportional PDF for visualization
                                return Math.pow(x, a - 1) * Math.pow(1 - x, b - 1);
                            }

                            const x = Array.from({length: 101}, (_, i) => i / 100);
                            const prior = x.map(v => beta_pdf(v, 1, 1)); // Beta(1,1)
                            const posterior_1 = x.map(v => beta_pdf(v, 1+5, 1+1)); // 5 successes, 1 failure
                            const posterior_2 = x.map(v => beta_pdf(v, 1+20, 1+5)); // 20 successes, 5 failures
                            
                            Plotly.newPlot(plotDiv, [
                                {x: x, y: prior, name: 'Prior: Beta(1, 1) - Uniform', line: {dash: 'dot'}},
                                {x: x, y: posterior_1, name: 'Posterior after 5S, 1F: Beta(6, 2)'},
                                {x: x, y: posterior_2, name: 'Posterior after 20S, 5F: Beta(21, 6)'}
                            ], {
                                title: 'Bayesian Updating of a Beta Distribution',
                                xaxis: {title: 'Arm\'s True Mean Reward (θ)'},
                                yaxis: {title: 'Probability Density'}
                            });
                        })();
                        </script>
                        <p>As shown in the plot, our belief starts as a flat uniform distribution (total uncertainty). After observing a few data points (5 successes, 1 failure), the distribution shifts to favor higher values but is still quite wide. After many data points (20 successes, 5 failures), the distribution becomes much narrower and more peaked around the empirical mean (20/25 = 0.8), reflecting our increased certainty.</p>
                    </div>
                </details>
                
                <div class="code-container">
                    <div class="code-block-header">
                        <span><i class="fab fa-python"></i> Python: Thompson Sampling Agent for Bernoulli Rewards</span>
                        <div class="buttons">
                            <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        </div>
                    </div>
                    <pre><code class="language-python">
import numpy as np

class ThompsonSamplingBernoulliAgent:
    """A Thompson Sampling agent for a bandit with Bernoulli rewards (0 or 1)."""
    def __init__(self, k):
        self.k = k
        # Each arm is modeled by a Beta distribution.
        # We start with a uniform prior (Beta(1, 1)).
        # alpha can be interpreted as the count of successes + 1.
        self.alphas = np.ones(k)
        # beta can be interpreted as the count of failures + 1.
        self.betas = np.ones(k)

    def select_action(self):
        """
        Selects an arm by sampling from each arm's Beta posterior
        and choosing the arm with the highest sample.
        """
        # Sample a value from each arm's current posterior distribution.
        # np.random.beta is highly optimized for this.
        sampled_theta = np.random.beta(self.alphas, self.betas)
        
        # Select the arm with the highest sampled value.
        return np.argmax(sampled_theta)

    def update(self, action, reward):
        """
        Updates the posterior for the chosen action using the observed reward.
        Reward must be 0 or 1 for the Beta-Bernoulli model.
        """
        if reward not in [0, 1]:
            raise ValueError("Reward must be 0 or 1 for Bernoulli Thompson Sampling.")
            
        # The Bayesian update is just a simple addition.
        if reward == 1:
            # If it was a success, increment the alpha parameter.
            self.alphas[action] += 1
        else:
            # If it was a failure, increment the beta parameter.
            self.betas[action] += 1

# --- Example of posterior updates ---
# agent = ThompsonSamplingBernoulliAgent(k=2)
# print(f"Initial beliefs: Arm 1 ~ Beta({agent.alphas[0]}, {agent.betas[0]}), Arm 2 ~ Beta({agent.alphas[1]}, {agent.betas[1]})")
# agent.update(action=0, reward=1) # Observe a success for Arm 1
# agent.update(action=1, reward=0) # Observe a failure for Arm 2
# print(f"Updated beliefs: Arm 1 ~ Beta({agent.alphas[0]}, {agent.betas[0]}), Arm 2 ~ Beta({agent.alphas[1]}, {agent.betas[1]})")
                    </code></pre>
                </div>
            </div>
        </details>
</section>	

        <div class="interactive-lab">
            <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Bandit Strategy Showdown</div>
            <p>
                Theory is essential, but intuition is often built by seeing algorithms perform. This interactive lab allows you to run a head-to-head comparison of the exploration strategies we've discussed on a simulated 10-armed bandit problem. You can select a strategy and its key hyperparameters, and then run a robust simulation (averaged over many independent runs) to see how it performs.
            </p>
            <p>
                Pay close attention to the two plots. The <strong>Average Reward</strong> plot shows how quickly the agent learns to pull good arms. The <strong>% Optimal Action</strong> plot shows how reliably the agent identifies and exploits the single best arm. A good strategy should not only achieve a high average reward but also converge to selecting the optimal action most of the time.
            </p>
            <div class="lab-controls" style="border: 1px solid var(--color-border); padding: 1rem; border-radius: var(--radius-md);">
                <div class="lab-control">
                    <label for="strategy-select">Select Strategy:</label>
                    <select id="strategy-select" style="padding: 8px; border-radius: var(--radius-sm); background: var(--color-surface-solid); color: var(--color-text-primary); border: 1px solid var(--color-border);">
                        <option value="greedy">Greedy</option>
                        <option value="egreedy" selected>Epsilon-Greedy</option>
                        <option value="ucb">UCB</option>
                        <option value="thompson">Thompson Sampling</option>
                    </select>
                </div>
                <div class="lab-control" id="epsilon-control">
                    <label for="epsilon-slider">Epsilon (ε): <span id="epsilon-value-lab">0.1</span></label>
                    <input type="range" id="epsilon-slider" min="0" max="0.5" step="0.01" value="0.1">
                </div>
                <div class="lab-control" id="ucb-control" style="display: none;">
                    <label for="ucb-slider">Exploration Constant (c): <span id="ucb-value-lab">2.0</span></label>
                    <input type="range" id="ucb-slider" min="0.1" max="5" step="0.1" value="2.0">
                </div>
                <button id="run-bandit-lab" style="padding: 8px 16px; border-radius: var(--radius-sm); background: var(--color-accent-success); color: var(--color-bg-primary); border: none; cursor: pointer;">Run Simulation</button>
            </div>
            <p id="lab-status" style="text-align: center; margin-top: 1rem; font-style: italic;"></p>
            <div id="bandit-lab-plot-reward" style="width:100%; height:300px;"></div>
            <div id="bandit-lab-plot-optimal" style="width:100%; height:300px;"></div>
        </div>
        <script>
            // This script is self-contained and uses vanilla JS + Plotly for the interactive lab.
            document.addEventListener('DOMContentLoaded', () => {
                const select = document.getElementById('strategy-select');
                const epsilonControl = document.getElementById('epsilon-control');
                const ucbControl = document.getElementById('ucb-control');
                const runBtn = document.getElementById('run-bandit-lab');
                const statusEl = document.getElementById('lab-status');

                if (!select) return; // Exit if lab elements are not on the page

                // UI update logic
                const updateUI = () => {
                    epsilonControl.style.display = select.value === 'egreedy' ? 'flex' : 'none';
                    ucbControl.style.display = select.value === 'ucb' ? 'flex' : 'none';
                };
                select.addEventListener('change', updateUI);
                document.getElementById('epsilon-slider').addEventListener('input', (e) => {
                    document.getElementById('epsilon-value-lab').textContent = parseFloat(e.target.value).toFixed(2);
                });
                document.getElementById('ucb-slider').addEventListener('input', (e) => {
                    document.getElementById('ucb-value-lab').textContent = parseFloat(e.target.value).toFixed(1);
                });

                // Core simulation logic
                runBtn.addEventListener('click', () => {
                    statusEl.textContent = "Running simulation... (this may take a moment)";
                    runBtn.disabled = true;

                    // Use a timeout to allow the UI to update before the heavy computation starts
                    setTimeout(() => {
                        const strategy = select.value;
                        const epsilon = parseFloat(document.getElementById('epsilon-slider').value);
                        const c = parseFloat(document.getElementById('ucb-slider').value);
                        
                        // --- Define Bandit and Agent classes in scope ---
                        class LabBandit {
                            constructor(k = 10, seed) {
                                const rng = new Math.seedrandom(seed);
                                this.means = Array.from({ length: k }, () => rng() * 4 - 2); // Means in [-2, 2]
                                this.optimal_arm = this.means.indexOf(Math.max(...this.means));
                            }
                            pull(arm) {
                                return this.means[arm] + (Math.random() - 0.5) * 2; // Gaussian-like noise
                            }
                        }
                        
                        class BaseAgent {
                            constructor(k) { this.k = k; this.q = Array(k).fill(0); this.n = Array(k).fill(0); }
                            update(a, r) { this.n[a]++; this.q[a] += (r - this.q[a]) / this.n[a]; }
                        }
                        class LabGreedy extends BaseAgent {
                            select() { return this.q.indexOf(Math.max(...this.q)); }
                        }
                        class LabEpsGreedy extends BaseAgent {
                            constructor(k, eps) { super(k); this.eps = eps; }
                            select() { return Math.random() < this.eps ? Math.floor(Math.random() * this.k) : this.q.indexOf(Math.max(...this.q)); }
                        }
                        class LabUCB extends BaseAgent {
                            constructor(k, c) { super(k); this.c = c; this.t = 0; }
                            select() {
                                this.t++;
                                for (let i = 0; i < this.k; i++) { if (this.n[i] === 0) return i; }
                                const bonus = this.q.map((val, idx) => val + this.c * Math.sqrt(Math.log(this.t) / this.n[idx]));
                                return bonus.indexOf(Math.max(...bonus));
                            }
                        }
                        class LabThompson extends BaseAgent {
                            constructor(k) {
                                super(k);
                                this.alphas = Array(k).fill(1);
                                this.betas = Array(k).fill(1);
                            }
                            select() {
                                const samples = this.alphas.map((alpha, i) => jStat.beta.sample(alpha, this.betas[i]));
                                return samples.indexOf(Math.max(...samples));
                            }
                            update(a, r) {
                                // Normalize reward to [0,1] for Beta distribution update
                                const norm_r = (r + 4) / 8; // Assuming rewards are roughly in [-4, 4]
                                if (Math.random() < norm_r) this.alphas[a]++; else this.betas[a]++;
                            }
                        }

                        // --- Run Simulation ---
                        const STEPS = 1000;
                        const RUNS = 50;
                        let avg_rewards = Array(STEPS).fill(0);
                        let optimal_counts = Array(STEPS).fill(0);

                        for (let i = 0; i < RUNS; i++) {
                            const env = new LabBandit(10, i.toString());
                            let agent;
                            if (strategy === 'greedy') agent = new LabGreedy(10);
                            else if (strategy === 'egreedy') agent = new LabEpsGreedy(10, epsilon);
                            else if (strategy === 'ucb') agent = new LabUCB(10, c);
                            else agent = new LabThompson(10);

                            for (let t = 0; t < STEPS; t++) {
                                const action = agent.select();
                                const reward = env.pull(action);
                                agent.update(action, reward);
                                avg_rewards[t] += reward;
                                if (action === env.optimal_arm) optimal_counts[t]++;
                            }
                        }
                        avg_rewards = avg_rewards.map(r => r / RUNS);
                        optimal_counts = optimal_counts.map(c => (c / RUNS) * 100);

                        // --- Plotting ---
                        Plotly.newPlot('bandit-lab-plot-reward', [{x: [...Array(STEPS).keys()], y: avg_rewards, line: {color: 'var(--color-accent-primary)'}}], {
                            title: 'Average Reward Over Time', xaxis: {title: 'Steps'}, yaxis: {title: 'Average Reward'}
                        }, {responsive: true});
                        Plotly.newPlot('bandit-lab-plot-optimal', [{x: [...Array(STEPS).keys()], y: optimal_counts, line: {color: 'var(--color-accent-success)'}}], {
                            title: '% Optimal Action Selected', xaxis: {title: 'Steps'}, yaxis: {title: '% Optimal Action', range: [0, 100]}
                        }, {responsive: true});

                        statusEl.textContent = "Simulation complete. Try another strategy!";
                        runBtn.disabled = false;
                    }, 50); // Timeout allows UI to update
                });
                // Initial UI setup
                updateUI();
                // Add jStat for beta distribution sampling
                const jStatScript = document.createElement('script');
                jStatScript.src = 'https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js';
                document.head.appendChild(jStatScript);
                 // Add seedrandom for reproducible randomness
                const seedRandomScript = document.createElement('script');
                seedRandomScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js';
                document.head.appendChild(seedRandomScript);
            });
        </script>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-forward"></i>
              2.8 From Bandits to Full Reinforcement Learning
            </summary>
            <div class="details-content">
                <p>
                    The strategies developed for the simple, single-state bandit problem form the foundation for exploration in full, multi-state Reinforcement Learning. In a full MDP, the challenge is not just to explore a small set of actions, but to explore a vast <strong>state-action space</strong>. The value of an action now depends critically on the state you are in, $Q(s,a)$.
                </p>
                <p>
                    However, the core principles remain the same. We must still balance acting on our current value estimates with trying new things to improve them. Here is how bandit strategies are adapted:
                </p>
                <ul>
                    <li><strong>DQN's $\epsilon$-Greedy Policy:</strong> The standard Deep Q-Network (DQN) algorithm, which we will study in depth in the next part, uses an $\epsilon$-greedy strategy for exploration. The Q-values it learns are state-dependent, $\hat{Q}(s,a;\theta)$, but the exploration mechanism is identical to the bandit version: with probability $\epsilon$, it takes a random action; otherwise, it takes the action $\arg\max_a \hat{Q}(s,a;\theta)$.</li>
                    <li><strong>Uncertainty-Aware Deep RL:</strong> More advanced deep RL methods adapt the principles of UCB and Thompson Sampling to high-dimensional function approximators.
                        <ul>
                            <li><strong>UCB in Deep RL:</strong> Some algorithms add an exploration bonus to the predicted Q-value based on state visitation counts (or pseudo-counts from a density model), mimicking UCB.</li>
                            <li><strong>Bayesian Deep Learning:</strong> Methods like Bootstrapped DQN train an ensemble of several Q-networks. At each step, one network is randomly chosen to guide action selection. The variance in the predictions across the different networks for a given state-action pair serves as a measure of uncertainty, similar to the variance in the posterior of Thompson Sampling. This encourages exploration in parts of the state space where the networks "disagree."</li>
                        </ul>
                    </li>
                    <li><strong>Intrinsic Motivation and Curiosity:</strong> In environments with extremely sparse rewards (e.g., a game where the only reward is at the very end), random exploration is hopelessly inefficient. Curiosity-driven methods provide the agent with an <strong>intrinsic reward</strong> for visiting novel or surprising states. For example, the agent might be rewarded for taking actions that lead to states that are hard for a predictive model to forecast. This encourages the agent to systematically explore its environment even in the absence of external rewards.</li>
                </ul>
                <p>
                    As we move into the next parts of this guide, we will see these exploration strategies appear again and again as essential components of the full deep reinforcement learning algorithms.
                </p>
            </div>
        </details>
</section>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-bullseye"></i>
            3.3 DQN's Innovations Part 2: Fixed Target Network
          </summary>
          <div class="details-content">
              <p>
                  Experience replay solves the problem of correlated data. However, it does not solve the second major issue: the <strong>non-stationary target problem</strong>. This is a subtle but critical source of instability that arises from the bootstrapping nature of the Q-learning update.
              </p>
              
              <h5>The "Dog Chasing Its Own Tail" Problem</h5>
              <p>
                  Recall the loss function for our Q-network. We are trying to minimize the Mean Squared Error between our network's prediction, $Q(s,a; \theta)$, and a target value, $y_t$.
              </p>
              $$ L(\theta) = \mathbb{E} \left[ \left( \underbrace{R_t + \gamma \max_{a'} Q(S_{t+1}, a'; \theta)}_{\text{Target } y_t} - \underbrace{Q(S_t, A_t; \theta)}_{\text{Prediction}} \right)^2 \right] $$
              <p>
                  Notice that the network's parameters, $\theta$, appear in both the prediction and the target. When we perform a gradient descent update on this loss, we are adjusting $\theta$ to make our prediction $Q(S_t, A_t; \theta)$ closer to the target $y_t$. However, because the target <em>also</em> depends on $\theta$, the target itself moves with every update step.
              </p>
              <div class="admonition danger">
                  <span class="admonition-title"><i class="fas fa-sync-alt fa-spin"></i>Analogy: Chasing a Moving Goalpost</span>
                  <p>
                      Imagine trying to shoot a soccer ball into a goal. This is a standard optimization problem. Now, imagine that every time you kick the ball, the goalpost immediately moves a little bit. This is the challenge of a non-stationary target. It's incredibly difficult to converge on a good solution because the objective is constantly shifting.
                  </p>
                  <p>
                      In Q-learning, this can lead to dangerous feedback loops. If the network slightly overestimates a Q-value, the target for that state will also increase, pushing the network to overestimate it even more in the next update. This can cause oscillations and, in the worst case, the Q-values can diverge to infinity.
                  </p>
              </div>

              <h5>The Solution: A Separate, Fixed Target Network</h5>
              <p>
                  DQN's second major innovation was to break this dependency by using two separate neural networks:
              </p>
              <ol>
                  <li>An <strong>Online Network</strong>, parameterized by $\theta$, which is the network we are actively training. It is used to select actions (the policy) and to calculate the "prediction" side of the loss.</li>
                  <li>A <strong>Target Network</strong>, parameterized by $\theta^-$, which is a periodic copy of the online network. Its weights are held frozen for a fixed number of steps (a hyperparameter, `TARGET_UPDATE_FREQUENCY`). This network is used <em>only</em> to calculate the "target" side of the loss.</li>
              </ol>
              <p>The loss function is modified to use this separation:</p>
              $$ L(\theta) = \mathbb{E} \left[ \left( \underbrace{R_t + \gamma \max_{a'} Q(S_{t+1}, a'; \theta^-)}_{\text{Stable Target from Target Network}} - \underbrace{Q(S_t, A_t; \theta)}_{\text{Prediction from Online Network}} \right)^2 \right] $$
              <p>
                  By using $\theta^-$ for the target, the goalpost is held fixed for many updates. The online network $\theta$ can now learn towards a stable, stationary target, making the optimization problem much more like standard supervised learning. After `TARGET_UPDATE_FREQUENCY` steps, the weights from the online network are copied over to the target network ($\theta^- \leftarrow \theta$), and the target is updated to a new, better, but still temporarily fixed position.
              </p>
              
              <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Training Step"
        OnlineNet["Online Network Q(s,a; θ)"]
        TargetNet["Target Network Q(s,a; θ⁻)"]
        Loss((Loss Calculation))
        
        OnlineNet -- "Prediction" --> Loss
        TargetNet -- "Target" --> Loss
        Loss -- "∇θ" --> Optimizer[SGD Update]
        Optimizer --> OnlineNet
    end
    
    subgraph "Parameter Sync"
       Sync["Synchronization θ⁻ ← θ"]
    end

    OnlineNet --> Sync
    Sync --> TargetNet

    class OnlineNet online-class
    class TargetNet target-class
                </div>
                <p style="text-align:center; font-style:italic;">The two-network architecture in DQN. The online network is updated at every step, while the target network's weights are frozen and only periodically updated, providing a stable target for the loss computation.</p>
              </div>

              <details class="deep-dive">
                <summary><i class="fas fa-wave-square"></i> Deep Dive: Soft Target Updates (Polyak Averaging)</summary>
                <div class="details-content">
                    <p>
                        The "hard" periodic copying of weights ($\theta^- \leftarrow \theta$) can sometimes cause a sudden shift in the target values, which can temporarily destabilize learning. A common and often superior alternative is to use <strong>soft target updates</strong>, also known as Polyak averaging.
                    </p>
                    <p>
                        Instead of a hard copy every C steps, the target network's weights are slowly updated at <em>every</em> training step to track the online network's weights:
                    </p>
                    $$ \theta^- \leftarrow \tau \theta + (1 - \tau) \theta^- $$
                    <p>
                        Here, $\tau$ is a small hyperparameter (e.g., $\tau=0.005$). This creates a smoothly moving average of the online network's weights. The target still moves much more slowly than the online network, providing stability, but the updates are continuous rather than discrete. This technique is now standard practice in many modern DRL algorithms, including DDPG and SAC.
                    </p>
                </div>
              </details>
          </div>
        </details>
        <div class="interactive-lab">
            <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Bandit Strategy Showdown</div>
            <p>
                Theory is essential, but intuition is often built by seeing algorithms perform. This interactive lab allows you to run a head-to-head comparison of the exploration strategies we've discussed on a simulated 10-armed bandit problem. You can select a strategy and its key hyperparameters, and then run a robust simulation (averaged over many independent runs) to see how it performs.
            </p>
            <p>
                Pay close attention to the two plots. The <strong>Average Reward</strong> plot shows how quickly the agent learns to pull good arms. The <strong>% Optimal Action</strong> plot shows how reliably the agent identifies and exploits the single best arm. A good strategy should not only achieve a high average reward but also converge to selecting the optimal action most of the time.
            </p>
            <div class="lab-controls" style="border: 1px solid var(--color-border); padding: 1rem; border-radius: var(--radius-md);">
                <div class="lab-control">
                    <label for="strategy-select">Select Strategy:</label>
                    <select id="strategy-select" style="padding: 8px; border-radius: var(--radius-sm); background: var(--color-surface-solid); color: var(--color-text-primary); border: 1px solid var(--color-border);">
                        <option value="greedy">Greedy</option>
                        <option value="egreedy" selected>Epsilon-Greedy</option>
                        <option value="ucb">UCB</option>
                        <option value="thompson">Thompson Sampling</option>
                    </select>
                </div>
                <div class="lab-control" id="epsilon-control">
                    <label for="epsilon-slider">Epsilon (ε): <span id="epsilon-value-lab">0.1</span></label>
                    <input type="range" id="epsilon-slider" min="0" max="0.5" step="0.01" value="0.1">
                </div>
                <div class="lab-control" id="ucb-control" style="display: none;">
                    <label for="ucb-slider">Exploration Constant (c): <span id="ucb-value-lab">2.0</span></label>
                    <input type="range" id="ucb-slider" min="0.1" max="5" step="0.1" value="2.0">
                </div>
                <button id="run-bandit-lab" style="padding: 8px 16px; border-radius: var(--radius-sm); background: var(--color-accent-success); color: var(--color-bg-primary); border: none; cursor: pointer;">Run Simulation</button>
            </div>
            <p id="lab-status" style="text-align: center; margin-top: 1rem; font-style: italic;"></p>
            <div id="bandit-lab-plot-reward" style="width:100%; height:300px;"></div>
            <div id="bandit-lab-plot-optimal" style="width:100%; height:300px;"></div>
        </div>
        <script>
            // This script is self-contained and uses vanilla JS + Plotly for the interactive lab.
            document.addEventListener('DOMContentLoaded', () => {
                const select = document.getElementById('strategy-select');
                const epsilonControl = document.getElementById('epsilon-control');
                const ucbControl = document.getElementById('ucb-control');
                const runBtn = document.getElementById('run-bandit-lab');
                const statusEl = document.getElementById('lab-status');

                if (!select) return; // Exit if lab elements are not on the page

                // UI update logic
                const updateUI = () => {
                    epsilonControl.style.display = select.value === 'egreedy' ? 'flex' : 'none';
                    ucbControl.style.display = select.value === 'ucb' ? 'flex' : 'none';
                };
                select.addEventListener('change', updateUI);
                document.getElementById('epsilon-slider').addEventListener('input', (e) => {
                    document.getElementById('epsilon-value-lab').textContent = parseFloat(e.target.value).toFixed(2);
                });
                document.getElementById('ucb-slider').addEventListener('input', (e) => {
                    document.getElementById('ucb-value-lab').textContent = parseFloat(e.target.value).toFixed(1);
                });

                // Core simulation logic
                runBtn.addEventListener('click', () => {
                    statusEl.textContent = "Running simulation... (this may take a moment)";
                    runBtn.disabled = true;

                    // Use a timeout to allow the UI to update before the heavy computation starts
                    setTimeout(() => {
                        const strategy = select.value;
                        const epsilon = parseFloat(document.getElementById('epsilon-slider').value);
                        const c = parseFloat(document.getElementById('ucb-slider').value);
                        
                        // --- Define Bandit and Agent classes in scope ---
                        class LabBandit {
                            constructor(k = 10, seed) {
                                const rng = new Math.seedrandom(seed);
                                this.means = Array.from({ length: k }, () => rng() * 4 - 2); // Means in [-2, 2]
                                this.optimal_arm = this.means.indexOf(Math.max(...this.means));
                            }
                            pull(arm) {
                                return this.means[arm] + (Math.random() - 0.5) * 2; // Gaussian-like noise
                            }
                        }
                        
                        class BaseAgent {
                            constructor(k) { this.k = k; this.q = Array(k).fill(0); this.n = Array(k).fill(0); }
                            update(a, r) { this.n[a]++; this.q[a] += (r - this.q[a]) / this.n[a]; }
                        }
                        class LabGreedy extends BaseAgent {
                            select() { return this.q.indexOf(Math.max(...this.q)); }
                        }
                        class LabEpsGreedy extends BaseAgent {
                            constructor(k, eps) { super(k); this.eps = eps; }
                            select() { return Math.random() < this.eps ? Math.floor(Math.random() * this.k) : this.q.indexOf(Math.max(...this.q)); }
                        }
                        class LabUCB extends BaseAgent {
                            constructor(k, c) { super(k); this.c = c; this.t = 0; }
                            select() {
                                this.t++;
                                for (let i = 0; i < this.k; i++) { if (this.n[i] === 0) return i; }
                                const bonus = this.q.map((val, idx) => val + this.c * Math.sqrt(Math.log(this.t) / this.n[idx]));
                                return bonus.indexOf(Math.max(...bonus));
                            }
                        }
                        class LabThompson extends BaseAgent {
                            constructor(k) {
                                super(k);
                                this.alphas = Array(k).fill(1);
                                this.betas = Array(k).fill(1);
                            }
                            select() {
                                const samples = this.alphas.map((alpha, i) => jStat.beta.sample(alpha, this.betas[i]));
                                return samples.indexOf(Math.max(...samples));
                            }
                            update(a, r) {
                                // Normalize reward to [0,1] for Beta distribution update
                                const norm_r = (r + 4) / 8; // Assuming rewards are roughly in [-4, 4]
                                if (Math.random() < norm_r) this.alphas[a]++; else this.betas[a]++;
                            }
                        }

                        // --- Run Simulation ---
                        const STEPS = 1000;
                        const RUNS = 50;
                        let avg_rewards = Array(STEPS).fill(0);
                        let optimal_counts = Array(STEPS).fill(0);

                        for (let i = 0; i < RUNS; i++) {
                            const env = new LabBandit(10, i.toString());
                            let agent;
                            if (strategy === 'greedy') agent = new LabGreedy(10);
                            else if (strategy === 'egreedy') agent = new LabEpsGreedy(10, epsilon);
                            else if (strategy === 'ucb') agent = new LabUCB(10, c);
                            else agent = new LabThompson(10);

                            for (let t = 0; t < STEPS; t++) {
                                const action = agent.select();
                                const reward = env.pull(action);
                                agent.update(action, reward);
                                avg_rewards[t] += reward;
                                if (action === env.optimal_arm) optimal_counts[t]++;
                            }
                        }
                        avg_rewards = avg_rewards.map(r => r / RUNS);
                        optimal_counts = optimal_counts.map(c => (c / RUNS) * 100);

                        // --- Plotting ---
                        Plotly.newPlot('bandit-lab-plot-reward', [{x: [...Array(STEPS).keys()], y: avg_rewards, line: {color: 'var(--color-accent-primary)'}}], {
                            title: 'Average Reward Over Time', xaxis: {title: 'Steps'}, yaxis: {title: 'Average Reward'}
                        }, {responsive: true});
                        Plotly.newPlot('bandit-lab-plot-optimal', [{x: [...Array(STEPS).keys()], y: optimal_counts, line: {color: 'var(--color-accent-success)'}}], {
                            title: '% Optimal Action Selected', xaxis: {title: 'Steps'}, yaxis: {title: '% Optimal Action', range: [0, 100]}
                        }, {responsive: true});

                        statusEl.textContent = "Simulation complete. Try another strategy!";
                        runBtn.disabled = false;
                    }, 50); // Timeout allows UI to update
                });
                // Initial UI setup
                updateUI();
                // Add jStat for beta distribution sampling
                const jStatScript = document.createElement('script');
                jStatScript.src = 'https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js';
                document.head.appendChild(jStatScript);
                 // Add seedrandom for reproducible randomness
                const seedRandomScript = document.createElement('script');
                seedRandomScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js';
                document.head.appendChild(seedRandomScript);
            });
        </script>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-forward"></i>
              2.8 From Bandits to Full Reinforcement Learning
            </summary>
            <div class="details-content">
                <p>
                    The strategies developed for the simple, single-state bandit problem form the foundation for exploration in full, multi-state Reinforcement Learning. In a full MDP, the challenge is not just to explore a small set of actions, but to explore a vast <strong>state-action space</strong>. The value of an action now depends critically on the state you are in, $Q(s,a)$.
                </p>
                <p>
                    However, the core principles remain the same. We must still balance acting on our current value estimates with trying new things to improve them. Here is how bandit strategies are adapted:
                </p>
                <ul>
                    <li><strong>DQN's $\epsilon$-Greedy Policy:</strong> The standard Deep Q-Network (DQN) algorithm, which we will study in depth in the next part, uses an $\epsilon$-greedy strategy for exploration. The Q-values it learns are state-dependent, $\hat{Q}(s,a;\theta)$, but the exploration mechanism is identical to the bandit version: with probability $\epsilon$, it takes a random action; otherwise, it takes the action $\arg\max_a \hat{Q}(s,a;\theta)$.</li>
                    <li><strong>Uncertainty-Aware Deep RL:</strong> More advanced deep RL methods adapt the principles of UCB and Thompson Sampling to high-dimensional function approximators.
                        <ul>
                            <li><strong>UCB in Deep RL:</strong> Some algorithms add an exploration bonus to the predicted Q-value based on state visitation counts (or pseudo-counts from a density model), mimicking UCB.</li>
                            <li><strong>Bayesian Deep Learning:</strong> Methods like Bootstrapped DQN train an ensemble of several Q-networks. At each step, one network is randomly chosen to guide action selection. The variance in the predictions across the different networks for a given state-action pair serves as a measure of uncertainty, similar to the variance in the posterior of Thompson Sampling. This encourages exploration in parts of the state space where the networks "disagree."</li>
                        </ul>
                    </li>
                    <li><strong>Intrinsic Motivation and Curiosity:</strong> In environments with extremely sparse rewards (e.g., a game where the only reward is at the very end), random exploration is hopelessly inefficient. Curiosity-driven methods provide the agent with an <strong>intrinsic reward</strong> for visiting novel or surprising states. For example, the agent might be rewarded for taking actions that lead to states that are hard for a predictive model to forecast. This encourages the agent to systematically explore its environment even in the absence of external rewards.</li>
                </ul>
                <p>
                    As we move into the next parts of this guide, we will see these exploration strategies appear again and again as essential components of the full deep reinforcement learning algorithms.
                </p>
            </div>
        </details>
</section>
      <hr/>
      <section id="part3">
        <h2 id="part3-title">
          <i class="fas fa-chart-line"></i>
          Part 3: Value-Based Methods
        </h2>
        <p>
            Having established the foundational concepts of RL and the exploration-exploitation dilemma, we now turn to the first major family of modern deep reinforcement learning algorithms: <strong>value-based methods</strong>. The core idea of this family is to learn an accurate estimate of the optimal action-value function, $Q^*(s,a)$. Once this function is known, the optimal policy is simple: in any given state, just choose the action with the highest Q-value. This part covers the genesis of this approach with Deep Q-Networks (DQN) and explores the key architectural and algorithmic innovations that have made it one of the most successful paradigms in deep RL.
        </p>

        <details open class="fade-in">
            <summary>
              <i class="fas fa-biohazard"></i>
              3.1 T	
            </summary>
            <div class="details-content">
                <p>
                    The idea of combining Q-learning with a neural network as a function approximator seems straightforward: simply use the Bellman optimality equation to create a loss function and train the network with gradient descent. However, for years, this naive approach was known to be fundamentally unstable.
                </p>
                <p>
                    Early attempts in the 1990s and 2000s were plagued by divergence, where the Q-values would oscillate wildly or explode towards infinity. This instability arises from a toxic combination of three properties that are inherent to many RL problems, a combination famously dubbed the <strong>"Deadly Triad"</strong> by Sutton and Barto.
                </p>
                
                <div class="admonition danger">
                    <span class="admonition-title">
                      <i class="fas fa-skull-crossbones"></i>
                      The Deadly Triad of Divergence
                    </span>
                    <p>Training becomes pathologically unstable when these three elements are combined, creating a dangerous feedback loop where errors can be correlated and recursively amplified:</p>
                    <ol>
                        <li><strong>Function Approximation:</strong> Using a powerful, non-linear model (like a neural network) that generalizes across states. This is necessary for large state spaces.</li>
                        <li><strong>Bootstrapping:</strong> Updating an estimate based on other estimates. The Q-learning update rule, $Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s',a')$, is a prime example, as it updates the current Q-value based on the next state's Q-value.</li>
                        <li><strong>Off-Policy Learning:</strong> Learning about an optimal (greedy) policy while following a different, more exploratory policy to gather data (e.g., an $\epsilon$-greedy policy). This is crucial for efficient exploration.</li>
                    </ol>
                    <p>When all three are present, the assumptions of standard supervised learning are violated. The data is not i.i.d. (identically and independently distributed) because it comes from a sequential trajectory, and the target value for the network's loss function is non-stationary because it depends on the network's own changing weights. This creates a "dog chasing its own tail" problem that often leads to divergence.</p>
                </div>

                <div class="interactive-lab">
                    <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Witnessing the Deadly Triad</div>
                    <p>
                        The "Deadly Triad" can seem abstract. This lab provides a concrete, interactive visualization of value function divergence on a simple MDP, Baird's Counterexample. The environment has 7 states, and the agent can choose between two actions: `solid` or `dashed`. The `dashed` action always leads to state 7, while the `solid` action moves between states 1-6. The optimal value function is known, but we will see how a naive Q-learning update with linear function approximation (a simple form of the triad) causes the values to diverge to infinity.
                    </p>
                    <div class="lab-controls">
                        <button id="baird-step-btn" style="padding: 8px 16px;">Run One Update Step</button>
                        <button id="baird-reset-btn" style="padding: 8px 16px;">Reset</button>
                    </div>
                    <div id="baird-plot" style="width:100%; height:350px;"></div>
                    <p id="baird-status" style="text-align: center; font-weight: bold;"></p>
                </div>
                <script>
                document.addEventListener('DOMContentLoaded', () => {
                    const bairdPlotDiv = document.getElementById('baird-plot');
                    if (!bairdPlotDiv) return;

                    let weights = [1, 1, 1, 1, 1, 1, 10, 1];
                    let step = 0;
                    const alpha = 0.01;
                    const gamma = 0.99;
                    
                    // Baird's features (phi)
                    const features = Array(7).fill(0).map(() => Array(8).fill(0));
                    for (let i = 0; i < 6; i++) {
                        features[i][i] = 2;
                        features[i][7] = 1;
                    }
                    features[6][6] = 1;
                    features[6][7] = 2;

                    function dot(v1, v2) { return v1.reduce((acc, val, i) => acc + val * v2[i], 0); }

                    function update_weights() {
                        const deltas = Array(8).fill(0);
                        
                        // Off-policy update: behavior policy is uniform random, target policy is greedy.
                        // For Baird's example, all transitions from s=0..5 lead to s=6 with r=0.
                        for (let s = 0; s < 6; s++) {
                            const v_next = dot(features[6], weights);
                            const target = 0 + gamma * v_next; // Reward is always 0
                            const current_v = dot(features[s], weights);
                            const error = target - current_v;
                            for(let i=0; i<8; i++) {
                                deltas[i] += features[s][i] * error;
                            }
                        }
                        
                        for(let i=0; i<8; i++) {
                            weights[i] += alpha * deltas[i];
                        }
                        step++;
                    }

                    function plot() {
                        const values = features.map(row => dot(row, weights));
                        Plotly.react(bairdPlotDiv, [{
                            x: Array.from({length: 7}, (_, i) => `State ${i+1}`),
                            y: values,
                            type: 'bar',
                            marker: {color: 'var(--color-accent-danger)'}
                        }], {
                            title: `Value Estimates after ${step} Updates`,
                            yaxis: {title: 'Estimated Value V(s)'}
                        }, {responsive: true});
                        document.getElementById('baird-status').textContent = `Max Value: ${Math.max(...values).toExponential(2)}`;
                    }

                    document.getElementById('baird-step-btn').addEventListener('click', () => {
                        update_weights();
                        plot();
                    });
                    document.getElementById('baird-reset-btn').addEventListener('click', () => {
                        weights = [1, 1, 1, 1, 1, 1, 10, 1];
                        step = 0;
                        plot();
                    });
                    
                    // Initial plot
                    plot();
                });
                </script>
            </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-cogs"></i>
            3.2 DQN's Innovations: Taming the Beast
          </summary>
          <div class="details-content">
              <p>
                  In 2013, and more famously in a 2015 Nature paper, researchers at DeepMind introduced the <strong>Deep Q-Network (DQN)</strong> algorithm. It was the first method to successfully train a deep neural network for a control policy that could master a wide range of complex tasks—specifically, Atari 2600 games—learning directly from raw pixel inputs.
              </p>
              <p>
                  DQN's success was not due to a single breakthrough, but rather the clever combination of existing ideas from machine learning and a couple of crucial innovations that directly attacked the instabilities of the Deadly Triad. These two key innovations were <strong>Experience Replay</strong> and the use of a <strong>Fixed Target Network</strong>.
              </p>

              <h4>Innovation 1: Experience Replay</h4>
              <p>
                  The first major problem DQN addressed is that the data generated by an RL agent is not i.i.d. (independent and identically distributed). The samples are generated sequentially, meaning that consecutive states are highly correlated. Training a neural network on such correlated data is extremely inefficient and can lead to catastrophic forgetting and oscillations, as the network's weights get pushed back and forth by similar, sequential inputs.
              </p>
              <p>
                  <strong>Experience Replay</strong>, an idea first explored by Lin in 1992, breaks these temporal correlations by creating a large dataset of past experiences, effectively making the RL problem more like a standard supervised learning problem.
              </p>
              
              <h5>The Mechanism of Experience Replay</h5>
              <ol>
                  <li><strong>Store Experiences:</strong> The agent stores every experience tuple $(s_t, a_t, r_{t+1}, s_{t+1})$ in a large, finite-sized circular buffer called the <strong>replay memory</strong> or replay buffer. This buffer can hold hundreds of thousands or millions of transitions.</li>
                  <li><strong>Sample Randomly:</strong> Instead of training on the most recent experience, the agent samples a random minibatch of transitions from the replay buffer.</li>
                  <li><strong>Train on the Minibatch:</strong> This randomly sampled minibatch is then used to perform a gradient descent update on the Q-network.</li>
              </ol>

              <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                </div>
                <div class="mermaid">
graph LR
    subgraph "Agent-Env Interaction"
        A["Agent"] -- "Action" --> B((Env))
        B -- "s,a,r,s'" --> C[Experience Tuple]
    end

    C -- "Store" --> D[("Replay Buffer")]

    subgraph "Training"
        D -- "Sample Minibatch" --> E[Minibatch]
        E -- "Train" --> F{Q-Network}
    end

    class D buffer-class
    class F network-class
                </div>
              </div>

              <div class="admonition info">
                  <span class="admonition-title">
                    <i class="fas fa-brain"></i>
                    Intuition: Learning from Shuffled Flashcards
                  </span>
                  <p>
                    Imagine you are trying to learn a new language. If you only study vocabulary chapter by chapter (e.g., all animals, then all foods), you will quickly forget the earlier chapters. The data is highly correlated. A much more effective strategy is to create flashcards for all the words and then shuffle the entire deck before each study session.
                  </p>
                  <p>
                    The replay buffer is like this deck of flashcards. By storing experiences from many different parts of the environment and from different time periods, and then training on a random shuffle of them, we get several critical benefits:
                  </p>
                  <ul>
                      <li><strong>Breaking Correlations:</strong> Shuffling decorrelates the data, leading to more stable and efficient training.</li>
                      <li><strong>Data Re-use:</strong> Each experience can be used in many weight updates, dramatically improving sample efficiency. A rare but important experience (like finding a key in a maze) can be replayed over and over.</li>
                      <li><strong>Approximating the State Distribution:</strong> By sampling from a large buffer, the minibatch provides a better approximation of the true underlying state distribution that the optimal policy would induce, leading to more robust updates.</li>
                  </ul>
              </div>

              <h4>Innovation 2: Fixed Target Network</h4>
              <p>
                  The second major source of instability is the "moving target" problem. In the Q-learning update, we are trying to make our network's prediction, $\hat{Q}(s,a;\theta)$, match a target value, $y_t = r + \gamma \max_{a'} \hat{Q}(s',a';\theta)$.
              </p>
              <p>
                  Notice that the parameters $\theta$ appear in both the prediction and the target. This means that with every gradient descent step, the target value itself shifts. This is like trying to hit a moving target that moves every time you adjust your aim. This coupling can lead to harmful feedback loops and oscillations in the Q-values.
              </p>
              <p>
                  DQN solves this by using a second, separate neural network called the <strong>target network</strong>.
              </p>

              <h5>The Mechanism of the Target Network</h5>
              <ol>
                  <li><strong>Two Networks:</strong> We maintain two networks with the exact same architecture: the <strong>online network</strong>, $Q(\cdot;\theta)$, which is updated at every step, and the <strong>target network</strong>, $Q(\cdot;\theta^-)$, which is used to calculate the TD target.</li>
                  <li><strong>Fixed Target:</strong> The target network's weights, $\theta^-$, are held fixed for a large number of steps (e.g., C=10,000). During this time, the TD target, $y_t = r + \gamma \max_{a'} \hat{Q}(s',a';\theta^-)$, is stable.</li>
                  <li><strong>Periodic Updates:</strong> After C steps, the weights of the online network are copied over to the target network: $\theta^- \leftarrow \theta$. Then the target network is frozen again.</li>
              </ol>

              <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                </div>
                <div class="mermaid">
graph TD
    subgraph "Loss Calculation at Step t"
        OnlineNet["Online Network&lt;br&gt;Q(s,a; θ_t)"]
        TargetNet["Target Network&lt;br&gt;Q(s',a'; θ⁻)"]
        
        OnlineNet -- "Prediction" --> Loss((Loss))
        TargetNet -- "Stable Target&lt;br&gt;y_t" --> Loss
    end
    
    subgraph "Weight Updates Over Time"
        OnlineNet_t0["θ_0"] --> OnlineNet_t1["θ_1"] --> OnlineNet_t2["..."] --> OnlineNet_tC["θ_C"]
        TargetNet_t0["θ⁻ = θ_0"] --> TargetNet_t1["θ⁻ = θ_0"] --> TargetNet_t2["..."] --> TargetNet_tC["θ⁻ = θ_0"]
        OnlineNet_tC -- "Copy Weights&lt;br&gt;θ⁻ ← θ_C" --> TargetNet_tC_new["θ⁻ = θ_C"]
    end

    class OnlineNet online-class
    class TargetNet target-class
                </div>
                <p style="text-align:center; font-style:italic;">The online network's weights (θ) are updated at every step via gradient descent. The target network's weights (θ⁻) are frozen, providing a stable target for the loss calculation. They are only updated by copying the online weights periodically.</p>
              </div>
              <p>
                  This simple trick of decoupling the network that generates the target from the network being trained makes the optimization problem much more like standard supervised learning, dramatically improving stability.
              </p>
              <details class="deep-dive">
                  <summary><i class="fas fa-wave-square"></i> Deep Dive: Hard vs. Soft Target Updates</summary>
                  <div class="details-content">
                      <p>
                          The original DQN paper used "hard" target updates, where the weights are copied wholesale every C steps. A popular and often more effective alternative is to use <strong>soft target updates</strong>, also known as Polyak averaging.
                      </p>
                      <p>
                          Instead of a periodic copy, the target network's weights are updated very slowly at every single training step to track the online network:
                      </p>
                      $$ \theta^- \leftarrow \tau\theta + (1-\tau)\theta^- $$
                      <p>
                          Here, $\tau$ is a very small constant (e.g., $\tau=0.005$). This causes the target network to be a slow-moving average of the online network, providing both stability and smoother policy evolution. This technique is now standard in many modern DRL algorithms like DDPG and SAC.
                      </p>
                  </div>
              </details>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-list-ol"></i>
            3.3 The Complete DQN Algorithm: A Code Deep Dive
          </summary>
          <div class="details-content">
              <p>
                  The full DQN algorithm integrates the core Q-learning update with the innovations of experience replay and a fixed target network. The pseudocode below provides a high-level overview, followed by a complete, heavily-commented PyTorch implementation that shows how these concepts translate into a working agent capable of solving a classic control benchmark like CartPole.
              </p>
              
              <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-book-open"></i> DQN Algorithm Pseudocode</span>
                <pre><code>
Initialize replay memory D to capacity N
Initialize online Q-network Q with random weights θ
Initialize target network Q̂ with weights θ⁻ = θ

for episode = 1 to M do:
    Initialize state s₁ from environment
    for t = 1 to T do:
        // 1. ACT using the online network and ε-greedy policy
        With probability ε select a random action aₜ
        otherwise select aₜ = argmaxₐ Q(sₜ, a; θ)

        // 2. INTERACT with the environment
        Execute action aₜ, observe reward rₜ and next state sₜ₊₁
        Store transition (sₜ, aₜ, rₜ, sₜ₊₁) in D

        // 3. LEARN from a minibatch of experiences
        Sample a random minibatch of transitions (sⱼ, aⱼ, rⱼ, sⱼ₊₁) from D
        
        // 4. Calculate the TD target using the fixed target network
        if sⱼ₊₁ is a terminal state:
            yⱼ = rⱼ
        else:
            yⱼ = rⱼ + γ * maxₐ' Q̂(sⱼ₊₁, a'; θ⁻)

        // 5. Perform a gradient descent step on the loss
        // Loss L = (yⱼ - Q(sⱼ, aⱼ; θ))²
        Update online network weights θ to minimize L

        // 6. Periodically update the target network
        Every C steps, set θ⁻ ← θ
    end for
end for
                </code></pre>
              </div>

            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Full DQN Implementation for CartPole</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                        <a href="https://colab.research.google.com/github/pytorch/tutorials/blob/master/intermediate_source/reinforcement_q_learning.py" target="_blank" class="colab-button" title="Run this code in Google Colab">
                           <i class="fas fa-play"></i> Run in Colab
                        </a>
                    </div>
                </div>
                <pre><code class="language-python">
# --- Step 1: Define the Replay Memory ---
# A named tuple is a convenient way to bundle together related values.
# Here, it represents a single transition in our environment.
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class ReplayMemory(object):
    """A cyclic buffer of bounded size that holds the transitions observed recently."""
    def __init__(self, capacity):
        # A deque is a double-ended queue, which is highly efficient for adding and removing items.
        # When the deque is full, adding a new item automatically discards the oldest item.
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        """Save a transition."""
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        """Select a random batch of transitions for training."""
        return random.sample(self.memory, batch_size)

    def __len__(self):
        """Return the current size of the memory."""
        return len(self.memory)

# --- Step 2: Define the Q-Network Architecture ---
class DQN(nn.Module):
    """A simple Multi-Layer Perceptron (MLP) architecture for the Q-Network."""
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    def forward(self, x):
        """
        Defines the forward pass of the network.
        Input: a state tensor (or a batch of state tensors)
        Output: a tensor of Q-values, one for each possible action from that state.
        """
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)

# --- Step 3: Set Hyperparameters and Initialize Components ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
BATCH_SIZE = 128       # Number of transitions sampled from the replay buffer
GAMMA = 0.99         # Discount factor
EPS_START = 0.9      # Starting value of epsilon
EPS_END = 0.05       # Final value of epsilon
EPS_DECAY = 1000     # Controls the rate of exponential decay of epsilon
TAU = 0.005          # The update rate of the target network (for soft updates)
LR = 1e-4            # The learning rate of the AdamW optimizer
TARGET_UPDATE = 10   # How often to update the target network (for hard updates)

# Setup the environment
env = gym.make("CartPole-v1")
n_actions = env.action_space.n
state, _ = env.reset()
n_observations = len(state)

# Initialize the online and target networks
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict()) # Copy weights

# Initialize the optimizer and replay memory
optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = ReplayMemory(10000)

steps_done = 0

# --- Step 4: Define the Action Selection Function ---
def select_action(state):
    """Selects an action using an epsilon-greedy policy."""
    global steps_done
    sample = random.random()
    # Calculate the current epsilon threshold using exponential decay
    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1
    
    if sample > eps_threshold:
        # Exploit: choose the best action according to the policy network
        with torch.no_grad(): # We don't need to track gradients for action selection
            # policy_net(state) returns Q-values for all actions.
            # .max(1) finds the maximum value in each row (we have a batch size of 1).
            # [1] gets the index of that maximum value, which corresponds to the best action.
            # .view(1, 1) reshapes it to the required tensor shape.
            return policy_net(state).max(1)[1].view(1, 1)
    else:
        # Explore: choose a random action
        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)

# --- Step 5: Define the Optimization (Training) Function ---
def optimize_model():
    """Performs a single step of the optimization process."""
    if len(memory) < BATCH_SIZE:
        return # Don't train until we have enough experiences in memory
    
    transitions = memory.sample(BATCH_SIZE)
    batch = Transition(*zip(*transitions))

    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)
    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
    
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    # Compute Q(s_t, a) for the actions that were actually taken
    # policy_net(state_batch) produces Q-values for all actions
    # .gather(1, action_batch) selects the Q-value for the action that was taken at each state
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # Compute V(s_{t+1}) for all next states using the target network
    # We initialize the next state values to 0 for terminal states
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        # We use the target_net to get the Q-values for the next states
        # .max(1)[0] selects the maximum Q-value, which is the estimate of V*(s_{t+1})
        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]
    
    # Compute the TD Target: y_j = r_j + γ * V*(s_{j+1})
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # Compute Huber loss, which is less sensitive to outliers than MSELoss
    criterion = nn.SmoothL1Loss()
    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))

    # Perform the optimization step
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100) # Gradient clipping
    optimizer.step()

# --- Main training loop would go here ---
                </code></pre>
            </div>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-bug"></i>
            3.4 DQN's Achilles' Heel: Maximization Bias
          </summary>
          <div class="details-content">
              <p>
                  Despite its groundbreaking success, the standard DQN algorithm has a subtle but significant flaw: it systematically overestimates the Q-values. This phenomenon, known as <strong>maximization bias</strong>, can lead to suboptimal policies, especially in environments with stochastic rewards.
              </p>

              <h4>The Source of the Overestimation</h4>
              <p>
                  The bias originates from the use of the $\max$ operator in the TD target calculation. The target is calculated as:
              </p>
              $$y_j = r_j + \gamma \max_{a'} Q_{\theta^-}(s_{j+1}, a')$$
              <p>
                  Here, the <em>same</em> Q-network (the target network) is used for two distinct purposes:
              </p>
              <ol>
                  <li><strong>Selecting</strong> the best action for the next state.</li>
                  <li><strong>Evaluating</strong> the value of that action.</li>
              </ol>
              <p>
                  Because the Q-values are just estimates and contain random noise, the $\max$ operator has a tendency to pick an action whose estimated value is high due to positive noise. When we then use that same noisy, overestimated value in our target, we are systematically propagating a positive bias through our learning updates.
              </p>

              <div class="admonition info">
                  <span class="admonition-title"><i class="fas fa-coins"></i>Analogy: The Noisy Coin Appraiser</span>
                  <p>
                      Imagine you are a treasure hunter with a collection of ancient coins. You hire an appraiser to estimate their value. However, this appraiser is a bit noisy: for any given coin, their estimate is the true value plus or minus some random error.
                  </p>
                  <ul>
                      <li>You ask the appraiser to value each of your 10 coins.</li>
                      <li>You then ask them, "Which of these coins is the most valuable?" They will point to the one with the highest <em>estimated</em> value.</li>
                      <li>Finally, you ask, "And what is the value of that coin?" They will tell you that same high estimated value.</li>
                  </ul>
                  <p>
                      The problem is that the coin they selected as "best" might just be a moderately valuable coin that received a large positive random error during appraisal. By asking the same noisy source to both <strong>select</strong> the best and <strong>evaluate</strong> the best, you are systematically picking the one with the highest positive error, leading you to believe your collection is more valuable than it really is. This is exactly what DQN does.
                  </p>
              </div>

              <div class="visualization-container">
                <div id="maximization-bias-plot" style="width:100%; height:400px;"></div>
              </div>
              <script>
                  (function() {
                      const plotDiv = document.getElementById('maximization-bias-plot');
                      if (!plotDiv || typeof Plotly === 'undefined') return;

                      const true_q_values = [1.0, 1.2, 1.5, 1.3]; // True values, Arm 3 is best
                      const actions = ['Action 1', 'Action 2', 'Action 3', 'Action 4'];
                      
                      // Simulate noisy estimates
                      const noisy_estimates_1 = true_q_values.map(q => q + (Math.random() - 0.5) * 1.5);
                      const noisy_estimates_2 = true_q_values.map(q => q + (Math.random() - 0.5) * 1.5);

                      // DQN's estimate
                      const dqn_max_action_idx = noisy_estimates_1.indexOf(Math.max(...noisy_estimates_1));
                      const dqn_value_estimate = noisy_estimates_1[dqn_max_action_idx];

                      // Double DQN's estimate
                      const ddqn_max_action_idx = noisy_estimates_1.indexOf(Math.max(...noisy_estimates_1));
                      const ddqn_value_estimate = noisy_estimates_2[ddqn_max_action_idx]; // Value from the *second* appraiser

                      Plotly.newPlot(plotDiv, [
                          {x: actions, y: true_q_values, name: 'True Q-Values', type: 'bar', marker: {color: 'grey'}},
                          {x: actions, y: noisy_estimates_1, name: 'Noisy Estimates (Appraiser 1)', type: 'bar', marker: {color: 'rgba(31, 119, 180, 0.6)'}},
                          {x: actions, y: noisy_estimates_2, name: 'Noisy Estimates (Appraiser 2)', type: 'bar', marker: {color: 'rgba(255, 127, 14, 0.6)'}}
                      ], {
                          title: 'Visualizing Maximization Bias',
                          barmode: 'group',
                          yaxis: {title: 'Q-Value'}
                      });
                      
                      // Add annotations to explain the bias
                      const annotations = [
                          { x: actions[dqn_max_action_idx], y: dqn_value_estimate, text: `DQN Target: ${dqn_value_estimate.toFixed(2)}<br>(Overestimated!)`, showarrow: true, arrowhead: 4, ax: 0, ay: -40},
                          { x: actions[ddqn_max_action_idx], y: ddqn_value_estimate, text: `Double DQN Target: ${ddqn_value_estimate.toFixed(2)}<br>(More Accurate)`, showarrow: true, arrowhead: 4, ax: 0, ay: 40, bordercolor: '#c7c7c7', borderwidth: 2}
                      ];
                      Plotly.relayout(plotDiv, {annotations: annotations});
                  })();
              </script>
              <p>The visualization above shows how this works. Appraiser 1 (our target network) gives its highest estimate to an action that isn't truly the best, due to random noise. DQN uses this flawed, high estimate as its target. Double DQN, as we will see, would instead ask Appraiser 2 (the online network) for the value of the action selected by Appraiser 1, resulting in a more conservative and accurate, unbiased estimate.</p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-people-arrows"></i>
            3.5 The Solution: Double DQN
          </summary>
          <div class="details-content">
              <p>
                  The solution to maximization bias is elegantly simple: decouple the action <strong>selection</strong> from the action <strong>evaluation</strong>. This is the core idea of <strong>Double Q-learning</strong>, first proposed by Hado van Hasselt in 2010 for the tabular case and later adapted to deep networks in <strong>Double DQN</strong> (van Hasselt et al., 2015).
              </p>
              <p>
                  The solution to our noisy appraiser analogy would be to hire a second, independent appraiser. You would ask Appraiser 1, "Which coin is best?" and then, crucially, you would take that selected coin to Appraiser 2 and ask, "What is the value of <em>this specific coin</em>?" Since the random errors of the two appraisers are independent, the second estimate is now an unbiased assessment of the selected coin's value.
              </p>
              <p>
                  Double DQN does exactly this using the two networks it already has: the online network ($Q_{\theta}$) and the target network ($Q_{\theta^-}$).
              </p>
              
              <h5>The Double DQN Update Rule</h5>
              <p>The standard DQN target is:</p>
              $$y_j^{\text{DQN}} = r_j + \gamma \max_{a'} Q_{\theta^-}(s_{j+1}, a')$$
              <p>The <strong>Double DQN</strong> target modifies this by using the <em>online</em> network to select the best action, and the <em>target</em> network to evaluate that action's value:</p>
              $$y_j^{\text{Double}} = r_j + \gamma Q_{\theta^-}(s_{j+1}, \arg\max_{a'} Q_{\theta}(s_{j+1}, a'))$$
              
              <div class="admonition tip">
                <span class="admonition-title"><i class="fas fa-code"></i>Implementation in Code</span>
                <p>This change is remarkably small in code but has a profound impact on performance. Here is how the `optimize_model` function from our previous implementation would be modified:</p>
                <pre><code class="language-python">
# --- Inside the optimize_model function ---
with torch.no_grad():
    # DQN Target Calculation:
    # next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]
    
    # Double DQN Target Calculation:
    # 1. Select the best action using the ONLINE network
    best_actions_from_online_net = policy_net(non_final_next_states).max(1)[1].unsqueeze(1)
    
    # 2. Evaluate that action's value using the TARGET network
    next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, best_actions_from_online_net).squeeze(1)

# The rest of the function (computing expected_state_action_values and the loss) remains the same.
                </code></pre>
              </div>
              <p>
                  By decoupling selection and evaluation, Double DQN significantly reduces the overestimation bias, leading to more stable training, more accurate value estimates, and often substantially better final policy performance. It has become a standard component in most modern value-based algorithms.
              </p>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-bug"></i>
            3.4 DQN's Achilles' Heel: Maximization Bias
          </summary>
          <div class="details-content">
              <p>
                  Despite its groundbreaking success, the standard DQN algorithm has a subtle but significant flaw: it systematically overestimates the Q-values. This phenomenon, known as <strong>maximization bias</strong>, can lead to suboptimal policies, especially in environments with stochastic rewards where value estimates are inherently noisy.
              </p>

              <h4>The Source of the Overestimation</h4>
              <p>
                  The bias originates from the use of the $\max$ operator in the TD target calculation. The target is calculated as:
              </p>
              $$y_j = r_j + \gamma \max_{a'} Q_{\theta^-}(s_{j+1}, a')$$
              <p>
                  Here, the <em>same</em> set of value estimates (from the target network) is used for two distinct purposes:
              </p>
              <ol>
                  <li><strong>Selecting</strong> the best action for the next state.</li>
                  <li><strong>Evaluating</strong> the value of that selected action.</li>
              </ol>
              <p>
                  Because the Q-values are just estimates and contain random noise, the $\max$ operator has a tendency to pick an action whose estimated value is high partly due to positive noise. When we then use that same noisy, overestimated value in our target, we are systematically propagating a positive bias through our learning updates.
              </p>

              <div class="admonition info">
                  <span class="admonition-title"><i class="fas fa-coins"></i>Analogy: The Noisy Coin Appraiser</span>
                  <p>
                      Imagine you are a treasure hunter with a collection of ancient coins. You hire an appraiser to estimate their value. However, this appraiser is a bit noisy: for any given coin, their estimate is the true value plus or minus some random error.
                  </p>
                  <ul>
                      <li>You ask the appraiser to value each of your 10 coins.</li>
                      <li>You then ask them, "Which of these coins is the most valuable?" They will point to the one with the highest <em>estimated</em> value.</li>
                      <li>Finally, you ask, "And what is the value of that coin?" They will tell you that same high estimated value.</li>
                  </ul>
                  <p>
                      The problem is that the coin they selected as "best" might just be a moderately valuable coin that received a large positive random error during appraisal. By asking the same noisy source to both <strong>select</strong> the best and <strong>evaluate</strong> the best, you are systematically picking the one with the highest positive error, leading you to believe your collection is more valuable than it really is. This is exactly what DQN does.
                  </p>
              </div>

              <div class="interactive-lab">
                  <div class="lab-header"><i class="fas fa-chart-bar"></i> Interactive Demonstration of Maximization Bias</div>
                  <p>
                      This visualization demonstrates how maximization bias occurs. We have four actions with underlying true Q-values. We simulate two independent "appraisers" (e.g., the online and target networks at a given point) by adding random noise to these true values. Use the "Generate New Noise" button to see how different random errors can mislead the standard DQN target calculation. The horizontal lines show the value that would be used as the TD target for the next state's value.
                  </p>
                  <div class="lab-controls">
                      <button id="noise-regen-btn" style="padding: 8px 16px;">Generate New Noise</button>
                  </div>
                  <div id="maximization-bias-plot-interactive" style="width:100%; height:450px;"></div>
                  <div id="bias-legend" style="padding-top: 1rem; border-top: 1px solid var(--color-border); margin-top: 1rem;">
                      <!-- Custom HTML legend will be populated by the script -->
                  </div>
              </div>
              <script>
                  document.addEventListener('DOMContentLoaded', () => {
                      const plotDiv = document.getElementById('maximization-bias-plot-interactive');
                      const legendDiv = document.getElementById('bias-legend');
                      if (!plotDiv) return;

                      const true_q_values = [1.0, 1.2, 1.5, 1.3];
                      const actions = ['Action 1', 'Action 2', 'Action 3 (True Best)', 'Action 4'];

                      function run_bias_demo() {
                          // Simulate two independent sets of noisy estimates
                          const noise1 = Array.from({length: 4}, () => (Math.random() - 0.5) * 1.5);
                          const noise2 = Array.from({length: 4}, () => (Math.random() - 0.5) * 1.5);
                          const estimates1 = true_q_values.map((q, i) => q + noise1[i]);
                          const estimates2 = true_q_values.map((q, i) => q + noise2[i]);

                          // DQN: Selects and evaluates using the same set of estimates (estimates1)
                          const dqn_max_action_idx = estimates1.indexOf(Math.max(...estimates1));
                          const dqn_value_estimate = estimates1[dqn_max_action_idx];

                          // Double DQN: Selects using estimates1, evaluates using estimates2
                          const ddqn_max_action_idx = dqn_max_action_idx; // Selection is the same
                          const ddqn_value_estimate = estimates2[ddqn_max_action_idx];

                          const plotData = [
                              {x: actions, y: true_q_values, name: 'True Q-Values', type: 'bar', marker: {color: 'grey', opacity: 0.7}},
                              {x: actions, y: estimates1, name: 'Estimates for Selection (Net A)', type: 'bar', marker: {color: 'rgba(31, 119, 180, 0.6)'}},
                              {x: actions, y: estimates2, name: 'Estimates for Evaluation (Net B)', type: 'bar', marker: {color: 'rgba(255, 127, 14, 0.6)'}}
                          ];
                          
                          const layout = {
                              title: 'Visualizing Maximization Bias',
                              barmode: 'group',
                              yaxis: {title: 'Q-Value', range: [-1, 4]},
                              legend: {orientation: 'h', y: 1.1, x: 0.5, xanchor: 'center'},
                              shapes: [
                                  { // DQN Target Line
                                      type: 'line',
                                      x0: -0.5, x1: 3.5,
                                      y0: dqn_value_estimate, y1: dqn_value_estimate,
                                      line: { color: '#d62728', width: 4, dash: 'solid' }
                                  },
                                  { // Double DQN Target Line
                                      type: 'line',
                                      x0: -0.5, x1: 3.5,
                                      y0: ddqn_value_estimate, y1: ddqn_value_estimate,
                                      line: { color: '#2ca02c', width: 4, dash: 'dash' }
                                  }
                              ],
                              annotations: [] // Clear previous annotations
                          };

                          Plotly.react(plotDiv, plotData, layout);

                          // Update custom HTML legend
                          legendDiv.innerHTML = `
                              <h4 style="margin-top:0;">Target Calculation Legend</h4>
                              <p style="margin: 0.5rem 0;">
                                  <span style="color: #d62728; font-weight: bold;">━ DQN Target: ${dqn_value_estimate.toFixed(2)}</span><br>
                                  <small>Calculated as $\\max_{a'} Q_A(s', a')$. Selects the highest blue bar and uses its value. Prone to overestimation.</small>
                              </p>
                              <p style="margin: 0.5rem 0;">
                                  <span style="color: #2ca02c; font-weight: bold;">- - Double DQN Target: ${ddqn_value_estimate.toFixed(2)}</span><br>
                                  <small>Calculated as $Q_B(s', \\arg\\max_{a'} Q_A(s', a'))$. Selects the highest blue bar, but uses the value of the corresponding orange bar. Unbiased.</small>
                              </p>
                          `;
                          // Re-render MathJax for the new legend content
                          if (window.MathJax) {
                              MathJax.typesetPromise([legendDiv]);
                          }
                      }

                      document.getElementById('noise-regen-btn').addEventListener('click', run_bias_demo);
                      run_bias_demo(); // Initial run
                  });
              </script>
              <p>
                  As the interactive demonstration shows, the action selected by the first set of estimates (blue bars) is often not the true best action, but rather one whose estimate was inflated by positive noise. Standard DQN uses the value of this selected blue bar as its target (solid red line). Double DQN, however, uses the value of the corresponding orange bar for its target (dashed green line). Since the noise on the blue and orange bars is independent, the green line provides an unbiased estimate, which is, on average, much closer to the true values (grey bars).
              </p>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-people-arrows"></i>
            3.5 The Solution: Decoupling with Double DQN
          </summary>
          <div class="details-content">
              <p>
                  The solution to maximization bias is elegantly simple yet profound: decouple the action <strong>selection</strong> from the action <strong>evaluation</strong>. This is the core idea of <strong>Double Q-learning</strong>, first proposed by Hado van Hasselt in 2010 for the tabular case and later adapted to deep networks in the landmark <strong>Double DQN</strong> paper (van Hasselt et al., 2015).
              </p>
              <p>
                  Returning to our noisy coin appraiser analogy, the solution is to hire a second, independent appraiser. The process becomes:
              </p>
              <ol>
                  <li>Ask Appraiser 1 (the online network): "Based on your estimates, which coin is the most valuable?" Let's say they choose Coin C.</li>
                  <li>Then, and this is the crucial step, take Coin C to Appraiser 2 (the target network) and ask: "What is the value of <em>this specific coin</em>?"</li>
              </ol>
              <p>
                  Because the random errors of the two appraisers are independent, the second estimate is now an unbiased assessment of the selected coin's value. Even if Appraiser 1 chose Coin C because of a large positive error, it's highly unlikely that Appraiser 2 will have the same large positive error for that specific coin. On average, this process will yield a much more accurate and less optimistic estimate of the value of the best action. Double DQN implements exactly this logic using the two networks it already has: the online network ($Q_{\theta}$) and the target network ($Q_{\theta^-}$).
              </p>
              
              <h5>The Double DQN Update Rule: A Tale of Two Networks</h5>
              <p>Let's compare the update targets side-by-side.</p>
              <p>The standard <strong>DQN</strong> target uses only the target network:</p>
              $$y_j^{\text{DQN}} = r_j + \gamma \underbrace{\max_{a'} Q_{\theta^-}(s_{j+1}, a')}_{\text{Select AND Evaluate with Target Net}}$$
              <p>The <strong>Double DQN</strong> target uses both networks in a decoupled fashion:</p>
              $$y_j^{\text{Double}} = r_j + \gamma \underbrace{Q_{\theta^-}(s_{j+1}, \overbrace{\arg\max_{a'} Q_{\theta}(s_{j+1}, a')}^{\text{SELECT with Online Net}})}_{\text{EVALUATE with Target Net}}$$
              
              <div class="admonition tip">
                <span class="admonition-title"><i class="fas fa-code"></i>Implementation in Code: A Small Change with a Big Impact</span>
                <p>This change is remarkably small in code but has a profound impact on performance. Here is how the `optimize_model` function from our previous implementation would be modified. Note the two distinct steps in calculating the next-state values.</p>
                <pre><code class="language-python">
# --- Inside the optimize_model function ---
with torch.no_grad():
    # --- This is the key modification for Double DQN ---
    
    # 1. Select the best action for the next states using the ONLINE network (policy_net)
    # policy_net(non_final_next_states) gives us Q-values for all actions from next states.
    # .max(1)[1] gives us the indices of the best actions.
    # .unsqueeze(1) reshapes it to [batch_size, 1] for the next step.
    best_next_actions = policy_net(non_final_next_states).max(1)[1].unsqueeze(1)
    
    # 2. Evaluate the value of those selected actions using the TARGET network (target_net)
    # We use .gather() to select the Q-values from the target_net corresponding to the best_next_actions.
    # .squeeze(1) removes the extra dimension to match the shape of reward_batch.
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, best_next_actions).squeeze(1)

# The rest of the function (computing the TD target and the loss) remains the same.
# expected_state_action_values = (next_state_values * GAMMA) + reward_batch
# ...
                </code></pre>
              </div>
              <p>
                  By decoupling selection and evaluation, Double DQN significantly reduces the overestimation bias. This leads to more stable training, more accurate value estimates, and often substantially better final policy performance, especially in environments where many actions have similar true values. It has become a standard, default improvement for any Q-learning-based algorithm.
              </p>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-chess-queen"></i>
            3.6 Dueling DQN: Separating State Value from Action Advantage
          </summary>
          <div class="details-content">
            <p>
              While Double DQN addresses a bias in the learning <em>update</em>, another line of innovation focuses on improving the neural network <em>architecture</em> itself. The <strong>Dueling Network Architecture</strong>, introduced by Wang et al. in 2016, is a powerful modification to the standard DQN that leads to better policy evaluation and faster learning, particularly in environments where the values of different actions are often very similar.
            </p>
            <p>
              The core insight is that the value of a state-action pair, $Q(s,a)$, can be decomposed into two separate, conceptually distinct components: the intrinsic value of the state itself, $V(s)$, and the relative advantage of taking a specific action in that state, $A(s,a)$.
            </p>

            <h4>The Core Intuition: State Value vs. Action Advantage</h4>
            <p>
              Let's formalize this decomposition. We can define the <strong>advantage function</strong>, $A^\pi(s, a)$, as the answer to the question: "How much better is it to take action $a$ in state $s$ compared to the average action I would take under my policy $\pi$?"
            </p>
            $$A^\pi(s, a) \doteq Q^\pi(s, a) - V^\pi(s)$$
            <p>
                By rearranging this, we can express the Q-function as a sum of the state value and the action advantage:
            </p>
            $$Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)$$
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-car-side"></i>Analogy: Driving on a Highway</span>
                <p>
                    Imagine you are an RL agent controlling a car on a highway.
                </p>
                <ul>
                    <li><strong>State 1: Open road, no traffic.</strong> In this state, your state value $V(s)$ is very high. It's a good situation to be in. However, the exact action you take (steer slightly left, steer slightly right, maintain course) makes very little difference to your future rewards. Therefore, the advantages $A(s,a)$ for all these actions are close to zero. The network should focus on learning the high value of $V(s)$.</li>
                    <li><strong>State 2: Approaching a collision.</strong> In this state, your state value $V(s)$ is very low. It's a bad situation. However, the choice of action is now critically important. The action "swerve left" might have a large positive advantage, while "continue straight" has a large negative advantage. The network should focus on learning the precise advantage $A(s,a)$ of each action.</li>
                </ul>
                <p>
                    A standard DQN has to learn a separate Q-value for every action in every state. The Dueling architecture allows the network to learn the state value $V(s)$ once, without having to learn the effect of every single action for that state. This is a much more efficient representation.
                </p>
            </div>
            
            <h4>The Two-Stream Architecture</h4>
            <p>
              The Dueling architecture modifies the end of a standard DQN. After the initial convolutional or fully-connected layers that act as a shared feature extractor, the network splits into two separate, fully-connected streams (or "heads"):
            </p>
            <ol>
              <li><strong>The Value Stream</strong>: This stream takes the features from the shared base and outputs a <strong>single scalar value</strong>. This is our estimate of the state-value function, $V(s; \theta, \alpha)$, where $\theta$ are the parameters of the shared layers and $\alpha$ are the parameters of the value stream.</li>
              <li><strong>The Advantage Stream</strong>: This stream also takes the features from the shared base but outputs a <strong>vector with $|A|$ values</strong>. Each output corresponds to the estimated advantage for taking the corresponding action, $A(s, a; \theta, \beta)$, where $\beta$ are the parameters of the advantage stream.</li>
            </ol>
            
            <div class="visualization-container">
                <div class="vis-toolbar">
                    <button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button>
                </div>
                <div class="mermaid">
graph TD
    Input[State Input s] --> ConvBase["Shared Feature Extractor"]
    
    subgraph ValueHead["Value Stream"]
        ConvBase --> ValueStream_FC[FC Layers] --> ValueStream_Out("V(s)")
    end
    
    subgraph AdvantageHead["Advantage Stream"]
        ConvBase --> AdvStream_FC[FC Layers] --> AdvStream_Out("A(s,a)")
    end

    subgraph Aggregation["Aggregation Layer"]
        ValueStream_Out --> Combine((Combine))
        AdvStream_Out --> Combine
    end
    
    Combine --> Output["Final Q(s,a)"]

    class ValueHead value-class
    class AdvantageHead advantage-class
                </div>
            </div>

            <h4>The Aggregation Layer and the Identifiability Problem</h4>
            <p>
              Now we have two outputs, $V(s)$ and a vector of $A(s, a)$ values. How do we combine them to get our final Q-values? The naive approach would be to simply add them together: $Q(s, a) = V(s) + A(s, a)$.
            </p>
            
            <div class="admonition danger">
              <span class="admonition-title">
                <i class="fas fa-exclamation-triangle"></i>
                The Identifiability Problem
              </span>
              <p>
                This naive aggregation has a serious flaw. We are trying to learn two quantities ($V$ and $A$) from a single target signal (the TD target for $Q$). The network can learn to produce the correct Q-values in a way that is meaningless. For example, given an optimal Q-function, the decomposition into V and A is not unique. If $V(s) = 10$ and $A(s,a) = [1, 5]$, the Q-values are $[11, 15]$. But we get the exact same Q-values if we add a constant $c$ to V and subtract it from A: if $V(s) = 12$ and $A(s,a) = [-1, 3]$, the Q-values are still $[11, 15]$. This is the <strong>identifiability problem</strong>, and this ambiguity can make training unstable.
              </p>
            </div>
            
            <p>
              To solve this, we must place a constraint on the advantage function to force a unique solution. The most common and stable solution is to subtract the mean of the advantages from all advantage values. This forces the sum of the advantages for any state to be zero, which uniquely pins down the values for $V(s)$ and $A(s, a)$.
            </p>
            <p>The corrected aggregation layer formula is:</p>
            $$Q(s, a) = V(s) + \left( A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a' \in \mathcal{A}} A(s, a') \right)$$
            <p>
              With this aggregation, $V(s)$ is forced to learn the true state-value function, and $A(s,a)$ is forced to learn the true advantage function. This leads to a more efficient and stable learning process.
            </p>
            
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: Dueling DQN Implementation</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                    </div>
                </div>
                <pre><code class="language-python">
import torch
import torch.nn as nn

class DuelingDQN(nn.Module):
    def __init__(self, state_dim, n_actions):
        super(DuelingDQN, self).__init__()
        
        # Shared feature learning layers
        self.feature_layer = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU()
        )
        
        # Value stream head
        self.value_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1) # Outputs a single scalar V(s)
        )
        
        # Advantage stream head
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions) # Outputs a vector A(s,a) for each action
        )

    def forward(self, x):
        # Pass input through the shared feature layer
        features = self.feature_layer(x)
        
        # Calculate value and advantage streams
        values = self.value_stream(features)
        advantages = self.advantage_stream(features)
        
        # Combine the streams using the mean-subtraction aggregation
        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a')))
        # .mean(dim=1, keepdim=True) calculates the mean advantage across actions for each state in the batch
        q_values = values + (advantages - advantages.mean(dim=1, keepdim=True))
        
        return q_values

# Example usage for CartPole
# state_dim = 4
# n_actions = 2
# model = DuelingDQN(state_dim, n_actions)
                </code></pre>
            </div>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-chart-area"></i>
            3.7 Distributional RL: Beyond a Single Expectation
          </summary>
          <div class="details-content">
              <p>
                  All the value-based methods we have discussed so far—DQN, Double DQN, Dueling DQN—share a common, fundamental limitation: they only model the <strong>expected value</strong> (the mean) of the distribution of future returns. The Q-value, $Q(s,a)$, is a single scalar that represents the average outcome over all possible futures. However, reality is stochastic. The same action in the same state can lead to wildly different outcomes. By collapsing this rich spectrum of possibilities into a single number, we lose a vast amount of information.
              </p>
              <p>
                  <strong>Distributional Reinforcement Learning</strong>, introduced in the seminal 2017 paper "A Distributional Perspective on Reinforcement Learning" by Bellemare, Dabney, and Munos, proposes a fundamental and powerful shift in perspective: instead of learning the <em>expectation</em> of the return, the agent should learn to approximate the full <strong>distribution of returns</strong>. This distribution is called the <strong>value distribution</strong>, denoted by the random variable $Z(s,a)$. The traditional Q-value is simply the expectation of this random variable: $Q(s,a) = \mathbb{E}[Z(s,a)]$.
              </p>

              <div class="admonition info">
                  <span class="admonition-title"><i class="fas fa-car-crash"></i>Analogy: The Self-Driving Car's Dilemma, Revisited</span>
                  <p>
                      Imagine a self-driving car at an intersection. It has two possible actions:
                  </p>
                  <ul>
                      <li><strong>Action A (Go Straight):</strong> 99% of the time, this is very fast (reward +10). 1% of the time, there is a catastrophic collision (reward -1000). The <strong>expected return</strong> is $0.99 \times 10 + 0.01 \times (-1000) = 9.9 - 10 = -0.1$.</li>
                      <li><strong>Action B (Wait):</strong> 100% of the time, this is slow but safe (reward +1). The <strong>expected return</strong> is $+1$.</li>
                  </ul>
                  <p>
                      A traditional Q-learning agent, comparing the expected values (-0.1 vs +1), would correctly choose to wait. However, it is making this choice "blindly." It doesn't understand the underlying risk structure; it just sees that one number is smaller than the other.
                  </p>
                  <p>
                      A distributional agent learns the full picture. For Action A, it learns a bimodal distribution with a large peak at +10 and a tiny but terrifying peak at -1000. For Action B, it learns a single, sharp peak at +1. This provides a much richer, more robust signal. A <strong>risk-averse</strong> agent could use this distributional information to explicitly avoid actions with even a small probability of catastrophic outcomes. For example, it could optimize for the 10th percentile of the return distribution (a measure known as Conditional Value at Risk, or CVaR) instead of the mean, leading to safer and more reliable behavior.
                  </p>
              </div>

              <h4>How to Represent a Distribution: The Categorical Approach (C51)</h4>
              <p>
                  How can a neural network output a full probability distribution instead of a single number? The first and most famous distributional algorithm, C51, uses a <strong>categorical distribution</strong>. The idea is to discretize the possible range of returns into a fixed set of "bins" or "atoms."
              </p>
              <ol>
                  <li><strong>Define a Fixed Support:</strong> We first define a fixed, discrete set of possible return values that the distribution can take. This set is called the support. We choose a minimum value $V_{min}$ and a maximum value $V_{max}$ that we believe will contain most of the returns, and we select $N$ atoms (C51 uses $N=51$) spaced equally within this range. The value of the $i$-th atom is:
                  $$z_i = V_{min} + i \frac{V_{max}-V_{min}}{N-1} \quad \text{for } i \in \{0, 1, \dots, N-1\}$$
                  </li>
                  <li><strong>Network Output as a PMF:</strong> The neural network's final layer is now a softmax layer with $N$ outputs for each action. For a given state-action pair $(s,a)$, it outputs a vector of $N$ probabilities, $\{p_i(s,a)\}_{i=0}^{N-1}$, which forms a Probability Mass Function (PMF) over the atoms. Each $p_i(s,a)$ is the learned probability that the return $Z(s,a)$ will take the value of the $i$-th atom, $z_i$.</li>
              </ol>


              <h4>The Distributional Bellman Update: A Projection</h4>
              <p>
                  The Bellman equation must also be adapted for distributions. The <strong>Distributional Bellman Operator</strong>, $\mathcal{T}^\pi$, maps one value distribution to another. For a transition $(s,a,r,s')$, the new distribution for $Z(s,a)$ is the distribution of the random variable $r + \gamma Z(s', \pi(s'))$.
              </p>
              <p>
                  In the C51 algorithm, this update is performed via a two-step process:
              </p>
              <ol>
                  <li><strong>Compute the Target Distribution's Atoms:</strong> For each atom $z_j$ in the next state's distribution $Z(s',a^*)$ (where $a^*$ is the greedy action), we compute its projected location by applying the Bellman update to the atom itself:
                  $$\hat{\mathcal{T}}z_j = r + \gamma z_j$$
                  This gives us a new set of $N$ target atoms, each with probability $p_j(s',a^*)$.
                  </li>
                  <li><strong>Project onto the Original Support:</strong> This new distribution, $(\hat{\mathcal{T}}z_j, p_j(s',a^*))$, will not have its atoms aligned with our fixed support $\{z_i\}$. We must therefore project it back. For each target atom $\hat{\mathcal{T}}z_j$, we find its two nearest neighbors in the original support, say $z_k$ and $z_{k+1}$, and we distribute its probability mass $p_j(s',a^*)$ between them proportionally to how close it is to each.</li>
              </ol>
              <p>
                  The training objective is then to minimize the Kullback-Leibler (KL) divergence between the network's predicted distribution for $Z(s,a)$ and this projected target distribution. This is equivalent to minimizing the cross-entropy loss and pulls the predicted distribution towards the Bellman target distribution.
              </p>
              
              <div class="visualization-container">
                <div id="c51-projection-plot" style="width:100%; height:400px;"></div>
              </div>
              <script>
                (function() {
                    const plotDiv = document.getElementById('c51-projection-plot');
                    if (!plotDiv || typeof Plotly === 'undefined') return;
                    
                    const Vmin = -10, Vmax = 10, N = 11;
                    const dz = (Vmax - Vmin) / (N - 1);
                    const support = Array.from({length: N}, (_, i) => Vmin + i * dz);

                    // Example next state distribution
                    const next_dist_probs = [0, 0, 0, 0.1, 0.4, 0.4, 0.1, 0, 0, 0, 0];
                    
                    // Bellman update params
                    const r = 3.0, gamma = 0.9;
                    const projected_support = support.map(z => r + gamma * z);

                    // Project one atom for visualization
                    const atom_idx_to_project = 5;
                    const projected_atom = projected_support[atom_idx_to_project];
                    const b = (projected_atom - Vmin) / dz;
                    const l = Math.floor(b);
                    const u = Math.ceil(b);
                    const p_mass = next_dist_probs[atom_idx_to_project];
                    
                    const target_dist_probs = Array(N).fill(0);
                    if (l >= 0 && l < N) target_dist_probs[l] += p_mass * (u - b);
                    if (u >= 0 && u < N) target_dist_probs[u] += p_mass * (b - l);

                    Plotly.newPlot(plotDiv, [
                        {x: support, y: next_dist_probs, name: 'Next State Dist. P(Z(s\'))', type: 'bar', marker: {color: 'rgba(31, 119, 180, 0.7)'}},
                        {x: support, y: target_dist_probs, name: 'Projected Mass for one Atom', type: 'bar', marker: {color: 'rgba(214, 39, 40, 0.9)'}}
                    ], {
                        title: 'C51 Projection Step',
                        xaxis: {title: 'Return Value (Atoms)'},
                        yaxis: {title: 'Probability'},
                        shapes: [{
                            type: 'line', x0: projected_atom, x1: projected_atom, y0: 0, y1: p_mass,
                            line: {color: 'grey', width: 2, dash: 'dash'}, name: 'Projected Atom'
                        }],
                        annotations: [{
                            x: projected_atom, y: p_mass, text: `Projected Atom<br>r+γz_j = ${projected_atom.toFixed(2)}`, showarrow: true, arrowhead: 2
                        }]
                    });
                })();
              </script>
              <p>
                  By learning the full distribution, distributional RL provides a richer training signal, is empirically more stable against noise, and achieves state-of-the-art results on many benchmarks, forming a key component of the famous Rainbow agent.
              </p>
          </div>
        </details>
</section>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale-right"></i>
            3.8 Summary and Comparative Analysis of Value-Based Methods
          </summary>
          <div class="details-content">
            <p>
              This part has traced the evolution of modern value-based deep reinforcement learning, starting from the foundational Deep Q-Network and layering on a series of powerful innovations. Each new technique addresses a specific flaw or limitation in the previous one, leading to more stable, efficient, and powerful agents.
            </p>
            <p>
              Let's summarize the key contributions and trade-offs of the algorithms we have explored.
            </p>

            <h4>An Evolutionary Path of Innovations</h4>
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Foundational Problem"
        A[Tabular Q-Learning] --> B{Large State Spaces?}
    end

    B -- "Yes" --> C[Naive DQN]
    C --> D{Instability?}
    
    subgraph "DQN Innovations"
        D -- "Solution 1" --> E[Experience Replay]
        D -- "Solution 2" --> F[Fixed Target Network]
    end
    
    G[Standard DQN]
    E --> G
    F --> G

    subgraph "Advanced Improvements"
        G --> H{Maximization Bias?}
        H -- "Solution" --> I[Double DQN]
        
        G --> J{Inefficient Representation?}
        J -- "Solution" --> K[Dueling DQN]

        G --> L{Loss of Information?}
        L -- "Solution" --> M[Distributional DQN]
    end

    subgraph "State of the Art"
        I --> Rainbow(Rainbow DQN)
        K --> Rainbow
        M --> Rainbow
    end

    class G primary-method
    class Rainbow advanced-method
                </div>
            </div>

            <h4>Comparative Analysis</h4>
            <table>
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Core Idea</th>
                        <th>Problem Solved</th>
                        <th>Key Mechanism</th>
                        <th>Primary Benefit</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>DQN</strong></td>
                        <td>Apply Deep Learning to Q-Learning</td>
                        <td>Handling large, high-dimensional state spaces (e.g., pixels).</td>
                        <td>Experience Replay & Fixed Target Network.</td>
                        <td><strong>Stability.</strong> Made training deep Q-networks feasible.</td>
                    </tr>
                    <tr>
                        <td><strong>Double DQN</strong></td>
                        <td>Decouple action selection from action evaluation.</td>
                        <td>Systematic overestimation of Q-values (Maximization Bias).</td>
                        <td>Uses the online network to select the best next action and the target network to evaluate it.</td>
                        <td><strong>Accuracy.</strong> Leads to more accurate value estimates and better policies.</td>
                    </tr>
                    <tr>
                        <td><strong>Dueling DQN</strong></td>
                        <td>Decompose Q-value into state value and action advantage.</td>
                        <td>Inefficient learning in states where actions have similar values.</td>
                        <td>A two-stream neural network architecture (V-stream and A-stream).</td>
                        <td><strong>Efficiency.</strong> Learns state values more efficiently without needing to evaluate every action.</td>
                    </tr>
                    <tr>
                        <td><strong>Distributional DQN</strong></td>
                        <td>Learn the full distribution of returns, not just the mean.</td>
                        <td>Loss of information by collapsing stochastic outcomes into a single expected value.</td>
                        <td>Network outputs a categorical distribution over a fixed support of return values.</td>
                        <td><strong>Richness & Stability.</strong> Provides a richer learning signal, is more robust to noise, and enables risk-aware decision making.</td>
                    </tr>
                </tbody>
            </table>

            <h4>When to Use Value-Based Methods</h4>
            <p>
                Value-based methods like DQN and its descendants are particularly well-suited for a specific class of problems:
            </p>
            <ul>
                <li><strong>Discrete Action Spaces:</strong> Their core mechanism relies on a `max` operation over a finite set of actions. This makes them a natural fit for problems like Atari games, board games, or any control problem with a discrete number of choices. Extending them to continuous action spaces is non-trivial and often requires different algorithmic families.</li>
                <li><strong>High Sample Efficiency Needs:</strong> As off-policy methods that utilize an experience replay buffer, they can reuse past experiences multiple times. This makes them significantly more sample-efficient than their on-policy counterparts (like the policy gradient methods we will see in the next part).</li>
                <li><strong>Deterministic or Stochastic Environments:</strong> They perform well in both types of environments. The advanced variants, especially Distributional RL, are particularly robust in highly stochastic settings.</li>
            </ul>
            <p>
                The progression from DQN to Rainbow demonstrates a key theme in deep RL research: identifying a specific source of error or inefficiency and designing a targeted architectural or algorithmic solution to fix it. These components are often modular and can be combined to create increasingly powerful and robust agents.
            </p>
          </div>
        </details>
</section>
      <hr/>
      <section id="part4">
        <h2 id="part4-title">
          <i class="fas fa-bullseye"></i>
          Part 4: Policy-Based and Actor-Critic Methods
        </h2>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-compass"></i>
            4.1 The Other Side of the Coin: From Values to Policies
          </summary>
          <div class="details-content">
            <p>
              In the previous part, we explored <strong>value-based</strong> methods, where the primary goal is to learn an accurate action-value function, $Q^*(s,a)$. The policy is then derived <em>implicitly</em> by always choosing the action with the highest Q-value. This is a powerful and sample-efficient approach, but it has fundamental limitations. We now turn our attention to a fundamentally different and complementary approach: <strong>policy-based methods</strong>.
            </p>
            <p>
              In this paradigm, we don't bother learning a value function first. Instead, we directly parameterize the policy itself, $\pi_\theta(a|s)$, and use optimization techniques to directly find the parameters $\theta$ that result in the best behavior. The agent's "brain" is a function that explicitly maps states to actions (or a distribution over actions).
            </p>
            
            <h4>Why Go Direct? The Critical Limitations of Value-Based Methods</h4>
            <p>
              If value-based methods like DQN are so powerful, why do we need a different approach? Policy-based methods are not just an alternative; they are a <em>necessity</em> for solving entire classes of problems where value-based methods fundamentally struggle or fail.
            </p>
            
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph ValueBased["Value-Based Approach"]
        State_V[State s] --> QNet["Q-Network"]
        QNet --> QValues["[Q(s,a1), Q(s,a2), ...]"]
        QValues --> Greedify["argmax"] --> Action_V[Action a]
    end

    subgraph PolicyBased["Policy-Based Approach"]
        State_P[State s] --> PolicyNet["Policy Network"]
        PolicyNet --> ActionDist["P(a|s)"]
        ActionDist --> Sample["Sample"] --> Action_P[Action a]
    end

    class ValueBased value-based-class
    class PolicyBased policy-based-class
                </div>
                <p style="text-align:center; font-style:italic;">Value-based methods learn values and derive a policy. Policy-based methods learn the policy directly.</p>
            </div>
            
            <h5>Limitation 1: Continuous Action Spaces</h5>
            <p>
                This is the most significant limitation of methods like DQN. The core of their policy is the `argmax` operation: $a = \arg\max_{a'} Q(s, a')$. This requires iterating through every possible action to find the one with the highest value. This is feasible for a small, discrete number of actions (like the 18 joystick commands in Atari), but it completely breaks down for <strong>continuous action spaces</strong>.
            </p>
            <div class="admonition danger">
                <span class="admonition-title"><i class="fas fa-robot"></i>Failure Case: Robotic Control</span>
                <p>
                    Imagine controlling a robotic arm. An action might be a 7-dimensional vector of torques to apply to each joint, where each torque is a real number between -1.0 and 1.0. The action space is infinite. How would a DQN-like agent find the `argmax`? It would have to solve a non-trivial, non-convex optimization problem at every single time step, which is computationally intractable.
                </p>
                <p>
                    A policy-based method handles this naturally. The policy network, $\pi_\theta(s)$, can be designed to directly output the 7-dimensional torque vector. Or, more commonly, it outputs the parameters of a continuous probability distribution (e.g., the mean and standard deviation of a multivariate Gaussian), from which the torque vector is then sampled.
                </p>
            </div>

            <h5>Limitation 2: Stochastic Optimal Policies</h5>
            <p>
                The greedy policy derived from a Q-function, $\arg\max_a Q(s,a)$, is always deterministic (for a given state, it always outputs the same action). While this is fine for many problems, there are environments where the optimal policy is inherently <strong>stochastic</strong>.
            </p>
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-hand-rock"></i>Analogy: Rock-Paper-Scissors</span>
                <p>
                    In rock-paper-scissors, any deterministic policy is easily exploited. If you always play "rock", your opponent will quickly learn to always play "paper". The optimal, unexploitable policy is to play each action with a probability of 1/3. A value-based method cannot represent this policy. A policy-based method, however, can learn a policy network whose softmax output for any state is $[0.33, 0.33, 0.33]$.
                </p>
            </div>

            <h5>Limitation 3: The Problem of Perceptual Aliasing</h5>
            <p>
                In partially observable environments (POMDPs), different true states can appear identical to the agent. This is called perceptual aliasing. In such cases, a deterministic policy can fail catastrophically, while a stochastic policy can be optimal.
            </p>
            
            <h4>The Objective: A Landscape in Policy Space</h4>
            <p>
                The goal of a policy-based method is to find the parameters $\theta$ of a policy $\pi_\theta(a|s)$ that maximize an objective function, $J(\theta)$. This objective function is defined as the expected cumulative return. For episodic tasks, this is typically the value of the starting state:
            </p>
            $$ J(\theta) \doteq \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t r_{t+1} \right] = V^{\pi_\theta}(s_0) $$
            <p>
                Here, $\tau = (s_0, a_0, r_1, s_1, a_1, \dots)$ represents a full trajectory (or episode) sampled by running the policy $\pi_\theta$ in the environment. The expectation $\mathbb{E}_{\tau \sim \pi_\theta}$ is over the distribution of all possible trajectories that could be generated by the policy. This is crucial: the distribution of trajectories we sample depends on the very parameters $\theta$ we are trying to optimize.
            </p>
            <p>
                We can think of $J(\theta)$ as a high-dimensional landscape. The parameters $\theta$ define a point in this landscape, and the height of the landscape at that point is the expected performance of the corresponding policy. Our goal is to find the highest peak in this landscape.
            </p>
            <p>
                The most common way to do this is using <strong>gradient ascent</strong>. We compute the gradient of the objective function with respect to the policy parameters, $\nabla_\theta J(\theta)$, which tells us the steepest direction of ascent, and then take a small step in that direction:
            </p>
            $$ \theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k) $$
            <p>
                The central technical challenge of policy-based methods is figuring out how to compute this gradient. The expectation depends on the policy, which depends on $\theta$, making the derivative non-trivial. The elegant solution to this is a powerful result known as the <strong>Policy Gradient Theorem</strong>, which will be the focus of the next section.
            </p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-calculator"></i>
            4.2 The Policy Gradient Theorem and REINFORCE
          </summary>
          <div class="details-content">
            <p>
              As we established, the goal of policy-based methods is to perform gradient ascent on the objective function $J(\theta)$. The entire field of policy gradients hinges on a single, elegant result that tells us how to compute this gradient: the <strong>Policy Gradient Theorem</strong>. This theorem provides a way to differentiate the expected return with respect to the policy parameters, even though the distribution of trajectories depends on those same parameters.
            </p>

            <h4>The Core Challenge: Differentiating an Expectation</h4>
            <p>
                The objective is $J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [G(\tau)]$. Let's write this out as an integral (or sum for discrete spaces) over the space of all possible trajectories $\mathcal{T}$:
            </p>
            $$ J(\theta) = \int_{\tau \in \mathcal{T}} P(\tau; \theta) G(\tau) d\tau $$
            <p>
                Here, $P(\tau; \theta)$ is the probability of trajectory $\tau$ occurring under policy $\pi_\theta$, and $G(\tau)$ is the total return of that trajectory. When we take the gradient, we have a problem:
            </p>
            $$ \nabla_\theta J(\theta) = \nabla_\theta \int P(\tau; \theta) G(\tau) d\tau = \int \nabla_\theta [P(\tau; \theta)] G(\tau) d\tau $$
            <p>
                The gradient is inside the integral and acts on the probability distribution itself. This is difficult to work with because we usually don't have an analytical form for $P(\tau; \theta)$ and can only sample from it. We need a way to turn this into an expectation that we can approximate with Monte Carlo samples.
            </p>

            <details class="deep-dive">
                <summary><i class="fas fa-book"></i> Deep Dive: The Log-Derivative Trick and the Policy Gradient Theorem</summary>
                <div class="details-content">
                    <p>The key to solving this is a mathematical identity known as the <strong>log-derivative trick</strong>. For any positive, differentiable function $f(x)$, its derivative can be written as:</p>
                    $$ \nabla f(x) = f(x) \frac{\nabla f(x)}{f(x)} = f(x) \nabla \log f(x) $$
                    <p>We can apply this to our gradient calculation. Let's apply it to $\nabla_\theta P(\tau; \theta)$:
                    $$ \nabla_\theta P(\tau; \theta) = P(\tau; \theta) \nabla_\theta \log P(\tau; \theta) $$
                    </p>
                    <p>Now, we substitute this back into our gradient integral:</p>
                    $$ \nabla_\theta J(\theta) = \int P(\tau; \theta) [\nabla_\theta \log P(\tau; \theta)] G(\tau) d\tau $$
                    <p>
                        This is now in the form of an expectation! It's the expected value of the term in the square brackets, under the same distribution $P(\tau; \theta)$.
                    </p>
                    $$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [(\nabla_\theta \log P(\tau; \theta)) G(\tau)] $$
                    <p>
                        The final step is to break down the trajectory probability. The probability of a trajectory is the product of the initial state probability and the probabilities of taking each action and transitioning to each next state.
                    </p>
                    $$ P(\tau; \theta) = p(s_0) \prod_{t=0}^{T-1} \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t) $$
                    <p>
                        Taking the logarithm turns the product into a sum. When we take the gradient $\nabla_\theta$, the terms that don't depend on $\theta$ (the environment dynamics $p(\cdot)$ and the initial state distribution $p(s_0)$) become zero. We are left with only the policy terms:
                    </p>
                    $$ \nabla_\theta \log P(\tau; \theta) = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) $$
                    <p>
                        Substituting this back into our expectation gives the final, practical form of the <strong>Policy Gradient Theorem</strong>:
                    </p>
                    $$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) G(\tau) \right] $$
                    <p>
                        Often, we use a slightly different but equivalent form where the return for each action is the cumulative future return from that point onward, $G_t = \sum_{k=t}^{T-1} \gamma^{k-t} r_{k+1}$. This is justified because an action at time $t$ cannot affect rewards that came before it.
                    </p>
                    $$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T-1} (\nabla_\theta \log \pi_\theta(a_t|s_t)) G_t \right] $$
                </div>
            </details>

            <h4>The REINFORCE Algorithm</h4>
            <p>
                The Policy Gradient Theorem gives us a way to estimate the gradient from experience. The simplest algorithm that uses this is called <strong>REINFORCE</strong>, also known as Monte Carlo Policy Gradient. The algorithm is straightforward:
            </p>
            <ol>
                <li>Initialize the policy network $\pi_\theta$ with random parameters $\theta$.</li>
                <li><strong>Collect Data:</strong> Run one full episode using the current policy $\pi_\theta$. Store the entire trajectory of states, actions, and rewards.</li>
                <li><strong>Calculate Returns:</strong> For each time step $t$ in the episode, calculate the discounted future return $G_t$.</li>
                <li><strong>Estimate Gradient and Update:</strong> Use the collected trajectory as a single sample to estimate the gradient and update the policy parameters. The update rule for a single trajectory is:
                    $$ \theta \leftarrow \theta + \alpha \sum_{t=0}^{T-1} G_t \nabla_\theta \log \pi_\theta(a_t|s_t) $$
                </li>
                <li>Repeat from step 2.</li>
            </ol>
            
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-bullseye"></i>Analogy: The Blind Archer</span>
                <p>
                    Imagine a blindfolded archer trying to hit a target. The archer's parameters, $\theta$, are their stance, grip, and release angle.
                </p>
                <ol>
                    <li><strong>Action:</strong> The archer shoots an arrow (takes an action $a_t$).</li>
                    <li><strong>Trajectory:</strong> The arrow follows a path and lands somewhere (a trajectory $\tau$).</li>
                    <li><strong>Reward:</strong> A spotter yells out a score based on how close the arrow was to the bullseye (the return $G_t$). A high score is a "good" outcome.</li>
                    <li><strong>Learning:</strong> How does the archer improve? The REINFORCE update tells them:
                        <ul>
                            <li>$\log \pi_\theta(a_t|s_t)$: This term represents the action itself.</li>
                            <li>$\nabla_\theta$: This asks, "How should I change my parameters $\theta$ to make that action more likely?"</li>
                            <li>$G_t$: This is the "credit." If the score was high (a good outcome), the archer wants to make the action they just took <em>more probable</em>. If the score was low, they want to make it <em>less probable</em>.</li>
                        </ul>
                    </li>
                </ol>
                <p>
                    So, the update rule simply means: "Increase the probability of actions that led to high returns, and decrease the probability of actions that led to low returns." It is a beautifully simple and intuitive form of trial-and-error learning.
                </p>
            </div>
            
            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: REINFORCE Implementation</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                    </div>
                </div>
                <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    def __init__(self, n_observations, n_actions):
        super(PolicyNetwork, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(n_observations, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions),
            nn.Softmax(dim=-1) # Outputs a probability distribution
        )

    def forward(self, x):
        return self.net(x)

def run_reinforce(env, num_episodes=1000):
    policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)
    optimizer = optim.Adam(policy_net.parameters(), lr=1e-2)
    
    for i_episode in range(num_episodes):
        # 1. Collect Data (one full episode)
        saved_log_probs = []
        rewards = []
        state, _ = env.reset()
        done = False
        while not done:
            state_tensor = torch.from_numpy(state).float().unsqueeze(0)
            
            # Get action probabilities from the policy network
            probs = policy_net(state_tensor)
            
            # Create a categorical distribution and sample an action
            m = Categorical(probs)
            action = m.sample()
            
            # Store the log probability of the action taken
            saved_log_probs.append(m.log_prob(action))
            
            # Interact with the environment
            state, reward, done, _, _ = env.step(action.item())
            rewards.append(reward)

        # 2. Calculate Returns
        returns = []
        R = 0
        # Iterate backwards through the rewards to calculate discounted returns
        for r in reversed(rewards):
            R = r + GAMMA * R
            returns.insert(0, R)
        returns = torch.tensor(returns)
        # Normalize returns for more stable training
        returns = (returns - returns.mean()) / (returns.std() + 1e-9)

        # 3. Estimate Gradient and Update
        policy_loss = []
        for log_prob, R in zip(saved_log_probs, returns):
            # The loss is -log_prob * R. We want to maximize the objective,
            # so we minimize the negative of the objective.
            policy_loss.append(-log_prob * R)
        
        optimizer.zero_grad()
        # Sum the losses for all time steps and perform backpropagation
        policy_loss = torch.cat(policy_loss).sum()
        policy_loss.backward()
        optimizer.step()
        
        # Logging, etc.
    return policy_net
                </code></pre>
            </div>
            
            <h4>The Achilles' Heel of REINFORCE: High Variance</h4>
            <p>
                While REINFORCE is theoretically sound, it suffers from a major practical problem: <strong>high variance</strong>. The gradient estimate is based on a single, full trajectory. A trajectory's return can vary dramatically based on the stochasticity of the policy and the environment. This means the gradient estimate is extremely noisy. As a result, training can be very slow and unstable, requiring many samples to average out the noise.
            </p>
            <p>
                The next sections will be dedicated to the evolution of algorithms that address this high variance problem, starting with the introduction of baselines and culminating in modern Actor-Critic methods.
            </p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas-random"></i>
            4.3 Taming the Gradient: Variance Reduction and Baselines
          </summary>
          <div class="details-content">
            <p>
              The REINFORCE algorithm provides an unbiased estimate of the policy gradient, but its practical utility is severely limited by <strong>high variance</strong>. The gradient estimate at each step depends on the total return of a single sampled trajectory, which can be wildly different from one episode to the next due to stochasticity. This noisy gradient signal makes the learning process unstable and extremely slow, like trying to find the peak of a mountain in a thick, swirling fog.
            </p>
            <p>
              The solution is to reshape the learning signal to be more informative. We need to change the question from "Was this action part of a good trajectory?" to the much more precise question, "Was this action <em>better than expected</em> for this state?" This is achieved by introducing a <strong>baseline</strong>.
            </p>

            <h4>The Intuition of a Baseline</h4>
            <p>
              Imagine a scenario where every possible trajectory in an environment gives a very high reward, say between 1000 and 1010.
            </p>
            <ul>
                <li>Using REINFORCE, every action taken will be associated with a large positive return ($G_t > 1000$). The algorithm will try to increase the probability of <em>all</em> actions it takes, even the ones that led to a return of 1000 (a "bad" outcome relative to the best possible 1010). It struggles to distinguish between "good" and "great" actions.</li>
            </ul>
            <p>
              Now, let's introduce a baseline. Suppose we know that the <em>average</em> return from any given state is around 1005. This is our baseline, $b(s_t)$. We can now modify our update rule to use the return <em>relative to this baseline</em>.
            </p>
            <ul>
                <li>If an action leads to a trajectory with a return of 1010, the learning signal is $1010 - 1005 = +5$. The policy will strongly increase the probability of this action.</li>
                <li>If an action leads to a trajectory with a return of 1000, the learning signal is $1000 - 1005 = -5$. The policy will now <em>decrease</em> the probability of this action, as it was worse than expected.</li>
            </ul>
            <p>
              By subtracting a baseline, we center the learning signal around zero. Actions that are better than average get reinforced, and actions that are worse than average get suppressed. This makes the gradient much more informative and dramatically reduces its variance.
            </p>

            <h4>The Advantage Function</h4>
            <p>
              This new learning signal, "Return - Baseline," has a formal name: the <strong>Advantage Function</strong>. The most natural choice for a state-dependent baseline is the state-value function itself, $V^\pi(s_t)$.
            </p>
            $$ A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t) $$
            <p>
              Since $Q^\pi(s_t, a_t) = \mathbb{E}[G_t | S_t=s_t, A_t=a_t]$, we can use the sampled return $G_t$ as a noisy estimate of the Q-value. This gives us an estimate of the advantage:
            </p>
            $$ \hat{A}_t = G_t - V^\pi(s_t) $$
            <p>The policy gradient update now becomes:</p>
            $$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T-1} (\nabla_\theta \log \pi_\theta(a_t|s_t)) (G_t - V^\pi(s_t)) \right] $$

            <details class="deep-dive">
                <summary><i class="fas fa-check-double"></i> Rigorous Proof: Why Baselines Don't Introduce Bias</summary>
                <div class="details-content">
                    <p>
                        It is crucial that our variance reduction technique does not introduce any bias into the gradient estimate. If it did, we would be optimizing for the wrong objective. We can prove that subtracting a state-dependent baseline $b(s_t)$ does not change the expected value of the gradient.
                    </p>
                    <p>
                        We need to show that the expected value of the term we are subtracting is zero:
                    </p>
                    $$ \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(a_t|s_t) \cdot b(s_t)] \stackrel{?}{=} 0 $$
                    <p>Let's expand the expectation:</p>
                    $$ \sum_{s} d^\pi(s) \sum_{a} \pi_\theta(a|s) [\nabla_\theta \log \pi_\theta(a|s)] b(s) $$
                    <p>Using the log-derivative trick, $\nabla_\theta \pi_\theta(a|s) = \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)$, we can rewrite the term in the brackets:</p>
                    $$ \sum_{s} d^\pi(s) \sum_{a} [\nabla_\theta \pi_\theta(a|s)] b(s) $$
                    <p>Since the baseline $b(s)$ does not depend on the action $a$, we can move it out of the inner sum:</p>
                    $$ \sum_{s} d^\pi(s) b(s) \sum_{a} \nabla_\theta \pi_\theta(a|s) $$
                    <p>We can swap the sum and the gradient:</p>
                    $$ \sum_{s} d^\pi(s) b(s) \nabla_\theta \sum_{a} \pi_\theta(a|s) $$
                    <p>
                        The sum of probabilities of all actions from a state must equal 1: $\sum_{a} \pi_\theta(a|s) = 1$. The gradient of a constant is zero: $\nabla_\theta 1 = 0$. Therefore, the entire expression is zero.
                    </p>
                    $$ \sum_{s} d^\pi(s) b(s) \cdot 0 = 0 $$
                    <p>
                        This proves that subtracting any baseline that depends only on the state $s$ does not change the expected gradient. It is an unbiased transformation.
                    </p>
                </div>
            </details>
            
            <h4>From REINFORCE with Baseline to Actor-Critic</h4>
            <p>
                This leads to a natural question: we don't know the true value function $V^\pi(s_t)$, so what can we use as a baseline? The answer is to <em>learn</em> it.
            </p>
            <p>
                This insight gives rise to the <strong>Actor-Critic</strong> architecture. We maintain two separate neural networks:
            </p>
            <ol>
                <li><strong>The Actor:</strong> A policy network, $\pi_\theta(a|s)$, that learns how to act. It takes a state and outputs action probabilities. This is the same as in REINFORCE.</li>
                <li><strong>The Critic:</strong> A value network, $\hat{V}_w(s)$, that learns how to estimate the state-value function. It takes a state and outputs a single number—its estimated value. Its job is to provide the baseline for the Actor.</li>
            </ol>
            <p>
                During training, these two networks work together. The Actor takes an action. The Critic observes the outcome and computes a TD error, which is a low-variance estimate of the advantage:
            </p>
            $$ \delta_t = R_{t+1} + \gamma \hat{V}_w(S_{t+1}) - \hat{V}_w(S_t) $$
            <p>
                This TD error is then used to update both networks:
            </p>
            <ul>
                <li><strong>The Actor is updated</strong> using the TD error as the advantage signal. It adjusts its parameters $\theta$ to make actions that led to a positive TD error more likely.</li>
                <li><strong>The Critic is updated</strong> using the TD error as its loss function. It adjusts its parameters $w$ to minimize this error, making its value estimates more accurate.</li>
            </ul>
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph Environment
        S_t[State S_t]
        R_t1["Reward R_{t+1}"]
        S_t1["Next State S_{t+1}"]
    end
    
    subgraph Agent
        Actor["Actor"]
        Critic["Critic"]
    end

    S_t --> Actor
    Actor --> A_t[(Action A_t)] 
    
    S_t --> Critic
    S_t1 --> Critic
    R_t1 --> Critic
    
    Critic -->|"TD Error"| Actor 
    Critic -->|"Value Loss"| Critic

    class Actor actor-class
    class Critic critic-class
                </div>
                <p style="text-align:center; font-style:italic;">The Actor-Critic loop. The Actor decides, the Critic evaluates, and the Critic's evaluation (the TD error) is used to train both components.</p>
            </div>
            <p>
                This symbiotic relationship, where the critic learns to provide better feedback and the actor learns to produce better actions based on that feedback, is the foundation of modern policy gradient methods. It is far more stable and sample-efficient than the vanilla REINFORCE algorithm.
            </p>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-users-cog"></i>
            4.4 Modern Actor-Critic: A2C and A3C
          </summary>
          <div class="details-content">
            <p>
              The Actor-Critic framework provides a powerful template for designing policy gradient algorithms. In 2016, researchers at DeepMind introduced a highly successful and influential implementation of this framework called the <strong>Asynchronous Advantage Actor-Critic (A3C)</strong>. A3C leverages modern multi-core CPUs to stabilize and accelerate training by running multiple agents in parallel, offering a compelling alternative to the experience replay buffer used in DQN.
            </p>

            <h4>The Core Idea: Parallelism as a Substitute for Experience Replay</h4>
            <p>
              Recall that one of the key innovations of DQN was the experience replay buffer, which was designed to break the temporal correlations in the agent's experience. Training on sequential, highly correlated samples is a major source of instability for deep neural networks. A3C achieves the same goal of decorrelation, but through a different and powerful mechanism: <strong>asynchronous parallel actors</strong>.
            </p>
            <p>The A3C setup consists of:</p>
            <ul>
                <li><strong>One Global Network:</strong> A single, master network that contains the shared parameters for both the actor ($\theta$) and the critic ($w$). This network lives in a central parameter server.</li>
                <li><strong>Multiple Worker Agents:</strong> A number of independent worker agents (typically one per CPU core). Each worker has its own copy of the environment and its own local copy of the actor-critic network parameters.</li>
            </ul>
            
            <h5>The Asynchronous Update Process: A Symphony of Independent Learners</h5>
            <ol>
                <li><strong>Sync Local with Global:</strong> At the beginning of an episode (or a segment of an episode), each worker pulls the latest parameters from the global network to update its local network.</li>
                <li><strong>Collect a Trajectory Segment:</strong> Each worker then interacts with its own copy of the environment for a small number of steps (e.g., $t_{max}=5$ or until a terminal state is reached), collecting a short trajectory of experiences $(s_t, a_t, r_{t+1}, s_{t+1}, \dots)$. Because each worker is in a different environment instance and is following a slightly different exploration path (due to the stochasticity of the policy and environment), their collected experiences are diverse and largely independent of each other.</li>
                <li><strong>Calculate Local Gradients:</strong> At the end of its short trajectory, each worker computes the policy loss (for the actor) and the value loss (for the critic) over its collected segment. It then calculates the gradients of these losses with respect to its local parameters.</li>
                <li><strong>Asynchronous Update:</strong> The worker sends these gradients up to the global network. The global network immediately uses these gradients to update its parameters (e.g., using a shared Adam optimizer). The worker does this without waiting for, or coordinating with, any of the other workers.</li>
                <li><strong>Repeat:</strong> The worker then repeats the process, pulling the new global parameters and starting a new trajectory segment.</li>
            </ol>
            
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
          graph TD
    Global["Global Network"]
    
    subgraph "CPU Core 1"
        W1["Worker 1"] -- "Interacts with" --> E1((Env 1))
        E1 -- "Generates Trajectory" --> C1["Compute Gradients"]
    end
    subgraph "CPU Core 2"
        W2["Worker 2"] -- "Interacts with" --> E2((Env 2))
        E2 -- "Generates Trajectory" --> C2["Compute Gradients"]
    end
    
    C1 -- "Push Gradients" --> Global
    C2 -- "Push Gradients" --> Global
    
    Global -- "Pull Parameters" --> W1
    Global -- "Pull Parameters" --> W2

    class Global global-class
                </div>
                <p style="text-align:center; font-style:italic;">The A3C architecture. Multiple workers explore their own environments in parallel and send asynchronous gradient updates to a central global network. This parallel experience collection serves to decorrelate the data used for training.</p>
            </div>
            
            <p>
                This parallelization is highly effective. Since the different workers are exploring different parts of the state space at the same time, the aggregated stream of gradient updates sent to the global network is much less correlated than the updates from a single agent. This inherent decorrelation is so powerful that A3C does not require an experience replay buffer, making it a fully <strong>on-policy</strong> algorithm that can learn from its most recent experiences.
            </p>

            <h5>A2C: The Simpler, Synchronous Counterpart</h5>
            <p>
                While A3C was a breakthrough, it was later found that the asynchronicity, while clever, was not the most critical component of its success. A simpler, synchronous version called <strong>Advantage Actor-Critic (A2C)</strong> often performs just as well or even better.
            </p>
            <p>
                In A2C, a central controller waits for all workers to finish their segment of experience collection. It then aggregates all their experiences into a single large batch, computes a single, high-quality gradient from this batch, and applies one update to the global network. All workers are then synchronized with the new parameters before starting the next round of data collection. A2C is often easier to implement, debug, and can make better use of modern hardware like GPUs, which thrive on large, parallelizable batches of data.
            </p>

            <details class="deep-dive">
                <summary><i class="fas fa-file-code"></i> Deep Dive: The A3C Loss Functions</summary>
                <div class="details-content">
                    <p>The update for A3C consists of three components:</p>
                    <ol>
                        <li><strong>Policy Loss (Actor):</strong> This is the standard policy gradient loss, using the advantage estimate as the learning signal. The goal is to maximize this, so we take the gradient ascent step (or minimize its negative).
                        $$ \mathcal{L}_{\pi} = -\log \pi_\theta(a_t|s_t) \hat{A}_t $$
                        </li>
                        <li><strong>Value Loss (Critic):</strong> This is a standard regression loss (e.g., L2 loss) that trains the critic to produce more accurate value estimates. The target is the Monte Carlo return $G_t$.
                        $$ \mathcal{L}_{V} = (G_t - V_w(s_t))^2 $$
                        </li>
                        <li><strong>Entropy Bonus (Exploration):</strong> To encourage exploration and prevent premature convergence to a deterministic policy, A3C adds an entropy bonus to the objective.
                        $$ H(\pi_\theta(s_t)) = - \sum_{a} \pi_\theta(a|s_t) \log \pi_\theta(a|s_t) $$
                        </li>
                    </ol>
                    <p>The final loss that is differentiated is a weighted sum of these components:</p>
                    $$ \mathcal{L} = \mathcal{L}_{\pi} + c_1 \mathcal{L}_{V} - c_2 H(\pi_\theta(s_t)) $$
                    <p>Where $c_1$ (e.g., 0.5) and $c_2$ (e.g., 0.01) are hyperparameters that balance the importance of the value function accuracy and the entropy bonus.</p>
                </div>
            </details>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale-left"></i>
            4.5 Generalized Advantage Estimation (GAE)
          </summary>
          <div class="details-content">
            <p>
              A crucial component of modern actor-critic methods like A2C and A3C is how they estimate the advantage function, $\hat{A}(s_t, a_t)$. The quality of this estimate is paramount; a good advantage estimate provides a low-variance, informative signal that tells the actor how to improve, while a poor estimate can lead to slow or unstable learning.
            </p>
            
            <h4>The Spectrum of Advantage Estimators: A Bias-Variance Trade-off</h4>
            <p>
              There is a spectrum of possible estimators for the advantage function, each with a different trade-off between bias and variance.
            </p>
            
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph LR
    A[High Bias&lt;br&gt;Low Variance] &lt;--> B[Low Bias&lt;br&gt;High Variance]
    subgraph "Estimator Spectrum"
        TD["TD(0) Error"] --- NStep["n-Step Return"] --- MC["Monte Carlo"]
    end
    A --> TD
    B --> MC
                </div>
            </div>

            <ul>
                <li>
                    <strong>The TD(0) Estimator (High Bias, Low Variance):</strong>
                    <p>The simplest estimator is the one-step TD error. It uses the immediate reward and the critic's value estimate of the very next state.</p>
                    $$ \hat{A}(s_t, a_t) = \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t) $$
                    <p>
                        This estimate has <strong>low variance</strong> because it only depends on a single real reward, $r_{t+1}$, and the rest is determined by the critic network. However, it can be <strong>highly biased</strong>, especially early in training when the critic's value estimates, $V(s_{t+1})$, are inaccurate. It trusts the critic's one-step prediction almost completely.
                    </p>
                </li>
                <li>
                    <strong>The Monte Carlo Estimator (Low Bias, High Variance):</strong>
                    <p>At the other extreme, we can use the full Monte Carlo return from the episode, $G_t = \sum_{k=t}^{T-1} \gamma^{k-t} r_{k+1}$.</p>
                    $$ \hat{A}(s_t, a_t) = G_t - V(s_t) $$
                    <p>
                        This estimate is <strong>unbiased</strong> because it uses the full sequence of actual rewards received. It doesn't rely on any of the critic's potentially wrong value estimates (except for the baseline at $s_t$). However, it suffers from <strong>extremely high variance</strong> because the sum of many stochastic rewards can vary dramatically from one trajectory to another.
                    </p>
                </li>
            </ul>
            
            <p>
              Neither of these extremes is ideal. We want an estimator that can flexibly interpolate between them. This is precisely what <strong>Generalized Advantage Estimation (GAE)</strong> provides. Introduced by Schulman et al. in the influential paper "High-Dimensional Continuous Control Using Generalized Advantage Estimation" (2015), GAE offers a principled way to balance this trade-off.
            </p>

            <h5>The GAE Formula: A Weighted Average of TD Errors</h5>
            <p>GAE introduces a parameter, $\lambda \in [0, 1]$, to control this bias-variance trade-off. The GAE estimator is defined as the exponentially-weighted average of n-step TD errors:</p>
            $$ \hat{A}^{\text{GAE}(\gamma, \lambda)}_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l} $$
            <p>where $\delta_{t+l} = r_{t+l+1} + \gamma V(s_{t+l+1}) - V(s_{t+l})$ is the TD error at step $t+l$.</p>

            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-sliders-h"></i>The Role of Lambda ($\lambda$): The Bias-Variance Knob</span>
                <ul>
                    <li><strong>If $\lambda = 0$:</strong> The sum collapses to only the first term, $(\gamma\lambda)^0 \delta_t = \delta_t$. This is the one-step TD error, which is <strong>low-variance but potentially high-bias</strong>. It gives full credit to the critic's next-step value estimate.</li>
                    <li><strong>If $\lambda = 1$:</strong> The GAE estimator becomes equivalent to the unbiased but <strong>high-variance</strong> Monte Carlo estimate, $G_t - V(s_t)$. It gives no credit to the critic's intermediate value estimates and relies only on the full sum of actual rewards.</li>
                    <li><strong>For $0 < \lambda < 1$:</strong> We get a blend of the two. A value like $\lambda=0.95$ is common. This heavily weights the immediate TD errors but also incorporates decaying information from further into the future. This "credit tapering" effectively reduces variance while keeping bias in check. $\lambda$ acts as a knob to control how much we trust our critic's value estimates versus the actual sampled rewards.</li>
                </ul>
            </div>
            
            <details class="deep-dive">
                <summary><i class="fas fa-calculator"></i> Deep Dive: The Recursive Form of GAE</summary>
                <div class="details-content">
                    <p>The infinite sum in the GAE formula can be calculated efficiently using a recursive relationship, which is ideal for implementation. We can expand the sum:</p>
                    $$ \hat{A}^{\text{GAE}}_t = \delta_t + \gamma\lambda \delta_{t+1} + (\gamma\lambda)^2 \delta_{t+2} + \dots $$
                    $$ \hat{A}^{\text{GAE}}_t = \delta_t + \gamma\lambda (\delta_{t+1} + \gamma\lambda \delta_{t+2} + \dots) $$
                    <p>We can see that the term in the parentheses is simply the GAE estimate for the next time step, $\hat{A}^{\text{GAE}}_{t+1}$. This gives us the recursive formula:</p>
                    $$ \hat{A}^{\text{GAE}}_t = \delta_t + \gamma\lambda \hat{A}^{\text{GAE}}_{t+1} $$
                    <p>
                        To compute the advantages for a trajectory, we start from the end of the trajectory (where $\hat{A}^{\text{GAE}}_T = 0$) and iterate backward in time, applying this simple update at each step.
                    </p>
                    <pre><code class="language-python">
def compute_gae_advantages(rewards, values, gamma, lam):
    """
    Computes the GAE for a trajectory segment.
    
    Args:
        rewards: A list of rewards for the trajectory [r_1, ..., r_T].
        values: A list of value estimates for each state [V(s_0), ..., V(s_T)]. Note: len(values) = len(rewards) + 1.
        gamma: The discount factor.
        lam: The lambda parameter for GAE.
        
    Returns:
        A list of advantage estimates for each time step.
    """
    T = len(rewards)
    advantages = [0] * T
    gae = 0
    
    # Iterate backwards from the last step
    for t in reversed(range(T)):
        # The value of the next state is V(s_{t+1}), which is at index t+1 in the values list.
        # The value of the current state is V(s_t), which is at index t.
        delta = rewards[t] + gamma * values[t+1] - values[t]
        
        # Apply the recursive GAE formula
        gae = delta + gamma * lam * gae
        
        advantages[t] = gae
        
    return advantages
                    </code></pre>
                </div>
            </details>

            <div class="interactive-lab">
                <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: The GAE Lambda Parameter</div>
                <p>
                    This lab visualizes the effect of the $\lambda$ parameter on the advantage estimates for a sample trajectory. The trajectory has a "surprise" positive reward near the end and a "surprise" negative reward near the beginning. Observe how different values of $\lambda$ propagate the credit (or blame) for these surprises backward in time.
                </p>
                <div class="lab-controls">
                    <div class="lab-control">
                        <label for="gae-lambda-slider">Lambda (λ): <span id="gae-lambda-value">0.95</span></label>
                        <input type="range" id="gae-lambda-slider" min="0" max="1" step="0.01" value="0.95">
                    </div>
                </div>
                <div id="gae-plot" style="width:100%; height:350px;"></div>
            </div>
            <script>
            document.addEventListener('DOMContentLoaded', () => {
                const gaePlotDiv = document.getElementById('gae-plot');
                if (!gaePlotDiv) return;

                const rewards = [-0.1, -5.0, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, 5.0];
                const values =  [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]; // V(s_10) = 0
                const gamma = 0.99;

                function compute_gae(lam) {
                    const T = rewards.length;
                    const advantages = Array(T).fill(0);
                    let gae = 0.0;
                    for (let t = T - 1; t >= 0; t--) {
                        const delta = rewards[t] + gamma * values[t+1] - values[t];
                        gae = delta + gamma * lam * gae;
                        advantages[t] = gae;
                    }
                    return advantages;
                }

                function plot_gae() {
                    const lambda = parseFloat(document.getElementById('gae-lambda-slider').value);
                    document.getElementById('gae-lambda-value').textContent = lambda.toFixed(2);
                    const advantages = compute_gae(lambda);
                    
                    Plotly.react(gaePlotDiv, [{
                        x: Array.from({length: 10}, (_, i) => `Step ${i}`),
                        y: advantages,
                        type: 'bar',
                        marker: {
                            color: advantages.map(v => v > 0 ? 'var(--color-accent-success)' : 'var(--color-accent-danger)')
                        }
                    }], {
                        title: 'Generalized Advantage Estimates (GAE)',
                        yaxis: {title: 'Advantage A(s,a)'},
                        xaxis: {title: 'Time Step in Trajectory'}
                    }, {responsive: true});
                }

                document.getElementById('gae-lambda-slider').addEventListener('input', plot_gae);
                plot_gae();
            });
            </script>
            <p>
                Notice how with $\lambda=0$, only the actions immediately preceding the rewards get non-zero advantage. As you increase $\lambda$ towards 1, the credit for the final positive reward and the blame for the early negative reward are propagated further and more smoothly back to earlier actions. This more effective credit assignment is a key reason for the stability and performance of modern policy gradient algorithms.
            </p>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-robot"></i>
            4.6 Advanced Algorithms for Continuous Control
          </summary>
          <div class="details-content">
            <p>
              While A2C/A3C provide a solid foundation, several advanced actor-critic algorithms have been developed that offer significant improvements in stability, sample efficiency, and performance, especially in the challenging domain of <strong>continuous control</strong> (e.g., robotics). This section explores the evolution of these state-of-the-art methods, from off-policy approaches like DDPG and TD3 to on-policy trust-region methods like PPO, and finally to the entropy-maximizing SAC.
            </p>

            <h4>DDPG: DQN for Continuous Actions</h4>
            <p>
              The <strong>Deep Deterministic Policy Gradient (DDPG)</strong> algorithm, introduced by Lillicrap et al. in 2015, can be thought of as "DQN for continuous actions." It successfully combines the ideas from DQN (experience replay, target networks) with an actor-critic architecture suitable for continuous spaces.
            </p>
            <ul>
                <li><strong>Deterministic Actor:</strong> Instead of a stochastic policy, the actor is a deterministic function $\mu_\theta(s)$ that directly outputs the best action for a given state.</li>
                <li><strong>Q-Function Critic:</strong> The critic, $Q_w(s,a)$, learns the value of state-action pairs, just like in Q-learning.</li>
                <li><strong>Off-Policy with Replay:</strong> DDPG is an off-policy algorithm that uses an experience replay buffer to store and sample transitions, just like DQN, which greatly improves its sample efficiency.</li>
                <li><strong>Soft Target Updates:</strong> DDPG uses soft target updates for both the actor and the critic networks, which was found to be much more stable than the hard updates of the original DQN.</li>
            </ul>
            <p>
                The critic is trained to minimize the standard Bellman error. The actor is trained to produce actions that maximize the critic's Q-value. It does this by performing gradient ascent on the objective $J(\theta) = \mathbb{E}_{s \sim D}[Q_w(s, \mu_\theta(s))]$.
            </p>

            <h4>TD3: Tackling the Flaws of DDPG</h4>
            <p>
              While DDPG was a major step forward, it suffers from a critical flaw: it is extremely sensitive to hyperparameters and can be quite unstable. This is largely due to the same <strong>maximization bias</strong> that plagued DQN, but in the context of the critic's value estimates. The <strong>Twin Delayed Deep Deterministic Policy Gradient (TD3)</strong> algorithm, introduced by Fujimoto et al. in 2018, addresses these issues with three key tricks.
            </p>
            <ol>
                <li><strong>Clipped Double Q-Learning:</strong> To combat overestimation, TD3 learns <em>two</em> independent critic networks, $Q_{w_1}$ and $Q_{w_2}$. When calculating the TD target, it uses the <em>minimum</em> of the two critics' value estimates for the next state. This provides a pessimistic but much more stable target.
                $$ y = r + \gamma \min_{i=1,2} Q_{w_i^-}(s', a') $$
                </li>
                <li><strong>Delayed Policy Updates:</strong> The actor network is updated less frequently than the critic networks (e.g., every 2 critic updates). This allows the critic to converge to a more accurate value estimate before the actor is updated, preventing the actor from exploiting errors in the critic.</li>
                <li><strong>Target Policy Smoothing:</strong> TD3 adds a small amount of clipped noise to the action selected by the target actor when forming the TD target. This helps to smooth the value landscape and makes the critic more robust to small errors in the actor's policy.</li>
            </ol>
            <p>These three additions make TD3 vastly more stable and robust than DDPG, and it remains a very strong baseline for continuous control tasks.</p>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph TD3_Architecture
        S_t[State s_t] --> Actor["Actor"]
        Actor --> A_t[Action a_t]
        
        S_t --> Critic1["Critic 1"]
        S_t --> Critic2["Critic 2"]
        A_t --> Critic1
        A_t --> Critic2
        
        Critic1 --> Loss1((Loss 1))
        Critic2 --> Loss2((Loss 2))
        
        subgraph TargetCalculation["Target Calculation"]
            S_t1["Next State s_{t+1}"] --> TargetActor["Target Actor"]
            TargetActor --> NoisyAction["a' + noise"]
            NoisyAction --> TargetCritic1["Target Critic 1"]
            NoisyAction --> TargetCritic2["Target Critic 2"]
            TargetCritic1 --> MinOp{{min}}
            TargetCritic2 --> MinOp
        end
        
        MinOp --> Loss1
        MinOp --> Loss2
        
        Critic1 -- "Gradient" --> Actor
    end
                </div>
                <p style="text-align:center; font-style:italic;">The TD3 architecture, highlighting the twin critics and the use of the minimum value for the target calculation.</p>
            </div>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-shield-alt"></i>
            4.7 On-Policy Stabilization: TRPO and PPO
          </summary>
          <div class="details-content">
            <p>
              While off-policy methods like DDPG and TD3 offer high sample efficiency, on-policy methods like A2C have their own appeal, often being more stable and easier to implement. However, a core problem with vanilla on-policy policy gradients is their sensitivity to the step size (learning rate). A single bad update step that is too large can catastrophically collapse the policy, from which it may never recover.
            </p>
            <p>
              The next evolution in policy gradient methods was to develop algorithms that could take the largest possible improvement step at each iteration without risking a performance collapse. This led to the idea of a <strong>trust region</strong>.
            </p>
            
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-hiking"></i>Analogy: Hiking in a Foggy Mountain Range</span>
                <p>
                    Imagine you are trying to reach the highest peak in a mountain range, but it's extremely foggy. You can only see the ground right under your feet. This is analogous to our policy optimization landscape.
                </p>
                <ul>
                    <li><strong>Vanilla Gradient Ascent:</strong> You measure the slope (the gradient) where you are standing and take a fixed-size step in the steepest direction. If you are on a gentle, well-behaved slope, this works well. But what if you are on the edge of a cliff? A single large step could send you plummeting down, far away from the peak. This is a policy collapse.</li>
                    <li><strong>Trust Region Methods:</strong> Before taking a step, you draw a small circle around your feet and say, "I trust that within this small circle (my 'trust region'), the terrain is roughly how I'm measuring it to be." You then find the highest point <em>within that circle</em> and step there. This guarantees that you are always making progress (or at least not getting worse) and will never step off a cliff. The challenge is that solving this constrained optimization problem can be complex.</li>
                </ul>
            </div>
            
            <h4>TRPO: Trust Region Policy Optimization</h4>
            <p>
              <strong>Trust Region Policy Optimization (TRPO)</strong>, introduced by Schulman et al. in 2015, formalizes this intuition. Instead of a simple gradient ascent step, TRPO tries to solve a constrained optimization problem at each iteration:
            </p>
            $$ \max_{\theta} \quad \mathbb{E}_{s \sim \rho_{\theta_{old}}, a \sim \pi_{\theta_{old}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} \hat{A}_{\theta_{old}}(s,a) \right] $$
            $$ \text{subject to} \quad \mathbb{E}_{s \sim \rho_{\theta_{old}}} [D_{KL}(\pi_{\theta_{old}}(\cdot|s) || \pi_\theta(\cdot|s))] \le \delta $$
            <p>
              Here, the objective is to maximize the expected advantage (the "surrogate objective"), but with a hard constraint that the new policy $\pi_\theta$ cannot be too different from the old policy $\pi_{\theta_{old}}$. The "distance" between policies is measured by the Kullback-Leibler (KL) divergence, a measure of how one probability distribution differs from another. $\delta$ is a small hyperparameter that defines the size of the trust region.
            </p>
            <p>
              TRPO is a powerful and theoretically sound algorithm, but solving this constrained optimization problem requires complex second-order methods (like the conjugate gradient algorithm), making it computationally expensive and difficult to implement.
            </p>

            <h4>PPO: Proximal Policy Optimization</h4>
            <p>
              <strong>Proximal Policy Optimization (PPO)</strong>, introduced by Schulman et al. in 2017, has become one of the most popular and widely used RL algorithms. It achieves the same goal as TRPO—stable policy updates within a trust region—but with a much simpler objective function that can be optimized with standard first-order methods like Adam.
            </p>
            <p>
              PPO's key innovation is the <strong>clipped surrogate objective</strong>. Let $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ be the probability ratio between the new and old policies. The PPO objective is:
            </p>
            $$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \quad \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right] $$
            <p>
              Let's break this down:
            </p>
            <ul>
                <li>$r_t(\theta) \hat{A}_t$: This is the standard policy gradient objective.</li>
                <li>$\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t$: This is the same term, but the probability ratio $r_t(\theta)$ is "clipped" to stay within a small window $[1-\epsilon, 1+\epsilon]$ around 1. $\epsilon$ is a small hyperparameter, typically 0.2.</li>
                <li>$\min(\dots)$: We then take the minimum of the unclipped and clipped objectives.</li>
            </ul>
            
            <div class="visualization-container">
                <div id="ppo-clip-plot" style="width:100%; height:400px;"></div>
            </div>
            <script>
            (function() {
                const plotDiv = document.getElementById('ppo-clip-plot');
                if (!plotDiv || typeof Plotly === 'undefined') return;
                
                const epsilon = 0.2;
                const r = Array.from({length: 201}, (_, i) => i * 0.02); // ratio from 0 to 4

                // Case 1: Positive Advantage
                const A_pos = 1.0;
                const L_pos = r.map(val => Math.min(val * A_pos, Math.max(1 - epsilon, Math.min(val, 1 + epsilon)) * A_pos));
                
                // Case 2: Negative Advantage
                const A_neg = -1.0;
                const L_neg = r.map(val => Math.max(val * A_neg, Math.max(1 - epsilon, Math.min(val, 1 + epsilon)) * A_neg));

                Plotly.newPlot(plotDiv, [
                    {x: r, y: L_pos, name: 'L_clip for A > 0', line: {color: 'var(--color-accent-success)'}},
                    {x: r, y: L_neg, name: 'L_clip for A < 0', line: {color: 'var(--color-accent-danger)'}}
                ], {
                    title: 'The PPO Clipped Surrogate Objective',
                    xaxis: {title: 'Probability Ratio r(θ)'},
                    yaxis: {title: 'Objective L_clip'},
                    shapes: [
                        {type: 'line', x0: 1 - epsilon, x1: 1 - epsilon, y0: -2, y1: 2, line: {color: 'grey', dash: 'dot'}},
                        {type: 'line', x0: 1 + epsilon, x1: 1 + epsilon, y0: -2, y1: 2, line: {color: 'grey', dash: 'dot'}}
                    ]
                });
            })();
            </script>
            
            <p>
              The effect of this clipping is brilliant. If the advantage $\hat{A}_t$ is positive (i.e., the action was better than expected), the objective increases with the probability ratio, encouraging the policy to take this action more. However, the clipping prevents the ratio from increasing beyond $1+\epsilon$, which stops the policy update from being too large. Conversely, if the advantage is negative, the objective is clipped to prevent the ratio from decreasing beyond $1-\epsilon$, again limiting the size of the update. This simple mechanism effectively creates a soft "trust region" without the need for complex second-order math, which is why PPO is so stable, effective, and easy to implement.
            </p>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-thermometer-half"></i>
            4.8 Maximum Entropy RL: Soft Actor-Critic (SAC)
          </summary>
          <div class="details-content">
            <p>
              The final algorithm we will explore in this part represents the state of the art in off-policy learning for continuous control: <strong>Soft Actor-Critic (SAC)</strong>. Introduced by Haarnoja et al. in 2018, SAC is an off-policy actor-critic algorithm that optimizes a policy within the framework of <strong>maximum entropy reinforcement learning</strong>.
            </p>
            <p>
              The core idea is to change the fundamental objective of the agent. Instead of just trying to maximize the cumulative reward, the agent tries to succeed at the task while acting as randomly as possible. This entropy bonus provides powerful, built-in exploration and makes the learned policy more robust to perturbations.
            </p>
            
            <h4>The Maximum Entropy Objective</h4>
            <p>The standard RL objective is to maximize the expected sum of discounted rewards. The maximum entropy objective adds an "entropy bonus" at each time step:</p>
            $$ J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[ r(s_t, a_t) + \alpha H(\pi(\cdot|s_t)) \right] $$
            <p>
              Here, $H(\pi(\cdot|s_t))$ is the entropy of the policy distribution at state $s_t$. The temperature parameter, $\alpha > 0$, determines the relative importance of the entropy term versus the reward. It controls the "softness" of the policy.
            </p>

            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-random"></i>Analogy: The Cautious Investor</span>
                <p>
                    Imagine an investor whose goal is to maximize returns.
                </p>
                <ul>
                    <li><strong>A Standard RL Investor (like TD3):</strong> Finds a single stock (or a very specific portfolio) that it believes will yield the absolute highest return. It puts all its money there. This is a high-risk, high-reward, deterministic strategy.</li>
                    <li><strong>A Maximum Entropy Investor (SAC):</strong> Also wants to maximize returns, but is also incentivized to diversify. The entropy bonus is like a reward for not putting all its eggs in one basket. It will find a portfolio of good stocks and distribute its investment among them. This policy is more random (stochastic) and far more robust. If its top stock suddenly crashes, it doesn't lose everything.</li>
                </ul>
                <p>
                    The temperature parameter $\alpha$ is like the investor's risk tolerance. A high $\alpha$ means high risk aversion, leading to a very diversified (high-entropy) portfolio. A low $\alpha$ means the investor cares more about raw returns and will focus on just a few top stocks (low-entropy).
                </p>
            </div>

            <h4>The SAC Algorithm: Three Networks</h4>
            <p>
              SAC is an actor-critic algorithm that uses five neural networks in total:
            </p>
            <ol>
                <li><strong>A Stochastic Actor (Policy Network) $\pi_\theta(a|s)$:</strong> Takes a state and outputs the parameters (mean and standard deviation) of a distribution (typically a squashed Gaussian) from which actions are sampled.</li>
                <li><strong>Two "Soft" Q-Function Critics ($Q_{w_1}, Q_{w_2}$):</strong> Like TD3, SAC uses two critics to mitigate maximization bias. They learn a "soft" Q-value that includes the future entropy bonuses.</li>
                <li><strong>Two Target Q-Networks ($Q_{w_1^-}, Q_{w_2^-}$):</strong> Target networks are used to stabilize the critic updates, just like in DQN and TD3.</li>
            </ol>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph SAC_Architecture
        S_t[State s_t] --> Actor["Actor"]
        Actor --> A_t[Action a_t]
        
        S_t --> Critic1["Q-Critic 1"]
        S_t --> Critic2["Q-Critic 2"]
        A_t --> Critic1
        A_t --> Critic2
        
        Critic1 --> Loss1((Critic Loss 1))
        Critic2 --> Loss2((Critic Loss 2))
        Actor --> Loss_Actor((Actor Loss))
        
        subgraph TargetCalculation["Soft TD Target"]
            S_t1["Next State s_{t+1}"] --> TargetActor["Actor"]
            TargetActor --> NextAction
            NextAction --> TargetCritic1["Target Q-Critic 1"]
            NextAction --> TargetCritic2["Target Q-Critic 2"]
            TargetCritic1 --> MinOp{{min}}
            TargetCritic2 --> MinOp
            TargetActor --> EntropyTerm((αH))
            MinOp --> AddOp(+)
            EntropyTerm --> AddOp
        end
        
        AddOp --> Loss1
        AddOp --> Loss2
    end
    class TargetCalculation target-class
</div>
                <p style="text-align:center; font-style-italic;">The SAC architecture. Note the stochastic actor and the addition of the entropy term to the soft TD target.</p>
            </div>
            
            <h5>The Loss Functions</h5>
            <p>The networks are trained by minimizing three separate loss functions:</p>
            <ul>
                <li><strong>Critic Loss:</strong> The critics are trained to minimize the soft Bellman residual. The target includes the reward, the discounted value of the next state, and the discounted entropy bonus.
                $$ y = r + \gamma \left( \min_{i=1,2} Q_{w_i^-}(s', a') - \alpha \log \pi_\theta(a'|s') \right) \quad \text{where } a' \sim \pi_\theta(\cdot|s') $$
                $$ L_{Q_i} = \mathbb{E}_{(s,a,r,s') \sim D} \left[ (Q_{w_i}(s,a) - y)^2 \right] $$
                </li>
                <li><strong>Actor Loss:</strong> The actor is trained to produce actions that maximize the soft Q-value. It is updated to maximize the expected value from the critic plus the entropy of its own actions.
                $$ L_{\pi} = \mathbb{E}_{s \sim D, a \sim \pi_\theta} \left[ \alpha \log \pi_\theta(a|s) - Q_{w_1}(s,a) \right] $$
                </li>
                <li><strong>(Optional) Temperature Loss:</strong> In modern SAC, the temperature parameter $\alpha$ is not a fixed hyperparameter but is learned automatically by training it to minimize its own loss function, which aims to keep the policy's average entropy close to a target entropy value.</li>
            </ul>

            <div class="interactive-lab">
                <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: The SAC Temperature Parameter</div>
                <p>
                    This lab visualizes the effect of the temperature parameter $\alpha$ on the SAC objective. We have a set of 5 actions with different expected returns and entropies. Observe how changing $\alpha$ alters the final objective value, potentially changing which action is considered "best".
                </p>
                <div class="lab-controls">
                    <div class="lab-control">
                        <label for="sac-alpha-slider">Temperature (α): <span id="sac-alpha-value">0.5</span></label>
                        <input type="range" id="sac-alpha-slider" min="0" max="2" step="0.1" value="0.5">
                    </div>
                </div>
                <div id="sac-plot" style="width:100%; height:350px;"></div>
            </div>
            <script>
            document.addEventListener('DOMContentLoaded', () => {
                const sacPlotDiv = document.getElementById('sac-plot');
                if (!sacPlotDiv) return;

                const returns = [1.0, 1.2, 0.5, 0.8, 1.1];
                const entropies = [0.2, 0.1, 0.9, 0.5, 0.3];
                const actions = ['Action 1', 'Action 2', 'Action 3', 'Action 4', 'Action 5'];

                function plot_sac() {
                    const alpha = parseFloat(document.getElementById('sac-alpha-slider').value);
                    document.getElementById('sac-alpha-value').textContent = alpha.toFixed(1);
                    const objectives = returns.map((r, i) => r + alpha * entropies[i]);
                    
                    Plotly.react(sacPlotDiv, [
                        {x: actions, y: returns, name: 'Expected Return', type: 'bar'},
                        {x: actions, y: objectives, name: 'Return + α * Entropy', type: 'bar'}
                    ], {
                        title: 'SAC Objective with Entropy Regularization',
                        barmode: 'group',
                        yaxis: {title: 'Objective Value'}
                    }, {responsive: true});
                }

                document.getElementById('sac-alpha-slider').addEventListener('input', plot_sac);
                plot_sac();
            });
            </script>
            <p>
                By combining the stability of the off-policy, twin-critic architecture of TD3 with the powerful exploration and robustness benefits of maximum entropy RL, SAC achieves state-of-the-art performance and sample efficiency on a wide range of continuous control benchmarks.
            </p>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-sitemap"></i>
            4.9 Summary and Comparative Analysis of Policy-Based Methods
          </summary>
          <div class="details-content">
            <p>
              This part has charted a course through the landscape of modern policy optimization, from the foundational Policy Gradient Theorem to the state-of-the-art Soft Actor-Critic. Unlike value-based methods that learn a policy indirectly, these algorithms directly parameterize and optimize the agent's behavior. This journey reveals a clear evolutionary path, with each new algorithm building upon the last to address fundamental challenges like high variance, stability, and efficient exploration.
            </p>

            <h4>An Evolutionary Path of Innovations</h4>
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Foundational Problem"
        A[Vanilla Policy Gradient] --> B{High Variance?}
    end

    B -- "Solution 1" --> C[REINFORCE with Baseline]
    C --> D{Baseline?}
    D -- "Solution" --> E[Actor-Critic]

    subgraph "Challenge 1: Stability"
        E --> F{Large Updates?}
        F -- "Solution 1" --> G[TRPO]
        G -- "Problem" --> H[PPO]
    end

    subgraph "Challenge 2: Sample Efficiency"
        E --> I{On-Policy?}
        I -- "Solution" --> J[DDPG]
        J --> K{Unstable?}
        K -- "Solution" --> L[TD3]
        L --> M{Exploration?}
        M -- "Solution" --> N[SAC]
    end

    subgraph "Modern Baselines"
       H
       N
    end

    class H,N advanced-method
                </div>
                <p style="text-align:center; font-style:italic;">The evolution of policy gradient and actor-critic algorithms, driven by the need to solve challenges of variance, stability, and efficiency.</p>
            </div>

            <h4>Comparative Analysis of Key Algorithms</h4>
            <table>
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Policy Type</th>
                        <th>On/Off-Policy</th>
                        <th>Key Mechanism</th>
                        <th>Strengths</th>
                        <th>Weaknesses</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>REINFORCE</strong></td>
                        <td>Stochastic</td>
                        <td>On-Policy</td>
                        <td>Monte Carlo update using the Policy Gradient Theorem.</td>
                        <td>Simple, foundational, unbiased gradient.</td>
                        <td>Extremely high variance, very sample inefficient.</td>
                    </tr>
                    <tr>
                        <td><strong>A2C/A3C</strong></td>
                        <td>Stochastic</td>
                        <td>On-Policy</td>
                        <td>Uses a learned value function (critic) as a baseline to reduce variance.</td>
                        <td>Much more stable and faster than REINFORCE. A3C allows for massive parallelism.</td>
                        <td>Still on-policy and can be sensitive to learning rates.</td>
                    </tr>
                    <tr>
                        <td><strong>DDPG</strong></td>
                        <td>Deterministic</td>
                        <td>Off-Policy</td>
                        <td>Applies DQN's replay buffer and target networks to an actor-critic setup for continuous control.</td>
                        <td>Highly sample-efficient due to off-policy learning.</td>
                        <td>Very unstable, sensitive to hyperparameters, suffers from maximization bias.</td>
                    </tr>
                    <tr>
                        <td><strong>TD3</strong></td>
                        <td>Deterministic</td>
                        <td>Off-Policy</td>
                        <td>Twin critics, delayed policy updates, and target policy smoothing.</td>
                        <td>Fixes many of DDPG's stability issues, making it a robust and high-performing algorithm.</td>
                        <td>More complex than DDPG; exploration is still a simple noise process.</td>
                    </tr>
                    <tr>
                        <td><strong>PPO</strong></td>
                        <td>Stochastic</td>
                        <td>On-Policy</td>
                        <td>Clipped surrogate objective to create a soft trust region for policy updates.</td>
                        <td>Excellent stability, reliable performance, and simple to implement. Often a very strong default choice.</td>
                        <td>Being on-policy, it is less sample-efficient than off-policy methods like SAC.</td>
                    </tr>
                    <tr>
                        <td><strong>SAC</strong></td>
                        <td>Stochastic</td>
                        <td>Off-Policy</td>
                        <td>Maximum entropy framework, twin soft Q-critics, and automatic temperature tuning.</td>
                        <td>State-of-the-art performance and sample efficiency for continuous control. Excellent exploration.</td>
                        <td>Can be complex to implement and debug due to its multiple components and objectives.</td>
                    </tr>
                </tbody>
            </table>

            <h4>Choosing the Right Algorithm</h4>
            <p>
                The "best" algorithm depends heavily on the problem domain.
            </p>
            <ul>
                <li>For <strong>continuous control tasks</strong> where sample efficiency is critical (e.g., training on a real robot), off-policy methods like <strong>SAC</strong> or <strong>TD3</strong> are usually the top contenders. SAC's built-in exploration often gives it an edge.</li>
                <li>For problems where stability and reliability are paramount, or when you can generate a large amount of data relatively cheaply (e.g., in a fast simulation), <strong>PPO</strong> is an exceptionally strong and often preferred baseline. Its simplicity and robustness make it a go-to algorithm in many research and applied settings (including the alignment of large language models).</li>
                <li>Older algorithms like A2C and DDPG are now less commonly used in favor of their more advanced successors, but they remain important for understanding the historical development of the field.</li>
            </ul>
            <p>
                The development of these algorithms showcases a beautiful interplay between theory and practice. From the clean mathematics of the Policy Gradient Theorem to the clever engineering tricks of TD3 and the principled optimization framework of SAC, this family of methods represents a powerful and versatile toolkit for solving complex decision-making problems.
            </p>
          </div>
        </details>
</section>
      <hr/>
      <section id="part5">
        <h2 id="part5-title">
          <i class="fas fa-industry"></i>
          Part 5: Applied Deep Reinforcement Learning
        </h2>
        <p>
            Having explored the core algorithmic families of value-based and policy-based methods, we now shift our focus from benchmark environments like CartPole and Atari to the complexities of real-world applications. Applying RL successfully requires more than just choosing the right algorithm; it demands a deep understanding of the problem domain, careful state and reward engineering, and a keen awareness of the practical challenges that arise when moving from simulation to reality.
        </p>
        <p>
            This part will present two detailed case studies that showcase how the principles we've learned can be translated into solutions for classic problems in operations research and robotics. We will focus heavily on the crucial first step of any RL project: <strong>formulating the problem as a Markov Decision Process</strong>.
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-boxes-stacked"></i>
            5.1 Case Study: Perishable Inventory Management
          </summary>
          <div class="details-content">
            <p>
              A classic and economically significant problem in operations research is <strong>perishable inventory management</strong>. The goal is to decide how much inventory of a product to order at each time step to meet uncertain customer demand, while facing the dual challenges of product spoilage (e.g., groceries, pharmaceuticals) and lead times for new orders. Mismanaging this trade-off leads to either costly waste (over-ordering) or lost sales and customer dissatisfaction (under-ordering).
            </p>
            <p>
              Reinforcement Learning is exceptionally well-suited for this kind of stochastic optimization problem. While traditional methods like the Economic Order Quantity (EOQ) model or (s, S) policies are effective under simplifying assumptions (like stationary demand), an RL agent can learn a highly nuanced, state-dependent ordering policy directly from data or simulation, adapting to complex dynamics like seasonality, promotions, and variable lead times.
            </p>

            <h4>Step 1: Formulating the MDP</h4>
            <p>The most critical step is to define the state, action, and reward that accurately capture the problem.</p>
            
            <h5>The State Space ($S_t$)</h5>
            <p>The state must capture all information necessary to make an optimal ordering decision. For a perishable product, this must include not just the total inventory, but the <em>age</em> of that inventory.</p>
            <ul>
                <li><strong>On-Hand Inventory:</strong> A vector where each element $i_j$ is the quantity of product that will expire in $j$ days. For a product with a 3-day shelf life, this could be $[i_1, i_2, i_3]$.</li>
                <li><strong>In-Transit Orders (Pipeline):</strong> A vector where each element $o_k$ is the quantity of an order scheduled to arrive in $k$ days. For a 2-day lead time, this could be $[o_1, o_2]$.</li>
            </ul>
            <p>The complete state is the concatenation of these vectors. This detailed representation is crucial because it allows the agent to distinguish between having 100 units that all expire tomorrow versus 100 units that are fresh.</p>
            $$S_t = [\underbrace{i_{t,1}, i_{t,2}, \dots, i_{t,m}}_{\text{On-hand by age}}, \underbrace{o_{t,1}, o_{t,2}, \dots, o_{t,L}}_{\text{In-transit by arrival time}}]$$

            <h5>The Action Space ($A_t$)</h5>
            <p>The action is the quantity of new product to order at the current time step. This can be modeled in two ways:</p>
            <ul>
                <li><strong>Discrete Action Space:</strong> The agent can only order from a predefined set of quantities (e.g., {0, 10, 20, 50, 100} units). This formulation is suitable for value-based methods like DQN.</li>
                <li><strong>Continuous Action Space:</strong> The agent can order any non-negative real number, $a_t \in [0, \infty)$. This is a more flexible and realistic representation, suitable for policy-based methods like PPO or SAC.</li>
            </ul>
            
            <h5>The Reward Function ($R_{t+1}$)</h5>
            <p>The goal is to maximize profit, or equivalently, minimize total costs. Therefore, the reward is typically defined as the <strong>negative of the total costs</strong> incurred in one time step. The key cost components are:</p>
            $$ \text{Cost}_t = (\text{Holding Cost}) + (\text{Shortage Cost}) + (\text{Wastage Cost}) + (\text{Ordering Cost}) $$
            $$ R_{t+1} = - \left( c_h \cdot (\text{Inventory Held}) + c_s \cdot (\text{Unmet Demand}) + c_w \cdot (\text{Spoiled Items}) + c_o \cdot (\text{Quantity Ordered}) \right) $$
            <p>Designing this reward function requires careful balancing. If the shortage cost $c_s$ is too high relative to the wastage cost $c_w$, the agent will learn to over-order and be wasteful. If $c_w$ is too high, it will under-order and frequently run out of stock.</p>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Time t"
        State_t[State S_t] --> Action_t[Action A_t]
    end

    subgraph "Environment Dynamics"
        Demand["Stochastic Demand"]
        Aging["Inventory Aging"]
        Arrival["Order Arrival"]
    end
    
    Action_t --> Demand
    Action_t --> Aging
    Action_t --> Arrival
    
    subgraph "Time t+1"
        State_t1["State S_{t+1}"]
        Reward_t1["Reward R_{t+1}"]
    end
    
    Demand --> State_t1
    Aging --> State_t1
    Arrival --> State_t1
    Demand --> Reward_t1

    class State_t,State_t1 state-class
                </div>
                <p style="text-align:center; font-style:italic;">The flow of a single time step in the inventory MDP. The agent's action influences the next state and reward through complex, stochastic environmental dynamics.</p>
            </div>

            <div class="code-container">
                <div class="code-block-header">
                    <span><i class="fab fa-python"></i> Python: A Gymnasium Environment for Perishable Inventory</span>
                    <div class="buttons">
                        <button class="copy-code-btn" title="Copy Code"><i class="fas fa-copy"></i> Copy</button>
                    </div>
                </div>
                <pre><code class="language-python">
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class PerishableInventoryEnv(gym.Env):
    """
    A gym-compatible environment for single-product perishable inventory management.
    - State: On-hand inventory by age + in-transit pipeline orders.
    - Action: Discrete set of order quantities.
    - Reward: Negative of total costs (holding, shortage, wastage, order).
    """
    def __init__(self, config):
        super(PerishableInventoryEnv, self).__init__()
        
        self.max_age = config.get("max_age", 3)
        self.lead_time = config.get("lead_time", 2)
        self.demand_lambda = config.get("demand_lambda", 10)
        
        # Action space: {0, 5, 10, 15, 20}
        self.order_quantities = np.array([0, 5, 10, 15, 20])
        self.action_space = spaces.Discrete(len(self.order_quantities))
        
        # Observation space
        state_dim = self.max_age + self.lead_time
        self.observation_space = spaces.Box(low=0, high=100, shape=(state_dim,), dtype=np.float32)
        
        # Cost parameters
        self.holding_cost = config.get("holding_cost", 0.5)
        self.shortage_cost = config.get("shortage_cost", 5.0)
        self.wastage_cost = config.get("wastage_cost", 2.0)
        self.order_cost = config.get("order_cost", 1.0)

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # Reset inventory and pipeline to zero
        self.on_hand_inventory = np.zeros(self.max_age, dtype=np.float32)
        self.pipeline = np.zeros(self.lead_time, dtype=np.float32)
        return self._get_state(), {}

    def _get_state(self):
        return np.concatenate([self.on_hand_inventory, self.pipeline])

    def step(self, action):
        # 1. Place a new order
        order_quantity = self.order_quantities[action]
        
        # 2. Receive incoming order
        arrived_stock = self.pipeline[0]
        
        # 3. Update pipeline (orders move one step closer)
        self.pipeline = np.roll(self.pipeline, -1)
        self.pipeline[-1] = order_quantity # New order enters the end of the pipeline
        
        # 4. Age the on-hand inventory
        spoiled_stock = self.on_hand_inventory[-1] # Oldest stock expires
        self.on_hand_inventory = np.roll(self.on_hand_inventory, 1)
        self.on_hand_inventory[0] = arrived_stock # Freshly arrived stock
        
        # 5. Meet stochastic demand (oldest-first policy)
        demand = self.np_random.poisson(self.demand_lambda)
        fulfilled = 0
        for age in range(self.max_age - 1, -1, -1): # Sell oldest stock first
            sell_from_this_age = min(self.on_hand_inventory[age], demand - fulfilled)
            self.on_hand_inventory[age] -= sell_from_this_age
            fulfilled += sell_from_this_age
        
        unmet_demand = demand - fulfilled
        
        # 6. Calculate costs and reward
        total_inventory_held = np.sum(self.on_hand_inventory)
        
        holding_c = self.holding_cost * total_inventory_held
        shortage_c = self.shortage_cost * unmet_demand
        wastage_c = self.wastage_cost * spoiled_stock
        order_c = self.order_cost * order_quantity
        
        total_cost = holding_c + shortage_c + wastage_c + order_c
        reward = -total_cost
        
        # This is a continuing task, so it never terminates on its own
        terminated = False
        truncated = False # Can be used to end episodes after a fixed time limit
        
        return self._get_state(), reward, terminated, truncated, {}

# Example of using the environment
# config = {"max_age": 3, "lead_time": 2, "demand_lambda": 10}
# env = PerishableInventoryEnv(config)
# state, info = env.reset()
# action = env.action_space.sample()
# next_state, reward, _, _, _ = env.step(action)
                </code></pre>
            </div>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-robot"></i>
            5.2 Case Study: Robot Path Planning with Potential Fields
          </summary>
          <div class="details-content">
            <p>
              Guiding a robot from a starting configuration to a goal configuration while avoiding obstacles is a classic problem in robotics. A key challenge when applying Reinforcement Learning to this task is <strong>reward sparsity</strong>. A naive reward function might give +1 for reaching the goal and -1 for crashing, with 0 for all other moves. In a large, continuous state space, an agent exploring randomly might never reach the goal, and thus never receive a learning signal.
            </p>
            <p>
              To overcome this, we can use a technique called <strong>reward shaping</strong>, where we provide the agent with a denser, more informative reward signal to guide its learning. A powerful way to do this is to draw inspiration from classical robotics and use the principles of <strong>Artificial Potential Fields (APF)</strong> to engineer a reward function. This method, pioneered by Oussama Khatib in his 1986 work, provides a beautiful example of how domain knowledge can be integrated into an RL framework.
            </p>

            <h4>The Core Idea: Physics-Based Reward Engineering</h4>
            <p>
              The APF method treats the robot as a point particle moving in a field of forces. The goal exerts an attractive force, pulling the robot towards it, while obstacles exert a repulsive force, pushing the robot away. The total "potential energy" of the robot at any configuration (state) $q$ is the sum of these two potentials:
            </p>
            $$ U(q) = U_{att}(q) + U_{rep}(q) $$
            <p>In classical control, the robot would simply move in the direction of the negative gradient of this field, $-\nabla U(q)$, which is the direction of steepest descent (the net force).</p>
            
            <h5>Mathematical Formulation</h5>
            <ul>
                <li>
                    <strong>Attractive Potential:</strong> A simple and effective attractive potential is a quadratic function of the distance to the goal, $d(q, q_{goal})$. This creates a smooth "bowl" with its minimum at the goal.
                    $$ U_{att}(q) = \frac{1}{2} \zeta d(q, q_{goal})^2 $$
                    Here, $\zeta$ is a scaling factor. The corresponding attractive force, $-\nabla U_{att}(q)$, is a vector that points directly at the goal with a magnitude proportional to the distance.
                </li>
                <li>
                    <strong>Repulsive Potential:</strong> The repulsive potential should be very high near an obstacle and fall to zero far away. A common choice is:
                    $$ U_{rep}(q) = \begin{cases} \frac{1}{2} \eta \left(\frac{1}{d(q, q_{obs})} - \frac{1}{\rho_0}\right)^2 & \text{if } d(q, q_{obs}) \le \rho_0 \\ 0 & \text{if } d(q, q_{obs}) > \rho_0 \end{cases} $$
                    Here, $d(q, q_{obs})$ is the distance to the nearest obstacle, $\eta$ is a scaling factor, and $\rho_0$ is a "zone of influence" beyond which the obstacle has no effect. This function creates a powerful repulsive force that grows to infinity as the robot gets closer to the obstacle.
                </li>
            </ul>

            <div class="visualization-container">
                <div id="potential-field-plot" style="width:100%; height:450px;"></div>
            </div>
            <script>
            (function() {
                const plotDiv = document.getElementById('potential-field-plot');
                if (!plotDiv || typeof Plotly === 'undefined') return;

                const goal = {x: 8, y: 8};
                const obs = {x: 4, y: 4, rho: 2.0};
                const zeta = 0.1, eta = 100.0;

                const x = Array.from({length: 101}, (_, i) => i * 0.1);
                const y = Array.from({length: 101}, (_, i) => i * 0.1);
                const z = [];

                for (let i = 0; i < x.length; i++) {
                    const row = [];
                    for (let j = 0; j < y.length; j++) {
                        const d_goal_sq = Math.pow(x[i] - goal.x, 2) + Math.pow(y[j] - goal.y, 2);
                        const u_att = 0.5 * zeta * d_goal_sq;
                        
                        const d_obs = Math.sqrt(Math.pow(x[i] - obs.x, 2) + Math.pow(y[j] - obs.y, 2));
                        let u_rep = 0;
                        if (d_obs <= obs.rho) {
                            u_rep = 0.5 * eta * Math.pow((1/d_obs) - (1/obs.rho), 2);
                        }
                        row.push(u_att + u_rep);
                    }
                    z.push(row);
                }

                Plotly.newPlot(plotDiv, [{
                    z: z, x: x, y: y, type: 'contour'
                }], {
                    title: 'Artificial Potential Field Visualization',
                    xaxis: {title: 'X Position'}, yaxis: {title: 'Y Position'},
                    shapes: [
                        {type: 'circle', x0: obs.x - 0.2, y0: obs.y - 0.2, x1: obs.x + 0.2, y1: obs.y + 0.2, fillcolor: 'red', line: {width: 0}},
                        {type: 'circle', x0: goal.x - 0.2, y0: goal.y - 0.2, x1: goal.x + 0.2, y1: goal.y + 0.2, fillcolor: 'green', line: {width: 0}}
                    ],
                    annotations: [
                        {x: obs.x, y: obs.y + 0.5, text: 'Obstacle (High Potential)'},
                        {x: goal.x, y: goal.y + 0.5, text: 'Goal (Low Potential)'}
                    ]
                });
            })();
            </script>
            
            <h5>From Potential Fields to Dense Rewards</h5>
            <p>
              Instead of using the potential field to directly control the robot, we can use it to create a dense reward signal for an RL agent. The change in potential energy from moving from state $q_t$ to $q_{t+1}$ provides a natural and principled reward signal:
            </p>
            $$ R_{t+1} = U(q_t) - U(q_{t+1}) $$
            <p>
              This reward is positive when the agent moves "downhill" towards lower potential energy (i.e., towards the goal and away from obstacles). This provides the agent with immediate, step-by-step feedback, making learning vastly more efficient than with a sparse reward.
            </p>

            <div class="admonition info">
              <span class="admonition-title">
                <i class="fas fa-lightbulb"></i>
                The Synergy of APF and RL: Escaping Local Minima
              </span>
              <p>
                A classic problem with the pure APF control method is that it can get stuck in <strong>local minima</strong>. This can happen, for example, when a robot is in a U-shaped obstacle, where the attractive force from the goal and the repulsive force from the obstacle walls balance out, creating a point of zero net force. A classical controller would get stuck there forever.
              </p>
              <p>
                An RL agent, however, has a natural mechanism to escape these minima: <strong>exploration</strong>. The stochasticity of its policy (e.g., from an entropy bonus in SAC or $\epsilon$-greedy exploration) means it will naturally "jiggle" around. This random movement can be enough to push it out of the local minimum, allowing it to continue on its path to the global minimum (the goal). This is a powerful example of how the structured knowledge from a classical method can be combined with the exploratory power of RL to create a more robust solution.
              </p>
            </div>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale-right"></i>
            5.3 Comparative Analysis and Practical Considerations
          </summary>
          <div class="details-content">
            <h4>RL vs. Traditional Planners: A Deeper Dive</h4>
            <p>
              For a well-defined problem like static path planning, traditional algorithms from computer science and control theory, such as A* search or Rapidly-exploring Random Trees (RRT), often seem like a more direct solution. A* is guaranteed to find the shortest path if one exists. So, why would we ever use a complex, data-driven method like RL?
            </p>
            <p>
              The answer lies in the assumptions that these methods make. Traditional planners require a <strong>perfect, known model of the world</strong>. RL, particularly model-free RL, does not. This distinction is the source of RL's power and flexibility.
            </p>
            <table>
              <thead>
                  <tr>
                      <th>Aspect</th>
                      <th>A* Search / RRT*</th>
                      <th>Reinforcement Learning (e.g., PPO with APF rewards)</th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td><strong>World Model</strong></td>
                      <td>Requires a perfect, explicit map of the environment, including the precise geometry of all obstacles.</td>
                      <td>Does not require an explicit map. The "model" is learned implicitly through interaction with the environment (or a simulator).</td>
                  </tr>
                  <tr>
                      <td><strong>Optimality</strong></td>
                      <td>Guaranteed to find the globally optimal (shortest) path.</td>
                      <td>Learns a policy that is often near-optimal but provides no hard guarantees. The quality depends on the reward function, network architecture, and training process.</td>
                  </tr>
                  <tr>
                      <td><strong>Adaptability to Change</strong></td>
                      <td>Extremely brittle. If a single new obstacle appears, the entire plan must be recomputed from scratch, which can be slow.</td>
                      <td>Highly adaptive. A well-trained policy network takes the current state (e.g., sensor readings) as input and outputs an action. It can react instantly to dynamic changes in the environment without re-planning.</td>
                  </tr>
                  <tr>
                      <td><strong>Handling Uncertainty</strong></td>
                      <td>Standard versions are deterministic. Handling stochasticity (e.g., a slippery floor) requires more complex probabilistic planners.</td>
                      <td>Naturally handles stochasticity. The agent learns a policy that is robust to randomness in the environment's dynamics by averaging over its experiences.</td>
                  </tr>
                  <tr>
                      <td><strong>Computational Cost</strong></td>
                      <td>Planning cost can be high for large, high-dimensional state spaces (the "curse of dimensionality"). The cost is paid at runtime.</td>
                      <td>Very high upfront training cost (the "curse of sample complexity"). However, once trained, executing the policy (inference) is extremely fast—just a single forward pass through a neural network.</td>
                  </tr>
              </tbody>
            </table>

            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-shipping-fast"></i>Use Case: Warehouse Robotics</span>
                <p>
                    Consider a fleet of robots in a dynamic warehouse.
                </p>
                <ul>
                    <li><strong>A* Approach:</strong> Each time a human worker moves a pallet, the central map must be updated, and all robots would need to halt and re-plan their paths. This is inefficient and not scalable.</li>
                    <li><strong>RL Approach:</strong> Each robot runs a learned policy that takes its current sensor readings (e.g., LiDAR) as input. When a new pallet appears, it is simply part of the robot's observed state. The policy, having been trained on millions of similar scenarios in simulation, can react instantly and navigate around the new obstacle smoothly. The computation is done at the edge, making the system highly scalable and robust.</li>
                </ul>
            </div>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-question-circle"></i>
            5.4 Open Challenges and the Path to Production
          </summary>
          <div class="details-content">
            <p>
                While these case studies are promising, deploying RL in the real world is fraught with challenges that are active areas of research.
            </p>
            
            <h4>The Simulation-to-Reality (Sim2Real) Gap</h4>
            <p>
                Training RL agents on real hardware is often prohibitively expensive, slow, and dangerous. Therefore, the standard workflow is to train the agent in a physics simulator and then transfer the learned policy to the real robot. However, no simulator is perfect. Small differences between the simulator's physics (e.g., friction, mass, sensor noise) and the real world's physics can cause a policy that was perfect in simulation to fail completely in reality. This is the <strong>Sim2Real Gap</strong>.
            </p>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph LR
    subgraph "Training Pipeline"
        Sim["Simulation Environment"] --> Agent["RL Agent"]
        Agent -- "Learns Policy π_sim" --> Agent
    end
    
    subgraph "Deployment Pipeline"
        Real["Real World"] --> Robot[Physical Robot]
        Policy["Transfer Policy π_sim"] --> Robot
        Robot -- "Executes Action" --> Real
        Robot --> Failure("Policy Fails!")
    end

    class Failure failure-class
                </div>
                <p style="text-align:center; font-style:italic;">The Sim2Real Gap: A policy trained in a perfect simulator often fails when transferred to the complex and noisy real world.</p>
            </div>
            
            <p>
                Techniques to bridge this gap are a major focus of robotics research:
            </p>
            <ul>
                <li><strong>Domain Randomization:</strong> Instead of training in one simulator, we train the agent in thousands of simulator variations at once. We randomize the physics parameters (friction, mass), the lighting, the textures, etc. A policy that learns to succeed across all these variations becomes robust to the small differences it will encounter in the real world.</li>
                <li><strong>System Identification:</strong> We first perform experiments on the real robot to build a more accurate model of its specific dynamics, and then use that model to create a more faithful simulator.</li>
                <li><strong>Fine-tuning:</strong> We take the policy trained in simulation and continue training it for a short time on the real robot with a much smaller learning rate, allowing it to adapt to the real-world dynamics.</li>
            </ul>

            <div class="interactive-lab">
                <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: The Sim2Real Gap</div>
                <p>
                    This lab visualizes the Sim2Real gap. A policy is trained to swing up a pendulum in a simulator with a specific gravity. You can then test this policy in a "real" environment where the gravity is slightly different. Observe how the policy's performance degrades as the reality gap widens.
                </p>
                <div class="lab-controls">
                    <div class="lab-control">
                        <label for="sim-gravity-slider">Simulator Gravity: <span id="sim-gravity-value">9.81</span></label>
                        <input type="range" id="sim-gravity-slider" min="8.0" max="12.0" step="0.01" value="9.81">
                    </div>
                    <div class="lab-control">
                        <label for="real-gravity-slider">"Real" Gravity: <span id="real-gravity-value">9.81</span></label>
                        <input type="range" id="real-gravity-slider" min="8.0" max="12.0" step="0.01" value="9.81">
                    </div>
                </div>
                <div id="sim2real-plot" style="width:100%; height:350px;"></div>
            </div>
            <script>
            document.addEventListener('DOMContentLoaded', () => {
                const sim2realPlotDiv = document.getElementById('sim2real-plot');
                if (!sim2realPlotDiv) return;

                const simGravitySlider = document.getElementById('sim-gravity-slider');
                const realGravitySlider = document.getElementById('real-gravity-slider');

                function update_sim2real_plot() {
                    const simG = parseFloat(simGravitySlider.value);
                    const realG = parseFloat(realGravitySlider.value);
                    document.getElementById('sim-gravity-value').textContent = simG.toFixed(2);
                    document.getElementById('real-gravity-value').textContent = realG.toFixed(2);

                    const gravity_diff = Math.abs(simG - realG);
                    const performance = Math.max(0, 100 - gravity_diff * 50); // Simple performance model

                    Plotly.react(sim2realPlotDiv, [{
                        x: ['Performance'],
                        y: [performance],
                        type: 'bar',
                        marker: {color: `rgba(44, 160, 44, ${performance/100})`}
                    }], {
                        title: 'Policy Performance in "Real" Environment',
                        yaxis: {title: 'Performance (%)', range: [0, 100]}
                    }, {responsive: true});
                }

                simGravitySlider.addEventListener('input', update_sim2real_plot);
                realGravitySlider.addEventListener('input', update_sim2real_plot);
                update_sim2real_plot();
            });
            </script>

            <h4>Safety, Ethics, and Scalability</h4>
            <ul>
              <li><strong>Safe Exploration:</strong> How can we ensure that an RL agent's exploration doesn't lead to catastrophic outcomes, such as a physical robot breaking itself or a financial agent bankrupting a company? This involves developing algorithms with safety constraints or using human oversight during the learning process.</li>
              <li><strong>Ethical Considerations:</strong> The application of RL raises significant ethical questions. For inventory management, should an agent be optimized to manage critical medical supplies, and who is responsible if it fails? For robotics, how do we ensure that autonomous systems are aligned with human values? These are not just technical problems but societal ones.</li>
              <li><strong>Scalability to Multi-Agent Systems:</strong> Many real-world problems involve multiple agents. Extending the inventory problem to a full supply chain or the robotics problem to a fleet of warehouse robots requires moving into the domain of Multi-Agent RL (MARL), which introduces new challenges like non-stationarity (other agents are learning and changing) and credit assignment within a team.</li>
            </ul>
            <p>
                These challenges highlight that applied RL is an interdisciplinary field, requiring expertise not just in machine learning, but also in classical engineering, control theory, ethics, and the specific application domain.
            </p>
          </div>
        </details>
</section>
      <hr/>
      <section id="part6">
        <h2 id="part6-title">
          <i class="fas fa-rocket"></i>
          Part 6: Frontiers & Emerging Directions
        </h2>
        <p>
            Having covered the foundational pillars of value-based and policy-based methods, we now venture into the frontiers of modern reinforcement learning research. The field is evolving at a breathtaking pace, increasingly drawing inspiration from other areas of machine learning, particularly the sequence modeling paradigm that has revolutionized natural language processing. This part explores several cutting-edge topics that are redefining what RL agents can do, from offline learning with Transformers to generative modeling and causal reasoning.
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-robot"></i>
            6.1 Transformers in RL: The Decision Transformer
          </summary>
          <div class="details-content">
            <p>
                One of the most exciting recent developments in RL is the application of the <strong>Transformer architecture</strong>, which has become the de facto standard for large-scale models in NLP (e.g., GPT-4) and computer vision. Instead of learning a value function or a policy gradient, this new approach frames reinforcement learning as a <strong>sequence modeling problem</strong>.
            </p>
            <p>
                The pioneering work in this area is the <strong>Decision Transformer</strong> (Chen et al., 2021). It discards the traditional notions of Bellman updates and value functions entirely. Instead, it treats an entire RL trajectory as a sequence of tokens and uses a GPT-like architecture to predict the next action.
            </p>

            <h4>The Core Idea: RL as Conditional Sequence Modeling</h4>
            <p>
                A standard RL algorithm tries to learn a policy that <em>produces</em> high returns. The Decision Transformer flips this on its head. It is trained on a static, offline dataset of expert trajectories and learns to answer the question: "Given a history of states and actions, and given that I want to achieve a certain future return, what action should I take next?"
            </p>
            <p>The input to the model is a sequence of past "return-to-go," state, and action tokens:</p>
            $$ (\hat{R}_{t-k}, s_{t-k}, a_{t-k}), \dots, (\hat{R}_{t-1}, s_{t-1}, a_{t-1}), (\hat{R}_t, s_t) $$
            <p>Where $\hat{R}_t = \sum_{t'=t}^T r_{t'}$ is the <strong>return-to-go</strong> from time $t$. The model is then trained, using a standard cross-entropy loss, to predict the next action, $a_t$. At test time, we can command the agent to achieve a desired level of performance by feeding a high return-to-go value as input.</p>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Input Sequence"
        T1["R_t-k"] --> S1["s_t-k"]
        S1 --> A1["a_t-k"]
        A1 --> T2["..."] --> S2["s_t"]
    end

    subgraph "Decision Transformer"
        InputEmbed["Token & Positional Embeddings"]
        TransformerBlocks["Transformer Blocks"]
        OutputHead["Action Prediction Head"]
    end
    
    S2 --> InputEmbed --> TransformerBlocks --> OutputHead
    
    OutputHead --> PredictedAction["Predicted Action a_t"]
                </div>
                <p style="text-align:center; font-style:italic;">The Decision Transformer architecture. It processes a sequence of past returns, states, and actions to autoregressively predict the next action that will lead to the desired return.</p>
            </div>

            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-language"></i>Analogy: RL as Language Translation</span>
                <p>
                    Think of this approach like language translation.
                </p>
                <ul>
                    <li><strong>Standard RL is like a student learning to write essays.</strong> They write an essay (generate a trajectory), get a grade (reward), and slowly learn what makes a good essay.</li>
                    <li><strong>Decision Transformer is like a student who has already read the entire library of great literature.</strong> You can give it the start of a sentence from a Shakespeare play and ask it to complete it <em>in the style of Shakespeare</em>. It's not "trying" to get a good grade; it's simply generating text that is consistent with the patterns it has already seen.</li>
                </ul>
                <p>
                    Similarly, the Decision Transformer is not "trying" to maximize reward. It is simply generating actions that are consistent with the high-reward trajectories it was trained on. This is a profound shift from the online, trial-and-error paradigm of traditional RL.
                </p>
            </div>

            <h4>Pros and Cons</h4>
            <ul>
                <li><strong>Pros:</strong>
                    <ul>
                        <li><strong>Leverages powerful, scalable architectures:</strong> Can benefit from the massive progress in Transformer models.</li>
                        <li><strong>Avoids bootstrapping:</strong> By not using Bellman updates, it avoids many of the instability issues associated with them.</li>
                        <li><strong>Excellent for offline RL:</strong> It is designed to work with static datasets of experience, which is a common scenario in real-world applications where online interaction is not possible.</li>
                    </ul>
                </li>
                <li><strong>Cons:</strong>
                    <ul>
                        <li><strong>Cannot learn online:</strong> In its basic form, it cannot improve from new experience gathered by interacting with the environment. It is not a true "online" RL agent.</li>
                        <li><strong>Performance is limited by the dataset:</strong> It can only learn to perform as well as the best trajectories in its training data. It cannot discover novel, better strategies.</li>
                    </ul>
                </li>
            </ul>
          </div>
        </details>
        <details class="fade-in research-tier">
          <summary>
            <i class="fas fa-wave-square"></i>
            6.2 Diffusion Models for RL Policy Generation
          </summary>
          <div class="details-content">
            <p>
              Following the trend of adopting powerful generative models from other domains, another exciting frontier is the use of <strong>diffusion models</strong> for reinforcement learning. Diffusion models, which have achieved state-of-the-art results in image and audio generation, are now being explored as a powerful tool for generating effective policies, particularly in the context of offline RL.
            </p>
            
            <h4>The Core Idea: Learning to Denoise Trajectories</h4>
            <p>
              A diffusion model is a generative model that learns to create data by reversing a gradual noising process. The core process involves two steps:
            </p>
            <ol>
                <li><strong>Forward Process (Fixed):</strong> We take a piece of data (e.g., an expert trajectory of actions) and gradually add a small amount of Gaussian noise over a large number of time steps, $T$. At the end of this process, the data is indistinguishable from pure Gaussian noise. This process is fixed and does not involve any learning.</li>
                <li><strong>Reverse Process (Learned):</strong> We then train a neural network to learn how to reverse this process. The network takes the noisy data at any time step $t$ and learns to predict the noise that was added. By repeatedly subtracting this predicted noise, the model can start from pure noise and gradually denoise it back into a clean, realistic piece of data (a new expert-like trajectory).</li>
            </ol>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Forward Process"
        X0["Clean Data"] -- "+ Noise" --> X1["Noisy"]
        X1 -- "+ Noise" --> X2["..."]
        X2 -- "+ Noise" --> XT["Noise"]
    end

    subgraph "Reverse Process"
        XT_rev["Noise"] -- "Denoise" --> XT1_rev["Less Noise"]
        XT1_rev -- "Denoise" --> XT2_rev["..."]
        XT2_rev -- "Denoise" --> X0_rev["Reconstructed Data"]
    end

    class XT,X0_rev diffusion-class
                </div>
                <p style="text-align:center; font-style:italic;">The diffusion model framework. A fixed forward process corrupts data into noise, and a learned reverse process generates new data by starting from noise and iteratively denoising it.</p>
            </div>

            <h4>Application to Reinforcement Learning</h4>
            <p>
              In RL, we can treat a sequence of actions or a full state-action trajectory as the data to be modeled. In an offline RL setting, we can train a diffusion model on a dataset of expert trajectories.
            </p>
            <p>
              The trained model can then be used as a policy. Given the current state, we can use the model to generate a full future trajectory of actions that is consistent with the expert behavior in the dataset. This is particularly powerful because it allows for planning and can generate diverse, multi-modal behaviors (e.g., learning both a left-turn and a right-turn strategy to solve a maze).
            </p>
            
            <details class="deep-dive">
                <summary><i class="fas fa-file-code"></i> Deep Dive: Pseudocode for a Diffusion Policy</summary>
                <div class="details-content">
                    <pre><code class="language-python">
# --- Training Time (Offline) ---
# 1. Load a dataset of expert trajectories D = { (s_0, a_0, ..., s_H, a_H)_i }

# 2. For each training step:
#    a. Sample a trajectory of actions A = (a_0, ..., a_H) from D
#    b. Sample a random noise level t from {1, ..., T}
#    c. Sample Gaussian noise ε ~ N(0, I)
#    d. Corrupt the action trajectory to that noise level: A_t = sqrt(α_bar_t) * A + sqrt(1 - α_bar_t) * ε
#    e. Train the denoising network ε_θ by minimizing the loss:
#       Loss = || ε_θ(A_t, t, s_0) - ε ||²
#       (Note: We condition the denoiser on the initial state s_0)

# --- Inference Time (Acting as a Policy) ---
# function get_action(current_state):
#    // 1. Start with pure noise for a future action trajectory
#    A_T = sample_gaussian_noise(shape=(H, action_dim))
#    
#    // 2. Iteratively denoise to generate a plan
#    for t = T down to 1:
#        predicted_noise = ε_θ(A_t, t, current_state)
#        A_{t-1} = denoise_step(A_t, predicted_noise)
#
#    // 3. Return the first action of the generated plan
#    return A_0[0]
                    </code></pre>
                </div>
            </details>
            
            <p>
                Diffusion models offer a highly flexible and powerful way to model complex, multi-modal policies. However, they are computationally expensive at inference time due to the iterative denoising process. Research is ongoing to "distill" these models into faster policies, making them more practical for real-time applications.
            </p>
          </div>
        </details>
        <details class="fade-in research-tier">
          <summary>
            <i class="fas fa-link"></i>
            6.3 Causality in RL: Moving Beyond Spurious Correlations
          </summary>
          <div class="details-content">
            <p>
              A fundamental, often unspoken, assumption in most of reinforcement learning is that "correlation implies causation." An agent observes that taking action $A$ in state $S$ is correlated with high rewards, and it learns to repeat that action. However, in the complex, messy real world, this assumption can be dangerously wrong. The intersection of <strong>Causality</strong> and Reinforcement Learning is an emerging frontier that aims to build agents that can reason about the true causal mechanisms of their environment, leading to more robust and generalizable policies.
            </p>
            
            <h4>The Problem: Spurious Correlations</h4>
            <p>
              A <strong>spurious correlation</strong> is a statistical relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a third, unseen factor (a "confounder"). An RL agent that learns a policy based on a spurious correlation will perform well during training but may fail catastrophically when the underlying conditions change.
            </p>

            <div class="admonition danger">
                <span class="admonition-title"><i class="fas fa-umbrella-beach"></i>Failure Case: The Ice Cream and Drowning Epidemic</span>
                <p>
                    Imagine an RL agent tasked with reducing the number of drowning deaths in a city. The agent is given data on daily ice cream sales and daily drowning incidents.
                </p>
                <ul>
                    <li><strong>Observation:</strong> The agent observes a strong positive correlation: on days when ice cream sales are high, drowning deaths are also high.</li>
                    <li><strong>Flawed Policy:</strong> A standard RL agent, acting on this correlation, might learn a policy to "ban ice cream sales." It believes that this action will cause a reduction in drownings.</li>
                    <li><strong>The Confounder:</strong> The policy would fail because the correlation is spurious. The true causal factor is a hidden confounder: the <strong>temperature</strong>. On hot days, more people buy ice cream, AND more people go swimming, which leads to more drownings.</li>
                </ul>
                <p>
                    Banning ice cream would have no effect on drownings. A causal RL agent would aim to first discover the true causal graph of the system and then learn to intervene on the correct variable (e.g., by promoting swimming safety on hot days).
                </p>
            </div>
            
            <h4>The Language of Causality: Structural Causal Models</h4>
            <p>
              Causal inference provides a formal language to talk about these relationships, primarily through <strong>Structural Causal Models (SCMs)</strong> and their graphical representations. An SCM consists of a set of variables and a set of functions that describe how each variable is generated from its direct causes.
            </p>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
                    graph TD
                        subgraph "Correlation-Based View (Standard RL)"
                            IceCream["Ice Cream Sales"] --- Drowning["Drowning Deaths"]
                        end
                        
                        subgraph "Causal View (Causal RL)"
                            Temp["Temperature<br>(Confounder)"] --> IceCream_C["Ice Cream Sales"]
                            Temp --> Drowning_C["Drowning Deaths"]
                        end
                        
                        style Confounder fill:var(--color-accent-danger),color:white
                </div>
                <p style="text-align:center; font-style:italic;">A causal graph reveals the confounding variable (Temperature) that creates a spurious correlation between Ice Cream Sales and Drowning Deaths.</p>
            </div>

            <h4>How Causal RL Can Help</h4>
            <p>
              By incorporating causal reasoning, RL agents can learn policies that are more robust and transferable. Key areas of research include:
            </p>
            <ul>
                <li><strong>Causal Discovery in MDPs:</strong> Algorithms that try to learn the causal structure of the environment's dynamics from interventional data. For example, the agent might learn that its wheel motors' RPMs cause the robot to move, but the color of the floor does not, even if a specific floor color is correlated with high speed in the training data.</li>
                <li><strong>Counterfactual Reasoning:</strong> Causal models allow an agent to ask counterfactual questions, such as "What would the reward have been if I had taken action B instead of action A in this state?" This is particularly powerful in offline RL, where it can help to evaluate policies that take actions not seen in the dataset.</li>
                <li><strong>Generalization and Transfer Learning:</strong> A policy based on true causal links is much more likely to generalize to a new, unseen environment. The agent that learns that "hot weather causes drownings" can apply this knowledge in any city, whereas the agent that learns "ice cream sales are correlated with drownings" cannot.</li>
            </ul>
            <p>
              The integration of causality into RL is still in its early stages and presents many open challenges, such as how to efficiently discover causal structures from high-dimensional data and how to perform causal reasoning at scale. However, it promises to be a crucial step towards building truly intelligent agents that can understand and operate in our complex world.
            </p>
          </div>
        </details>
</section>

      <hr/>
      <section id="part7">
        <h2 id="part7-title">
          <i class="fas fa-map"></i>
          Part 7: Model-Based Reinforcement Learning
        </h2>
        <p>
            Up to this point, we have primarily focused on <strong>model-free</strong> reinforcement learning. In the model-free paradigm, the agent is a pure trial-and-error learner. It doesn't know the rules of the environment (the transition probabilities $P(s'|s,a)$ or the reward function $R(s,a)$) and learns a value function or a policy directly from its interactions. This is like learning to play chess by playing millions of games, without ever being told the rules of how the pieces move.
        </p>
        <p>
            We now turn to an alternative and often much more sample-efficient approach: <strong>model-based reinforcement learning</strong>. In this paradigm, the agent explicitly learns a model of the environment's dynamics and then uses this learned model to simulate experiences and plan a course of action.
        </p>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-balance-scale"></i>
            7.1 The Core Trade-Off: Sample Efficiency vs. Model Bias
          </summary>
          <div class="details-content">
            <p>
              The primary motivation for model-based RL is to improve <strong>sample efficiency</strong>. Model-free algorithms like DQN or PPO often require millions or even billions of interactions with the real environment to learn a good policy. This is feasible in a fast simulator like a video game, but it is completely impractical for problems like learning to control a physical robot, where each interaction is slow and costly.
            </p>
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-chess"></i>Analogy: Learning Chess</span>
                <p>
                    <ul>
                        <li><strong>A Model-Free Learner (like DQN):</strong> Learns to play chess by playing 10 million games against itself. It slowly, through trial and error, learns that moving a bishop diagonally tends to lead to good outcomes, without ever explicitly understanding the rule "bishops move diagonally."</li>
                        <li><strong>A Model-Based Learner:</strong> Plays just a few hundred games. From these games, it deduces the rules: "Aha, every time I move this piece, it seems to stay on the same color squares." It learns an approximate model of the game's dynamics. Then, it can use this learned model to "imagine" or "simulate" thousands of games inside its own "head" without ever having to interact with the real board again. This allows it to learn a strong policy with far fewer real-world interactions.</li>
                    </ul>
                </p>
            </div>
            <p>
              However, this power comes with a significant challenge: <strong>model bias</strong>. If the learned model of the world is inaccurate, the agent will plan using flawed physics. The policy it derives from this flawed plan will be suboptimal. This creates a fundamental trade-off:
            </p>
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Sample Efficiency</th>
                        <th>Asymptotic Performance</th>
                        <th>Key Challenge</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Model-Free RL</strong></td>
                        <td>Low (requires many real interactions)</td>
                        <td>High (can converge to the true optimal policy)</td>
                        <td>Temporal credit assignment, exploration.</td>
                    </tr>
                    <tr>
                        <td><strong>Model-Based RL</strong></td>
                        <td>High (can learn from few real interactions)</td>
                        <td>Limited by the accuracy of the learned model.</td>
                        <td>Model bias (planning in a flawed model).</td>
                    </tr>
                </tbody>
            </table>
            <p>
                The history of model-based RL is a story of developing increasingly sophisticated techniques to mitigate the problem of model bias while retaining the benefits of sample efficiency.
            </p>
            <div class="interactive-lab">
                <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: The Effect of Model Bias</div>
                <p>
                    This lab visualizes the effect of model bias. An agent is trying to find the optimal launch angle to hit a target. The "real world" has a certain amount of drag. The agent learns a model of the world's physics, but this model might be inaccurate. Observe how the agent's "planned" optimal action (based on its flawed model) differs from the true optimal action.
                </p>
                <div class="lab-controls">
                    <div class="lab-control">
                        <label for="true-drag-slider">True World Drag: <span id="true-drag-value">0.1</span></label>
                        <input type="range" id="true-drag-slider" min="0" max="0.5" step="0.01" value="0.1">
                    </div>
                    <div class="lab-control">
                        <label for="model-drag-slider">Learned Model Drag: <span id="model-drag-value">0.1</span></label>
                        <input type="range" id="model-drag-slider" min="0" max="0.5" step="0.01" value="0.1">
                    </div>
                </div>
                <div id="model-bias-plot" style="width:100%; height:350px;"></div>
            </div>
            <script>
            document.addEventListener('DOMContentLoaded', () => {
                const modelBiasPlotDiv = document.getElementById('model-bias-plot');
                if (!modelBiasPlotDiv) return;

                const trueDragSlider = document.getElementById('true-drag-slider');
                const modelDragSlider = document.getElementById('model-drag-slider');

                function get_trajectory(angle, drag) {
                    let x = 0, y = 0, vx = 50 * Math.cos(angle), vy = 50 * Math.sin(angle);
                    const dt = 0.1;
                    const g = 9.81;
                    const traj_x = [0], traj_y = [0];
                    while (y >= 0) {
                        vx -= drag * vx * dt;
                        vy -= (g + drag * vy) * dt;
                        x += vx * dt;
                        y += vy * dt;
                        traj_x.push(x);
                        traj_y.push(y);
                    }
                    return {x: traj_x, y: traj_y};
                }

                function find_optimal_angle(drag) {
                    // Simple search for optimal angle
                    let best_angle = 0, max_dist = 0;
                    for (let angle = 0; angle < Math.PI / 2; angle += 0.01) {
                        const traj = get_trajectory(angle, drag);
                        const dist = traj.x[traj.x.length - 1];
                        if (dist > max_dist) {
                            max_dist = dist;
                            best_angle = angle;
                        }
                    }
                    return best_angle;
                }

                function update_model_bias_plot() {
                    const true_drag = parseFloat(trueDragSlider.value);
                    const model_drag = parseFloat(modelDragSlider.value);
                    document.getElementById('true-drag-value').textContent = true_drag.toFixed(2);
                    document.getElementById('model-drag-value').textContent = model_drag.toFixed(2);

                    const true_opt_angle = find_optimal_angle(true_drag);
                    const model_opt_angle = find_optimal_angle(model_drag);

                    const true_traj = get_trajectory(true_opt_angle, true_drag);
                    const planned_traj = get_trajectory(model_opt_angle, model_drag); // Trajectory in the flawed model
                    const executed_traj = get_trajectory(model_opt_angle, true_drag); // What actually happens

                    Plotly.react(modelBiasPlotDiv, [
                        {x: true_traj.x, y: true_traj.y, name: 'True Optimal Trajectory', line: {color: 'green', dash: 'dash'}},
                        {x: planned_traj.x, y: planned_traj.y, name: 'Planned Trajectory (in Model)', line: {color: 'blue', dash: 'dot'}},
                        {x: executed_traj.x, y: executed_traj.y, name: 'Executed Trajectory (in Reality)', line: {color: 'red'}}
                    ], {
                        title: 'Effect of Model Bias on Planning',
                        xaxis: {title: 'Distance'}, yaxis: {title: 'Height', range: [0, 150]}
                    }, {responsive: true});
                }

                trueDragSlider.addEventListener('input', update_model_bias_plot);
                modelDragSlider.addEventListener('input', update_model_bias_plot);
                update_model_bias_plot();
            });
            </script>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-cogs"></i>
            7.2 Classic Approach: The Dyna-Q Architecture
          </summary>
          <div class="details-content">
            <p>
              One of the earliest and most influential architectures for integrating learning and planning is <strong>Dyna-Q</strong>, introduced by Richard Sutton in 1990. Dyna-Q provides a simple and elegant framework for a model-based agent.
            </p>
            <p>A Dyna-Q agent consists of two main learning loops that operate simultaneously:</p>
            <ol>
                <li><strong>Direct RL (The "Real" Loop):</strong> This is a standard model-free learning loop. The agent takes an action in the real environment, observes the real outcome $(s', r)$, and uses this real experience to update its value function (e.g., using a Q-learning update).</li>
                <li><strong>Model Learning and Planning (The "Imagined" Loop):</strong>
                    <ul>
                        <li><strong>Model Learning:</strong> The same real experience $(s, a, s', r)$ is used to update a learned model of the environment. For a tabular environment, this can be as simple as storing the observed outcome: $Model(s,a) \leftarrow (s', r)$.</li>
                        <li><strong>Planning:</strong> After the direct RL update, the agent performs a number of "planning steps." In each step, it randomly samples a state-action pair $(s_k, a_k)$ that it has experienced before, queries its learned model to get a simulated next state and reward $(\hat{s}', \hat{r}) = Model(s_k, a_k)$, and then uses this <em>simulated</em> experience to perform another Q-learning update.</li>
                    </ul>
                </li>
            </ol>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    %% -------- Agent --------
    subgraph Agent
        Q["Q‑Values / Policy"]
        Model["Learned Model M(s,a)"]
    end

    %% ---- Real Interaction ----
    subgraph realInteraction["Real Interaction"]
        Env((Env)) -->|Experience| DirectRL["Direct RL Update"]
        DirectRL --> Q
        Env --> ModelLearning["Model Update"]
        ModelLearning --> Model
    end

    %% -------- Planning --------
    subgraph planning["Planning"]
        Model -->|Simulated Experience| PlanningUpd["Planning Update"]
        PlanningUpd --> Q
    end
</div>
                <p style="text-align:center; font-style:italic;">The Dyna-Q architecture. Real experience is used for both a direct RL update and to update the world model. The model is then used to generate many simulated experiences for additional planning updates.</p>
            </div>
            
            <details class="deep-dive">
                <summary><i class="fas fa-file-code"></i> Deep Dive: Dyna-Q Pseudocode</summary>
                <div class="details-content">
                    <pre><code>
Initialize Q(s,a) and Model(s,a) for all s, a

Loop forever:
  (a) Get current state S from the real environment
  (b) Choose action A using an ε-greedy policy based on Q(S,A)
  (c) Take action A, observe real next state S' and reward R
  
  // Direct RL Update
  (d) Q(S,A) ← Q(S,A) + α [R + γ * max_a Q(S',a) - Q(S,A)]
  
  // Model Learning
  (e) Model(S,A) ← R, S'
  
  // Planning (or "Imagination")
  (f) Loop n times:
      i.   Randomly select a previously observed state S_k
      ii.  Randomly select an action A_k previously taken in S_k
      iii. R_k, S'_k ← Model(S_k, A_k) // Query the model
      iv.  Q(S_k,A_k) ← Q(S_k,A_k) + α [R_k + γ * max_a Q(S'_k,a) - Q(S_k,A_k)]
                    </code></pre>
                </div>
            </details>
            <p>
                The planning steps in Dyna-Q are what make it so sample-efficient. For every one interaction with the real world, the agent can perform $n$ "imagined" interactions, allowing it to propagate value information much more quickly through its state space. If the environment's dynamics suddenly change, the agent will receive surprising real experiences, which will update its model. The planning process will then quickly spread this new information throughout the value function.
            </p>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-brain"></i>
            7.3 Deep Model-Based RL: Learning in Latent Space
          </summary>
          <div class="details-content">
            <p>
              The Dyna-Q architecture is powerful for tabular or low-dimensional state spaces, but it struggles with high-dimensional observations like images. Trying to build a model that predicts the next video frame pixel-by-pixel is incredibly complex and computationally expensive. Furthermore, most of the information in a high-dimensional observation (like the color of the clouds in the sky) is irrelevant for decision-making.
            </p>
            <p>
              Modern deep model-based RL addresses this challenge by learning a model not in the raw observation space, but in a compressed, abstract <strong>latent space</strong>. The agent learns to encode the essential information from an image into a small latent vector and then learns the dynamics of how these latent vectors evolve over time. This is the core idea behind groundbreaking architectures like <strong>World Models</strong> and the <strong>Dreamer</strong> series.
            </p>

            <h4>The World Models Architecture</h4>
            <p>
              The "World Models" paper (Ha & Schmidhuber, 2018) proposed a powerful, decoupled architecture with three key components:
            </p>
            <ol>
                <li>
                    <strong>The Vision Model (V): A Variational Autoencoder (VAE).</strong> This network is responsible for compressing the high-dimensional raw observation (e.g., a game screen) into a low-dimensional latent vector, $z_t$. It learns to capture the most important features of the state in this compact representation.
                </li>
                <li>
                    <strong>The Memory Model (M): A Mixture Density Network-Recurrent Neural Network (MDN-RNN).</strong> This is the predictive model of the world. It is a recurrent network (like an LSTM) that takes the previous latent state $z_t$ and the action $a_t$ as input and predicts the <em>probability distribution</em> of the next latent state, $P(z_{t+1}|z_t, a_t)$. The use of a Mixture Density Network allows it to model the inherent stochasticity of the environment.
                </li>
                <li>
                    <strong>The Controller (C): A Simple Policy Network.</strong> This is a small, simple feedforward neural network that takes the latent state $z_t$ as input and outputs an action $a_t$.
                </li>
            </ol>
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Perception & World Model"
        O_t[Observation o_t] --> VAE["Vision Model"]
        VAE --> Z_t["Latent z_t"]
        
        Z_t --> RNN["Memory Model"]
        A_t[Action a_t] --> RNN
        RNN --> Z_t1_hat("Imagined z_{t+1}")
    end

    subgraph "Controller Training"
        Z_t1_hat --> Controller["Controller"]
        Controller --> A_t1_hat("Imagined a_{t+1}")
        A_t1_hat --> RNN
    end
                            </div>
            </div>
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-bed"></i>The "Dreaming" Analogy</span>
                <p>
                    The most brilliant part of the World Models approach is how the controller is trained. It is trained <strong>entirely inside the "dream" of the learned world model</strong>.
                </p>
                <ol>
                    <li>First, the VAE and MDN-RNN are trained on a dataset of random interactions with the real environment to learn a good world model.</li>
                    <li>Then, the real environment is thrown away.</li>
                    <li>The controller is then trained entirely within the fast, parallelizable, learned world model. The MDN-RNN generates a "dream" trajectory of latent states, and the controller learns a policy to maximize rewards within this dream.</li>
                    <li>Finally, the trained controller is transferred back to the real environment, where it often performs remarkably well, having learned a compact and effective policy in a highly sample-efficient manner.</li>
                </ol>
                <p>
                    This decoupling separates the complex task of perception and prediction from the simpler task of control.
                </p>
            </div>


            <h4>Dreamer and DreamerV2: Learning Values in Imagination</h4>
            <p>
                The <strong>Dreamer</strong> architecture (Hafner et al., 2019) and its successors build upon the World Models idea by integrating an actor-critic framework that is also trained entirely on imagined trajectories.
            </p>
            <p>
                Instead of just learning a policy, Dreamer's world model also learns to predict rewards. This allows it to learn a critic (a value function) and an actor (a policy) by "unrolling" long trajectories within the latent space and applying actor-critic updates (similar to A2C) to these imagined experiences. This approach, known as <strong>learning from imagination</strong>, has achieved state-of-the-art performance on many challenging continuous control benchmarks from pixel inputs, demonstrating incredible sample efficiency.
            </p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-search"></i>
            7.4 The Pinnacle: MuZero and Combining Search with a Learned Model
          </summary>
          <div class="details-content">
            <p>
              The final step in our journey through model-based RL is <strong>MuZero</strong>, introduced by DeepMind in 2019. MuZero represents a major breakthrough by combining a learned model with the powerful lookahead search capabilities of algorithms like AlphaGo. It achieved superhuman performance on Go, chess, and shogi, as well as Atari, without being told the rules of the game.
            </p>
            <p>
              The key difference from the previous methods is that MuZero's learned model is not designed to be a perfect simulator of the future. Instead, it learns only the aspects of the environment that are critical for planning.
            </p>
            
            <h5>The Three Networks of MuZero</h5>
            <p>MuZero uses a single, deep neural network that performs three critical functions:</p>
            <ol>
                <li><strong>Representation Function ($h$):</strong> This is analogous to the VAE in World Models. It takes the current observation (e.g., the board state) and encodes it into a hidden, latent state $s_0$.</li>
                <li><strong>Dynamics Function ($g$):</strong> This is the learned model. It takes a hidden state $s_{k-1}$ and a hypothetical action $a_k$ and predicts the next hidden state, the reward, and the policy for that next state: $s_k, r_k, p_k = g(s_{k-1}, a_k)$.</li>
                <li><strong>Prediction Function ($f$):</strong> This function takes a hidden state $s_k$ and predicts two things: the policy (a distribution over actions) $p_k$ and the value (the expected outcome) $v_k$.</li>
            </ol>
            
            <h5>Planning with Monte Carlo Tree Search (MCTS)</h5>
            <p>
              At each turn, MuZero does not simply pick an action. It performs a sophisticated lookahead search using its learned model.
            </p>
            <ol>
                <li>The current board state is encoded into a root hidden state $s_0$.</li>
                <li>It then runs a Monte Carlo Tree Search. In each simulation within the tree, it selects a sequence of actions using an algorithm like UCB.</li>
                <li>Crucially, to get the next state in the search tree, it doesn't use a real game engine; it uses its learned dynamics function $g$. This allows it to "imagine" deep into the future of the game.</li>
                <li>The values and policies predicted by the prediction function $f$ are used to guide the search and evaluate the leaves of the search tree.</li>
                <li>After many simulations, the search tree provides a robust estimate of the best action to take from the current state.</li>
            </ol>
            <p>
              By combining a learned, abstract model with the powerful, principled lookahead of MCTS, MuZero achieves a level of performance and generality that surpasses previous methods, representing the current state of the art in this domain.
            </p>
          </div>
        </details>
</section>
      <hr/>
      <section id="part8">
        <h2 id="part8-title">
          <i class="fas fa-database"></i>
          Part 8: Offline (Batch) Reinforcement Learning
        </h2>
        <p>
            All the reinforcement learning paradigms we have discussed so far—both model-free and model-based—have operated under a fundamental assumption: the agent can <strong>continuously interact with the environment</strong> to collect new data. This online interaction, with its cycle of trial, error, and exploration, is the engine of learning. However, in many high-stakes, real-world applications, this is a luxury we do not have.
        </p>
        <p>
            <strong>Offline Reinforcement Learning</strong>, also known as Batch RL, is a paradigm designed for this reality. It is the science of learning a decision-making policy from a large, static, pre-collected dataset of experiences, with the critical constraint of having <strong>no further interaction with the live environment</strong>.
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-hospital"></i>
            8.1 The Motivation: Why Learn From a Fixed Logbook?
          </summary>
          <div class="details-content">
            <p>
              Why would we willingly give up the ability to explore and gather new data? In many real-world scenarios, online exploration is either:
            </p>
            <ul>
                <li><strong>Prohibitively Expensive:</strong> Running a new marketing campaign to see its effect costs real money and resources. Each online interaction has a real cost.</li>
                <li><strong>Unacceptably Slow:</strong> Testing a new agricultural strategy requires a full growing season. The feedback loop is measured in months, not milliseconds.</li>
                <li><strong>Irreversibly Dangerous:</strong> Allowing a self-driving car to explore "interesting" steering angles on a public road is catastrophic. Allowing an RL agent to explore different drug treatments on real patients is unethical and illegal.</li>
            </ul>
            <p>
              In these domains, we often have access to massive logs of historical data. A hospital has electronic health records detailing the treatments given to thousands of past patients and their outcomes. An e-commerce site has logs of which products were recommended to users and whether they made a purchase. A factory has sensor logs from its robotic assembly lines. Offline RL aims to leverage this existing data to learn a better policy than the one that was used to collect the data in the first place, unlocking the value hidden in these vast datasets.
            </p>
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-notes-medical"></i>Analogy: Learning a Medical Treatment Policy</span>
                <p>
                    Imagine you want to train an RL agent to be an expert doctor for treating a specific illness.
                </p>
                <ul>
                    <li><strong>Online RL (Unethical):</strong> You let a randomly initialized agent take over a hospital ward. It tries random treatments on patients to see what happens. This is obviously not permissible.</li>
                    <li><strong>Offline RL (The Practical Approach):</strong> You collect a massive, anonymized dataset of patient records from the last 10 years. This dataset contains the patient's state (symptoms, lab results), the action taken by the human doctor (treatment administered), and the outcome (reward, e.g., patient improved or not). You then train your RL agent on this static dataset to learn a policy that, given a patient's state, recommends the optimal treatment. The goal is to learn a policy that is better than the average human doctor in the dataset.</li>
                </ul>
            </div>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-exchange-alt"></i>
            8.2 The Core Challenge: The Perils of Distributional Shift
          </summary>
          <div class="details-content">
            <p>
              Offline RL seems simple at first glance. Why not just apply a powerful off-policy algorithm like SAC directly to the static dataset, treating it like a giant, non-updating replay buffer? The reason this fails spectacularly is a problem known as <strong>distributional shift</strong>.
            </p>
            <p>
              This is the single most important concept in offline RL. It arises because the distribution of state-action pairs induced by the learned policy, $\pi_\theta$, will be different from the distribution of the behavior policy, $\pi_b$, that generated the dataset. The agent will inevitably learn to take actions that were not well-represented in the original data.
            </p>
            <p>
              The problem becomes critical during the Bellman backup. When we calculate the TD target, we need to evaluate the value of the next state, which involves a $\max$ or an expectation over actions in that next state:
            </p>
            $$ y = r + \gamma \max_{a'} Q(s', a') $$
            <p>
              The agent might consider an action $a'$ that was <strong>never taken</strong> in state $s'$ in the original dataset. The Q-network has never been trained on the outcome of this state-action pair. Because neural networks are notoriously bad at extrapolating to out-of-distribution (OOD) inputs, the Q-value it produces for this unseen action, $Q(s', a')$, is effectively garbage. Worse, due to function approximation errors, this garbage value is often erroneously high. This is called <strong>extrapolation error</strong>. The $\max$ operator will greedily select this erroneously high value, leading to a biased and incorrect target that gets propagated through the learning process, causing the policy to diverge.
            </p>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Data Distribution"
        S1[State s'] --> A1("Action 1")
        S1 --> A2("Action 2")
    end
    
    subgraph "Policy Evaluation"
        S1_eval[State s'] --> A_OOD["Action 3 (OOD)"]
        A_OOD --> Q_err["Q(s', a3)"]
    end
    
    Q_err --> Target["Biased TD Target"]
    Target --> Q_Net["Q-Network"]

    class A_OOD,Q_err error-class
                </div>
                <p style="text-align:center; font-style:italic;">The distributional shift problem. The learned policy considers an out-of-distribution action, leading to an erroneous Q-value from the function approximator, which in turn corrupts the training target.</p>
            </div>
            
            <div class="interactive-lab">
                <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: The Failure of Naive Offline RL</div>
                <p>This lab demonstrates why you can't just apply a standard off-policy algorithm (like SAC) to an offline dataset. We have a simple continuous control task where the optimal action is 0.5. The offline dataset was collected by a behavior policy that mostly took actions around -0.5. Watch how the Q-function of a naive learner quickly learns to exploit its own errors and believes that a large, unseen action is optimal.</p>
                <div id="offline-fail-plot" style="width:100%; height:350px;"></div>
            </div>
            <script>
            document.addEventListener('DOMContentLoaded', () => {
                const plotDiv = document.getElementById('offline-fail-plot');
                if (!plotDiv) return;

                function run_offline_fail_demo() {
                    const actions = Array.from({length: 101}, (_, i) => -1 + i * 0.02);
                    const true_q = actions.map(a => -Math.pow(a - 0.5, 2)); // True Q is a parabola peaking at 0.5
                    
                    // Behavior policy is centered at -0.5
                    const behavior_dist = actions.map(a => Math.exp(-20 * Math.pow(a - (-0.5), 2)));
                    
                    // Simulate a naive Q-learner's estimates after a few updates
                    // It fits the data well in-distribution, but has large errors out-of-distribution
                    const learned_q = actions.map((a, i) => {
                        if (a > 0) { // Out-of-distribution
                            return true_q[i] + 0.5 * Math.sin(a * 10) + (a - 0.5) * 2; // Erroneous and high
                        }
                        return true_q[i] + (Math.random() - 0.5) * 0.1; // Fits well in-distribution
                    });
                    
                    const max_q_action = actions[learned_q.indexOf(Math.max(...learned_q))];

                    Plotly.react(plotDiv, [
                        {x: actions, y: true_q, name: 'True Q-Function', line: {color: 'grey', dash: 'dash'}},
                        {x: actions, y: behavior_dist.map(d => d * -2), name: 'Behavior Policy Data Density', fill: 'tozeroy', line: {color: 'rgba(255, 127, 14, 0.5)'}},
                        {x: actions, y: learned_q, name: 'Learned Q-Function (Naive)', line: {color: 'red'}}
                    ], {
                        title: 'Failure of Naive Q-Learning in Offline RL',
                        xaxis: {title: 'Action'}, yaxis: {title: 'Q-Value'},
                        annotations: [{
                            x: max_q_action, y: Math.max(...learned_q),
                            text: `Erroneous Peak at OOD Action: ${max_q_action.toFixed(2)}`,
                            showarrow: true, arrowhead: 2, ax: 0, ay: -40
                        }]
                    }, {responsive: true});
                }
                run_offline_fail_demo();
            });
            </script>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-cogs"></i>
            8.3 Key Algorithms: Constrain or Be Conservative
          </summary>
          <div class="details-content">
            <p>
              Modern offline RL algorithms are designed to explicitly combat the distributional shift problem. They are all built around one of two core principles, which can be thought of as two different philosophical approaches to dealing with the uncertainty of out-of-distribution actions.
            </p>
            <ol>
                <li><strong>Policy Constraint:</strong> "If I'm not sure about an action because I haven't seen it much, I shouldn't take it." These methods explicitly constrain the learned policy to stay close to the behavior policy that generated the data.</li>
                <li><strong>Conservatism / Pessimism:</strong> "If I'm not sure about an action, I should assume it's bad." These methods modify the learning objective to be pessimistic about the values of unseen actions, preventing the agent from exploiting its own extrapolation errors.</li>
            </ol>

            <h4>Policy Constraint: Behavior Cloning with a Q-Filter (BCQ)</h4>
            <p>
              <strong>Batch-Constrained Q-Learning (BCQ)</strong>, introduced by Fujimoto et al. in 2019, is a prime example of a policy constraint method. It directly prevents the policy from selecting actions that are far from the data distribution.
            </p>
            <h5>The BCQ Mechanism</h5>
            <ol>
                <li><strong>Learn the Behavior Policy:</strong> First, BCQ trains a generative model, typically a conditional Variational Autoencoder (VAE), on the state-action pairs in the dataset. This model, $G_\omega(s)$, learns to generate a batch of actions that are "similar" to the actions the behavior policy would have taken in state $s$.</li>
                <li><strong>Q-Filtered Action Selection:</strong> During the Bellman backup, instead of taking the `max` over all possible actions, the agent first uses the generative model to produce a set of $k$ plausible candidate actions. It then selects the action from this restricted set that has the highest Q-value.
                $$ y = r + \gamma \max_{a_i \in \{G_\omega(s')\}_{i=1}^k} Q_{\theta^-}(s', a_i) $$
                </li>
            </ol>
            <p>
              By restricting the maximization to only actions that are "in-distribution" according to the learned behavior model, BCQ effectively avoids querying the Q-function with arbitrary OOD actions, thus preventing extrapolation errors from corrupting the TD target.
            </p>

            <h4>Conservatism: Conservative Q-Learning (CQL)</h4>
            <p>
              <strong>Conservative Q-Learning (CQL)</strong>, introduced by Kumar et al. in 2020, is a powerful and popular algorithm based on the principle of conservatism. Instead of constraining the policy, it modifies the Q-function's learning objective itself.
            </p>
            <h5>The CQL Mechanism</h5>
            <p>
              The core idea is to add a regularizer to the standard Bellman error loss. This regularizer forces the learned Q-function to assign low values to OOD actions while pushing up the values of actions that were actually seen in the dataset.
            </p>
            <p>The modified CQL objective is:</p>
            $$ \mathcal{L}_{CQL}(\theta) = \alpha \left( \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\theta(\cdot|s)} [Q_\theta(s,a)] - \mathbb{E}_{s,a \sim \mathcal{D}} [Q_\theta(s,a)] \right) + \mathcal{L}_{Bellman}(\theta) $$
            <p>Let's break down the regularizer (the first term):</p>
            <ul>
                <li>$\mathbb{E}_{s,a \sim \mathcal{D}} [Q_\theta(s,a)]$: This term pushes UP the Q-values for the state-action pairs that are in the dataset.</li>
                <li>$\mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\theta(\cdot|s)} [Q_\theta(s,a)]$: This term pushes DOWN the Q-values for actions that the current learned policy would take. Since the policy will explore and try to find high Q-value regions, this term specifically targets and suppresses potentially erroneous high Q-values on OOD actions.</li>
                <li>$\alpha$: This is a hyperparameter that controls the strength of the conservative penalty.</li>
            </ul>
            <p>By learning a Q-function that is a conservative lower bound of the true Q-function, CQL ensures that even if the policy queries an OOD action, its value will be appropriately low, preventing the agent from exploiting its own errors.</p>

            <div class="visualization-container">
                <div id="cql-plot" style="width:100%; height:400px;"></div>
            </div>
            <script>
            document.addEventListener('DOMContentLoaded', () => {
                const plotDiv = document.getElementById('cql-plot');
                if (!plotDiv) return;

                const actions = Array.from({length: 101}, (_, i) => -1 + i * 0.02);
                const true_q = actions.map(a => -Math.pow(a - 0.5, 2));
                const behavior_dist = actions.map(a => Math.exp(-20 * Math.pow(a - (-0.5), 2)));
                
                const naive_q = actions.map((a, i) => (a > 0) ? true_q[i] + 0.5 * Math.sin(a * 10) + (a - 0.5) * 2 : true_q[i] + (Math.random() - 0.5) * 0.1);
                // CQL pushes down the OOD values
                const cql_q = naive_q.map((q, i) => (actions[i] > 0) ? q - 1.5 * (actions[i]) : q);

                Plotly.react(plotDiv, [
                    {x: actions, y: true_q, name: 'True Q-Function', line: {color: 'grey', dash: 'dash'}},
                    {x: actions, y: behavior_dist.map(d => d * -2), name: 'Behavior Policy Data Density', fill: 'tozeroy', line: {color: 'rgba(255, 127, 14, 0.5)'}},
                    {x: actions, y: naive_q, name: 'Naive Q-Function', line: {color: 'red', dash: 'dot'}},
                    {x: actions, y: cql_q, name: 'CQL Q-Function (Conservative)', line: {color: 'green', width: 4}}
                ], {
                    title: 'How CQL Corrects Extrapolation Errors',
                    xaxis: {title: 'Action'}, yaxis: {title: 'Q-Value'}
                }, {responsive: true});
            });
            </script>
            <p style="text-align:center; font-style:italic;">The CQL regularizer effectively "pushes down" on the erroneously high Q-values in out-of-distribution regions, resulting in a conservative but more reliable value function.</p>

            <h4>Implicit Q-Learning (IQL)</h4>
            <p>
              <strong>Implicit Q-Learning (IQL)</strong>, by Kostrikov et al. (2021), offers a simpler and often very effective alternative. It completely avoids querying the Q-function on OOD actions by cleverly reformulating the update. IQL learns a state-value function $V$ and a Q-function simultaneously. The key insight is that for actions within the dataset, the advantage $A(s,a) = Q(s,a) - V(s)$ should be high for good actions. IQL learns the Q-function using standard Bellman updates but learns the V-function using <strong>expectile regression</strong>, which provides a more robust estimate. The policy is then learned by simply doing supervised learning (behavior cloning) weighted by the exponentiated advantage values. This elegant approach avoids any explicit maximization over actions in the target, sidestepping the core issue of distributional shift.
            </p>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-notes-medical"></i>
            8.4 Case Study: Learning Medical Treatment Policies
          </summary>
          <div class="details-content">
            <p>
              One of the most promising and high-impact applications of offline RL is in learning optimal treatment policies from electronic health records (EHR). In this setting, the problem is to learn a policy that maps a patient's current clinical state to a medical intervention to maximize their long-term health outcome.
            </p>
            <ul>
                <li><strong>Dataset:</strong> A large, anonymized database of patient histories from an ICU (e.g., the MIMIC dataset).</li>
                <li><strong>State ($s_t$):</strong> A high-dimensional vector of a patient's vital signs, lab results, and demographic information at a given time.</li>
                <li><strong>Action ($a_t$):</strong> The treatment administered by the human doctor, e.g., the dosage of vasopressors or intravenous fluids.</li>
                <li><strong>Reward ($r_t$):</strong> A reward function designed to reflect patient outcome. For example, a large positive reward for surviving and being discharged, a large negative reward for mortality, and small intermediate rewards/penalties for maintaining stable vitals.</li>
            </ul>
            <p>
              Researchers have applied offline RL algorithms like CQL to these datasets. The learned policies are then evaluated in a safe, offline manner using techniques like off-policy evaluation. In several retrospective studies, the policies learned by offline RL have been shown to suggest treatments that would have led to better patient outcomes (lower mortality rates) than the policies actually followed by human clinicians. This demonstrates the immense potential of offline RL to distill best practices from vast amounts of clinical data and assist doctors in making critical, life-saving decisions.
            </p>
          </div>
        </details>
</section>
      <hr/>
      <section id="part9">
        <h2 id="part9-title">
          <i class="fas fa-hands-helping"></i>
          Part 9: Reinforcement Learning from Human Feedback (RLHF)
        </h2>
        <p>
            One of the most significant breakthroughs in the practical application of AI, particularly for Large Language Models (LLMs) like ChatGPT, is a technique that combines the power of reinforcement learning with the nuance of human judgment. This paradigm, known as <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>, addresses a fundamental challenge in AI alignment: how do we specify complex, often ineffable human values and preferences as a reward function?
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-pencil-ruler"></i>
            9.1 The Problem: The Difficulty of Reward Engineering
          </summary>
          <div class="details-content">
            <p>
              As we've discussed, the reward function is the bedrock of an RL agent's behavior. For simple goals, like winning a game of chess (+1 for win, -1 for loss), the reward is easy to define. But what about for an AI assistant? What is the reward for being "helpful," "harmless," "truthful," or "charismatic"?
            </p>
            <p>
              Manually designing a reward function for such complex, subjective goals is practically impossible. Any simple heuristic is bound to have loopholes that can be exploited, leading to reward hacking. For example, a model rewarded for "long answers" might produce verbose but useless text. A model rewarded for "using complex words" might become unreadable.
            </p>
            <div class="admonition danger">
                <span class="admonition-title"><i class="fas fa-book-dead"></i>The King Midas Problem</span>
                <p>
                    The challenge of reward specification is often called the "King Midas Problem" or the "Sorcerer's Apprentice Problem." The agent will optimize for exactly what you tell it to, not what you <em>meant</em> for it to do. If you tell it to maximize paperclip production, it might turn the entire Earth into paperclips. RLHF is an attempt to solve this by learning the objective function itself from human examples, rather than specifying it by hand.
                </p>
            </div>
            <p>
              RLHF's core insight is that for many complex tasks, it is much easier for a human to <em>compare</em> two outputs and say which one is better than it is to write down a precise numerical score for a single output. By learning from a large dataset of these human comparisons, we can train a <strong>reward model</strong> that acts as a proxy for human preferences.
            </p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-sitemap"></i>
            9.2 The Three-Stage RLHF Pipeline
          </summary>
          <div class="details-content">
            <p>
              The full RLHF process, as popularized by OpenAI, is a sophisticated three-stage pipeline.
            </p>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Stage 1: SFT"
        A[Pre-trained LM] --> B{Demonstrations}
        B --> C[Fine-tune] --> D(SFT Model)
    end
    
    subgraph "Stage 2: Reward Model"
        D --> E{Human Preferences}
        E --> F[Train RM] --> G(Reward Model)
    end

    subgraph "Stage 3: RL Optimization"
        H[Copy SFT Model] --> I{Generate Response}
        I --> G
        G --> PPO[PPO Update] --> H
        
        subgraph KL_Penalty["Constraint"]
            H --> D
            D --> PPO
        end
    end

    class G,H model-class
                </div>
                <p style="text-align:center; font-style:italic;">The three stages of the RLHF pipeline: Supervised Fine-Tuning, Reward Model Training, and Reinforcement Learning via PPO.</p>
            </div>

            <h5>Stage 1: Supervised Fine-Tuning (SFT)</h5>
            <p>
                We start with a large, pre-trained language model (e.g., GPT-4). This model has a general understanding of language but is not specialized for any particular task. The first step is to fine-tune it on a smaller, high-quality dataset of curated demonstrations. This dataset consists of (prompt, desired_response) pairs created by human labelers. The model is trained using standard supervised learning to mimic these demonstrations. The result is the SFT model, which serves as a strong starting point for the reinforcement learning stage.
            </p>

            <h5>Stage 2: Reward Model Training</h5>
            <p>
                This is the heart of learning from human preferences.
            </p>
            <ol>
                <li><strong>Data Collection:</strong> We take a prompt and use the SFT model to generate several different responses (e.g., A, B, C, D).</li>
                <li><strong>Human Ranking:</strong> A human labeler is shown the prompt and the responses and is asked to rank them from best to worst (e.g., D > A > C > B). This process is repeated for thousands of prompts.</li>
                <li><strong>Training the Reward Model:</strong> This ranking data is used to train a separate language model, the <strong>Reward Model (RM)</strong>. The RM takes a (prompt, response) pair as input and outputs a single scalar value that represents "goodness" or "preference score." It is trained on all pairs of responses from the ranking data, with a loss function that encourages the score of the preferred response to be higher than the score of the less-preferred response.
                $$ \mathcal{L}(\phi) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log(\sigma(r_\phi(x, y_w) - r_\phi(x, y_l))) \right] $$
                Here, $y_w$ is the winning response and $y_l$ is the losing response for prompt $x$. The model is trained to maximize the log-likelihood of the human-provided rankings.
                </li>
            </ol>

            <div class="interactive-lab">
                <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Reward Modeling</div>
                <p>This lab visualizes the reward modeling process. We have two responses, A and B, with underlying "true" scores. The human labeler provides noisy feedback, sometimes making mistakes. The reward model is trained on this noisy feedback. Observe how the model learns to approximate the true scores, even with imperfect data.</p>
                <div class="lab-controls">
                    <div class="lab-control">
                        <label for="noise-slider">Human Noise Level: <span id="noise-value">0.1</span></label>
                        <input type="range" id="noise-slider" min="0" max="0.5" step="0.01" value="0.1">
                    </div>
                    <button id="train-rm-btn" style="padding: 8px 16px;">Train Reward Model</button>
                </div>
                <div id="rm-plot" style="width:100%; height:350px;"></div>
            </div>
            <script>
            document.addEventListener('DOMContentLoaded', () => {
                const rmPlotDiv = document.getElementById('rm-plot');
                if (!rmPlotDiv) return;

                const noiseSlider = document.getElementById('noise-slider');
                const trainRmBtn = document.getElementById('train-rm-btn');

                function update_rm_plot() {
                    const noise = parseFloat(noiseSlider.value);
                    document.getElementById('noise-value').textContent = noise.toFixed(2);

                    const true_scores = {'Response A': 2.0, 'Response B': -1.0};
                    let learned_scores = {'Response A': 0.0, 'Response B': 0.0};
                    
                    // Simulate training
                    for (let i = 0; i < 100; i++) {
                        let human_preference = 'A';
                        if (Math.random() < noise) {
                            human_preference = 'B';
                        }
                        
                        if (human_preference === 'A') {
                            if (learned_scores['Response A'] < learned_scores['Response B'] + 1) {
                                learned_scores['Response A'] += 0.1;
                                learned_scores['Response B'] -= 0.1;
                            }
                        } else {
                            if (learned_scores['Response B'] < learned_scores['Response A'] + 1) {
                                learned_scores['Response B'] += 0.1;
                                learned_scores['Response A'] -= 0.1;
                            }
                        }
                    }

                    Plotly.react(rmPlotDiv, [
                        {x: ['Response A', 'Response B'], y: [true_scores['Response A'], true_scores['Response B']], name: 'True Scores', type: 'bar'},
                        {x: ['Response A', 'Response B'], y: [learned_scores['Response A'], learned_scores['Response B']], name: 'Learned Scores', type: 'bar'}
                    ], {
                        title: 'Reward Model Training',
                        barmode: 'group',
                        yaxis: {title: 'Preference Score'}
                    }, {responsive: true});
                }

                noiseSlider.addEventListener('input', update_rm_plot);
                trainRmBtn.addEventListener('click', update_rm_plot);
                update_rm_plot();
            });
            </script>

            <h5>Stage 3: Reinforcement Learning with PPO</h5>
            <p>
                Now we have a policy (the SFT model) and a reward function (the RM). We can finally perform reinforcement learning.
            </p>
            <ul>
                <li><strong>The RL Environment:</strong> The "environment" is the prompt dataset. At each step, a random prompt is sampled and given to the policy.</li>
                <li><strong>The Action Space:</strong> The action space is the entire vocabulary of the language model. The policy generates a response one word (token) at a time.</li>
                <li><strong>The Reward:</strong> After a full response is generated, it is shown to the Reward Model, which produces a scalar reward.</li>
                <li><strong>The Update:</strong> This reward is used to update the policy using an RL algorithm, almost always PPO. The objective is to maximize the reward from the RM, but with a crucial constraint: a <strong>KL-divergence penalty</strong>. This penalty term ensures that the updated policy does not stray too far from the original SFT model. This is critical to prevent the policy from finding a bizarre, "un-language-like" sequence of tokens that happens to trick the reward model into giving a high score (a form of reward hacking).
                $$ \text{Objective}(\theta) = \mathbb{E}_{x \sim \mathcal{D}} \left[ r_\phi(x, y) - \beta D_{KL}(\pi_\theta(y|x) || \pi_{SFT}(y|x)) \right] $$
                </li>
            </ul>
            <p>
                This PPO optimization stage is what fine-tunes the model to be more helpful, follow instructions, and align with the nuanced preferences captured by the reward model.
            </p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-exclamation-triangle"></i>
            9.3 Limitations and Ethical Considerations
          </summary>
          <div class="details-content">
            <p>
              While incredibly powerful, RLHF is not a silver bullet and has significant limitations.
            </p>
            <ul>
                <li><strong>Scalability of Human Feedback:</strong> The process is expensive and slow, as it requires a large amount of high-quality human labor for both demonstrating and ranking.</li>
                <li><strong>Bias in Human Preferences:</strong> The final model will inherit the biases, blind spots, and cultural values of the human labelers. If the labelers have a particular political or social bias, the "aligned" model will reflect that. This makes the issue of labeler diversity and training critical.</li>
                <li><strong>Reward Hacking and "Goodhart's Law":</strong> The reward model is only a proxy for true human preference. As the policy is optimized against the RM, it can find exploits or edge cases that achieve a high score from the RM but are not actually good responses. This is an example of Goodhart's Law: "When a measure becomes a target, it ceases to be a good measure."</li>
                <li><strong>Transparency and Interpretability:</strong> The final model is a black box. It is difficult to understand why it makes certain decisions or how the human feedback shaped its internal representations.</li>
            </ul>
            <p>
              Addressing these challenges is a major open area of research in AI alignment and safety.
            </p>
          </div>
        </details>
</section>
      <hr/>
      <section id="part10">
        <h2 id="part10-title">
          <i class="fas fa-users"></i>
          Part 10: Multi-Agent Reinforcement Learning (MARL)
        </h2>
        <p>
            Thus far, our journey has focused on a single agent learning in a static or stochastic, but ultimately passive, environment. However, the real world is rarely so simple. Most interesting environments are filled with other agents, all learning, adapting, and pursuing their own goals. This introduces a profound new challenge. This is the domain of <strong>Multi-Agent Reinforcement Learning (MARL)</strong>.
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-wave-square"></i>
            10.1 The Core Challenge: Non-Stationarity
          </summary>
          <div class="details-content">
            <p>
              The foundational assumption of single-agent RL is that the environment's dynamics are stationary. The rules of the game, $P(s'|s,a)$, do not change over time. In a multi-agent setting, this assumption is shattered. From the perspective of any single agent, the other agents are part of the environment. As those other agents learn and adapt their policies, the environment's dynamics become <strong>non-stationary</strong>.
            </p>
            <div class="admonition danger">
                <span class="admonition-title"><i class="fas fa-futbol"></i>The Moving Goalposts Problem</span>
                <p>
                    Imagine you are an RL agent learning to play soccer. You learn a brilliant strategy to dribble down the left side because you've learned that the opponent's defender on that side is slow. But what happens when that defender also learns? They adapt their policy to better counter your strategy.
                </p>
                <p>
                    From your perspective, the "rules" of the environment have just changed. The transition probability $P(\text{goal} | \text{dribble left})$ is now much lower. What was a good policy yesterday is a bad policy today. This is the "moving goalposts" problem, and it makes learning incredibly difficult. A naive single-agent RL algorithm applied in a multi-agent setting will fail because it is trying to learn a stationary policy for a non-stationary world.
                </p>
            </div>
            <p>
              MARL algorithms are specifically designed to handle this non-stationarity, allowing a collection of agents to learn effective, coordinated, or competitive strategies.
            </p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-sitemap"></i>
            10.2 The MARL Framework: From MDPs to Stochastic Games
          </summary>
          <div class="details-content">
            <p>
              The standard MDP is insufficient for modeling multi-agent systems. The formal framework for MARL is the <strong>Stochastic Game</strong> (or Markov Game). A stochastic game for $N$ agents is a tuple $(S, A_1, \dots, A_N, P, R_1, \dots, R_N, \gamma)$.
            </p>
            <ul>
                <li>$S$: A shared state space.</li>
                <li>$A_1, \dots, A_N$: An action space for each agent.</li>
                <li>$P(s'|s, a_1, \dots, a_N)$: The transition function now depends on the <strong>joint action</strong> of all agents.</li>
                <li>$R_1, \dots, R_N$: A reward function for each agent, which also depends on the joint action.</li>
            </ul>
            <p>The type of MARL problem is defined by the structure of the reward functions:</p>
            <ul>
                <li><strong>Fully Cooperative:</strong> All agents share the same reward function ($R_1 = R_2 = \dots = R_N$). Their goal is to coordinate to maximize a team reward. Example: a team of robots cleaning a room.</li>
                <li><strong>Fully Competitive:</strong> A zero-sum game, where the rewards for all agents sum to zero ($ \sum_i R_i = 0$). One agent's gain is another's loss. Example: a game of chess.</li>
                <li><strong>Mixed (General-Sum):</strong> Agents have their own individual reward functions that are not necessarily opposed. This is the most general and complex setting. Example: competing companies in a market.</li>
            </ul>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-users-cog"></i>
            10.3 The CTDE Paradigm: Centralized Training, Decentralized Execution
          </summary>
          <div class="details-content">
            <p>
              A major breakthrough in MARL is the paradigm of <strong>Centralized Training with Decentralized Execution (CTDE)</strong>. This approach elegantly addresses the non-stationarity problem.
            </p>
            <ul>
                <li><strong>Centralized Training:</strong> During the learning phase (in simulation), we use a centralized critic that has access to the full state and the actions of <em>all</em> agents. This critic can learn an accurate value function for the joint action, overcoming the non-stationarity problem because it has a global view.</li>
                <li><strong>Decentralized Execution:</strong> At test time, each agent acts using only its own local observations and its own actor network. The centralized critic is discarded.</li>
            </ul>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Centralized Training"
        S[Global State] --> Critic["Centralized Critic"]
        A1[Action 1] --> Critic
        A2[Action 2] --> Critic
        Critic --> Actor1["Actor 1"]
        Critic --> Actor2["Actor 2"]
    end
    
    subgraph "Decentralized Execution"
        O1[Observation 1] --> Actor1_dec["Actor 1"]
        O2[Observation 2] --> Actor2_dec["Actor 2"]
        Actor1_dec --> A1_dec[Action 1]
        Actor2_dec --> A2_dec[Action 2]
    end

                </div>
            </div>
            <p>This "best of both worlds" approach is the foundation for many modern MARL algorithms.</p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-cogs"></i>
            10.4 Key Algorithms in MARL
          </summary>
          <div class="details-content">
            <h4>MADDPG: Multi-Agent DDPG</h4>
            <p>
              <strong>Multi-Agent Deep Deterministic Policy Gradient (MADDPG)</strong> is a direct application of the CTDE paradigm to DDPG. Each agent has its own actor and critic. The critics are augmented during training to include the observations and actions of all other agents. This allows the critics to learn a stable Q-function, which is then used to guide the training of the decentralized actors.
            </p>
            
            <h4>QMIX: Q-Learning for Cooperative Settings</h4>
            <p>
              <strong>QMIX</strong> is a powerful algorithm for fully cooperative tasks. It is based on the principle of <strong>value decomposition</strong>. It learns a joint action-value function, $Q_{tot}$, as a monotonic combination of the individual Q-values of each agent. This is enforced by a mixing network that takes the individual Q-values as input and outputs $Q_{tot}$. The key constraint is that the mixing network must have positive weights, which ensures that an increase in an individual agent's Q-value will always lead to an increase in the total team Q-value. This simplifies the credit assignment problem.
            </p>
          </div>
        </details>

        <div class="interactive-lab">
            <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: The Coordination Challenge</div>
            <p>This lab visualizes a simple cooperative MARL problem. Two agents start at opposite corners of a grid and must coordinate to reach two goals simultaneously. If they learn independently (without communication or a centralized critic), they often fail to coordinate. A centralized approach allows them to learn complementary paths.</p>
            <div class="lab-controls">
                <button id="marl-run-btn" style="padding: 8px 16px;">Run Simulation</button>
            </div>
            <div id="marl-grid" style="width: 200px; height: 200px; border: 1px solid var(--color-border); position: relative;"></div>
        </div>
        <script>
        document.addEventListener('DOMContentLoaded', () => {
            const marlGrid = document.getElementById('marl-grid');
            if (!marlGrid) return;

            document.getElementById('marl-run-btn').addEventListener('click', () => {
                marlGrid.innerHTML = '';
                const agent1 = document.createElement('div');
                agent1.style.cssText = 'width: 20px; height: 20px; background: blue; position: absolute; top: 0; left: 0; transition: all 0.5s;';
                const agent2 = document.createElement('div');
                agent2.style.cssText = 'width: 20px; height: 20px; background: red; position: absolute; top: 180px; left: 180px; transition: all 0.5s;';
                marlGrid.append(agent1, agent2);

                setTimeout(() => {
                    agent1.style.top = '180px';
                    agent1.style.left = '0px';
                    agent2.style.top = '0px';
                    agent2.style.left = '180px';
                }, 500);
            });
        });
        </script>
    </details>
</section>


      <hr/>
      <section id="part11">
        <h2 id="part11-title">
          <i class="fas fa-fast-forward"></i>
          Part 11: Meta-Learning and Transfer Learning
        </h2>
        <p>
            A significant limitation of the algorithms we've studied so far is their need to learn a new policy from scratch for every new task. A human who has learned to play one video game can very quickly learn to play a similar one; they don't start from zero. This ability to leverage past experience to adapt quickly to new situations is a hallmark of intelligence. <strong>Meta-Learning</strong>, or "learning to learn," is the subfield of machine learning that aims to imbue our agents with this capability.
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-graduation-cap"></i>
            11.1 The Goal: Rapid Adaptation
          </summary>
          <div class="details-content">
            <p>
              Standard RL is data-hungry. An agent might need millions of steps to master a single task. If the task changes even slightly—a robot needs to pick up a slightly heavier object, or a game character's friction changes—the agent would traditionally need to be retrained for millions more steps. This is incredibly inefficient.
            </p>
            <p>
              The goal of Meta-Reinforcement Learning is to train an agent not on a single task, but on a <em>distribution of related tasks</em>. The objective is not to find a single optimal policy, but to learn an underlying structure or learning process that allows the agent to adapt to any new task from this distribution with very few interactions (i.e., "few-shot" adaptation).
            </p>
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-biking"></i>Analogy: Learning to Ride a Bicycle</span>
                <p>
                    <ul>
                        <li><strong>Standard RL is like learning to ride your first bicycle.</strong> It takes a long time, many falls (negative rewards), and a lot of practice to learn the complex policy of balancing, pedaling, and steering.</li>
                        <li><strong>Meta-Learning is like learning to ride a whole fleet of different bicycles.</strong> You spend a month practicing on road bikes, mountain bikes, cruisers, and unicycles. During this "meta-training" phase, you are not trying to become a world champion on any single bike. Instead, you are learning the general principles of balance and control.</li>
                        <li><strong>The Payoff (Fast Adaptation):</strong> After this meta-training, if someone gives you a completely new bicycle you've never seen before, you can likely get the hang of it in a few minutes, not a few weeks. You have "learned how to learn" to ride bicycles. This is the promise of Meta-RL.</li>
                    </ul>
                </p>
            </div>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-sitemap"></i>
            11.2 Formalizing the Meta-RL Problem
          </summary>
          <div class="details-content">
            <p>
              To formalize this, we introduce the concept of a distribution of tasks, $p(\mathcal{T})$. Each task $\mathcal{T}_i$ is a separate MDP, often sharing the same state and action spaces but having different transition dynamics $P_i$ or reward functions $R_i$.
            </p>
            <p>The learning process is split into two levels:</p>
            <ul>
                <li><strong>The Inner Loop (Adaptation):</strong> For a specific task $\mathcal{T}_i$ sampled from the distribution, the agent uses a small amount of experience (e.g., a few trajectories) to quickly adapt its policy.</li>
                <li><strong>The Outer Loop (Meta-Training):</strong> The agent's performance <em>after</em> adaptation is evaluated. The parameters of the initial policy or the learning rule itself are then updated to maximize this post-adaptation performance, averaged over all tasks in the distribution.</li>
            </ul>
            <p>The meta-objective is to find a set of meta-parameters $\theta$ that maximizes the expected return after the inner-loop adaptation:</p>
            $$ \max_{\theta} \quad \mathbb{E}_{\mathcal{T}_i \sim p(\mathcal{T})} [ J(\theta'_i) ] $$
            <p>
              where $\theta'_i$ are the parameters obtained by adapting $\theta$ to task $\mathcal{T}_i$ using a specific learning rule.
            </p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-crosshairs"></i>
            11.3 Key Algorithm: Model-Agnostic Meta-Learning (MAML)
          </summary>
          <div class="details-content">
            <p>
              <strong>Model-Agnostic Meta-Learning (MAML)</strong>, introduced by Finn et al. in 2017, is a simple yet powerful meta-learning algorithm. Its core idea is to find an initial set of policy parameters $\theta$ that is "pre-loaded" for fast adaptation. MAML finds an initialization that is not necessarily great for any single task, but is positioned such that just one or a few gradient steps can lead to very good performance on any new task from the distribution.
            </p>
            <h5>The MAML Update Mechanism</h5>
            <ol>
                <li><strong>Outer Loop: Sample a batch of tasks.</strong></li>
                <li><strong>Inner Loop (for each sampled task $\mathcal{T}_i$):</strong>
                    <ol>
                        <li>Copy the current meta-parameters: $\theta_i = \theta$.</li>
                        <li>Collect a small amount of data (e.g., $K$ trajectories) from task $\mathcal{T}_i$ using the policy $\pi_{\theta_i}$.</li>
                        <li>Compute the policy gradient loss for this data and take one (or a few) gradient descent steps to get the adapted parameters for this specific task:
                        $$ \theta'_i = \theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}(\pi_\theta) $$
                        </li>
                    </ol>
                </li>
                <li><strong>Meta-Update:</strong> Now, for each task, we evaluate the performance of the <em>adapted</em> policy $\pi_{\theta'_i}$ on new data from that task. The meta-objective is the sum of these post-adaptation performances. We then compute the gradient of this meta-objective with respect to the original meta-parameters $\theta$. This involves a "gradient through a gradient" calculation (a second-order derivative).
                $$ \theta \leftarrow \theta - \beta \nabla_\theta \sum_i \mathcal{L}_{\mathcal{T}_i}(\pi_{\theta'_i}) $$
                </li>
            </ol>
            
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph Meta-ParameterSpace
        theta["Meta-Parameters θ"]
        
        subgraph Task1["Task 1"]
            L1["Loss Surface L1"]
            Opt1("Optimal θ1*")
        end
        subgraph Task2["Task 2"]
            L2["Loss Surface L2"]
            Opt2("Optimal θ2*")
        end
        subgraph Task3["Task 3"]
            L3["Loss Surface L3"]
            Opt3("Optimal θ3*")
        end
        
        theta -- "Inner-loop update" --> theta_prime1("Adapted θ'_1")
        theta -- "Inner-loop update" --> theta_prime2("Adapted θ'_2")
        theta -- "Inner-loop update" --> theta_prime3("Adapted θ'_3")
        
        theta_prime1 --> theta
        theta_prime2 --> theta
        theta_prime3 --> theta
    end

    class theta meta-class                </div>
                <p style="text-align:center; font-style:italic;">The MAML algorithm. The meta-parameters θ are positioned such that a single gradient step (inner loop) can move them close to the optimum for any specific task (θ*). The meta-update then adjusts θ based on the performance of these adapted parameters.</p>
            </div>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-memory"></i>
            11.4 Key Algorithm: Recurrent Policies (RL²)
          </summary>
          <div class="details-content">
            <p>
              An alternative approach, called <strong>RL²</strong> (Duan et al., 2016), frames meta-learning as a single, large RL problem. The core idea is to train a single Recurrent Neural Network (RNN) agent where the RNN's hidden state serves as the "memory" of its own learning process.
            </p>
            <h5>The RL² Mechanism</h5>
            <ul>
                <li><strong>Meta-Episodes:</strong> The agent is trained over "meta-episodes." Each meta-episode consists of multiple regular episodes, all drawn from the <em>same</em> task.</li>
                <li><strong>Modified Input:</strong> The input to the RNN at each time step $t$ is not just the state $s_t$, but a tuple containing the previous action, the resulting reward, and the done flag, in addition to the current state: $(s_t, a_{t-1}, r_t, d_{t-1})$.</li>
                <li><strong>Learning in the Hidden State:</strong> The RNN's hidden state is not reset between episodes within a meta-episode. This allows the hidden state to accumulate information about the current task's dynamics and rewards. The RNN learns to use its hidden state to implicitly perform a learning algorithm.</li>
            </ul>
            <p>
              In this framework, the "slow" learning of the outer loop (the standard RL updates to the RNN's weights) trains the network to perform "fast" learning in the inner loop (by rapidly updating its hidden state based on recent experience). The agent effectively learns its own, task-specific RL algorithm in the dynamics of its recurrent state.
            </p>
          </div>
        </details>
        
        <div class="interactive-lab">
            <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Rapid Adaptation</div>
            <p>This lab visualizes the concept of rapid adaptation. A pre-trained (meta-learned) agent is compared to an agent trained from scratch on a simple navigation task. Observe how the meta-learned agent adapts to a new goal location much faster than the agent learning from scratch.</p>
            <div class="lab-controls">
                <button id="meta-run-btn" style="padding: 8px 16px;">Run Adaptation</button>
            </div>
            <div id="meta-plot" style="width:100%; height:350px;"></div>
        </div>
        <script>
        document.addEventListener('DOMContentLoaded', () => {
            const metaPlotDiv = document.getElementById('meta-plot');
            if (!metaPlotDiv) return;

            document.getElementById('meta-run-btn').addEventListener('click', () => {
                const scratch_learning = Array.from({length: 100}, (_, i) => 100 * Math.exp(-i / 20) + 10);
                const meta_learning = Array.from({length: 100}, (_, i) => 100 * Math.exp(-i / 5) + 10);

                Plotly.react(metaPlotDiv, [
                    {x: Array.from({length: 100}, (_, i) => i), y: scratch_learning, name: 'Learning from Scratch', type: 'scatter', mode: 'lines'},
                    {x: Array.from({length: 100}, (_, i) => i), y: meta_learning, name: 'Meta-Learned Agent', type: 'scatter', mode: 'lines'}
                ], {
                    title: 'Rapid Adaptation with Meta-Learning',
                    xaxis: {title: 'Adaptation Steps'},
                    yaxis: {title: 'Distance to Goal'}
                }, {responsive: true});
            });
        });
        </script>
</section>
      
      <hr/>
      <section id="part12">
        <h2 id="part12-title">
          <i class="fas fa-sitemap"></i>
          Part 12: Hierarchical Reinforcement Learning (HRL)
        </h2>
        <p>
            A major challenge for the RL algorithms we've discussed is the problem of <strong>long-horizon tasks</strong>. When a goal can only be reached after a very long sequence of actions, it becomes incredibly difficult to assign credit correctly. Rewards are so delayed that the learning signal gets washed out over time. Furthermore, exploring a vast state space to find a single rewarding path can be exponentially difficult.
        </p>
        <p>
            Humans solve this by thinking hierarchically. To make a cup of coffee, you don't think about the sequence of a thousand muscle contractions. You think in terms of sub-goals: 1. Go to the kitchen. 2. Get the coffee beans. 3. Grind the beans. 4. Brew the coffee. <strong>Hierarchical Reinforcement Learning (HRL)</strong> aims to equip our agents with this same ability to think and act at multiple levels of temporal abstraction.
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-layer-group"></i>
            12.1 The Core Idea: Temporal Abstraction
          </summary>
          <div class="details-content">
            <p>
              The central idea of HRL is to move away from "primitive" single-step actions and towards temporally extended actions, or "sub-policies." Instead of the agent's policy choosing a primitive action at every single time step, a high-level policy can choose a sub-policy that then runs for multiple time steps to achieve a sub-goal.
            </p>
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-gamepad"></i>Analogy: Playing a Video Game</span>
                <p>
                    <ul>
                        <li><strong>A "Flat" RL Agent:</strong> To solve a level, it must learn a single, massive policy that maps every possible pixel configuration to a joystick movement. To get from the start to a key, and then from the key to a door, is one long, undifferentiated sequence of actions. If the key is in a slightly different place, the entire policy might fail.</li>
                        <li><strong>A Hierarchical RL Agent:</strong> It has a high-level "manager" policy and several low-level "worker" policies.
                            <ul>
                                <li>The manager's action space is not joystick movements, but sub-goals like {"Get Key", "Open Door"}.</li>
                                <li>If the manager chooses "Get Key," it activates a specialized worker policy whose only job is to navigate to the key. This worker runs until it has the key, and then returns control to the manager.</li>
                                <li>The manager then chooses "Open Door," activating a different worker policy.</li>
                            </ul>
                        </li>
                    </ul>
                </p>
            </div>
            <p>This hierarchical structure provides several key benefits:</p>
            <ul>
                <li><strong>Improved Exploration:</strong> Instead of exploring randomly at the level of primitive actions (which is like a drunkard's walk), the manager can explore strategically at the level of sub-goals. Exploring by trying to "get the key" is far more meaningful than exploring by pressing "right" 50 times.</li>
                <li><strong>Efficient Credit Assignment:</strong> The manager operates on a much shorter time scale. It might only make two decisions ("Get Key", "Open Door") instead of a thousand. This makes it much easier to determine which high-level decision was responsible for the final reward.</li>
                <li><strong>Reusability and Transfer:</strong> The learned worker policies (or "skills") can be reused in new tasks. A "Go to Door" skill is useful in almost any indoor navigation problem.</li>
            </ul>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-stream"></i>
            12.2 The Options Framework
          </summary>
          <div class="details-content">
            <p>
              The most influential formalization of these ideas is the <strong>Options Framework</strong>, developed by Sutton, Precup, and Singh. An "option" is a generalization of a primitive action. It is a temporally extended course of action.
            </p>
            <p>An option $o$ is defined by a tuple $(I_o, \pi_o, \beta_o)$:
            </p>
            <ul>
                <li>$I_o$: The <strong>initiation set</strong>. This is the set of states $s$ in which the option can be started.</li>
                <li>$\pi_o$: The <strong>option's policy</strong>. This is a low-level policy that is followed as long as the agent is executing the option.</li>
                <li>$\beta_o$: The <strong>termination condition</strong>. This is a function that gives the probability of the option terminating in any given state.</li>
            </ul>
            <p>
              In this framework, the agent's top-level policy, $\pi_\Omega$, doesn't choose from primitive actions, but from the set of available options. Once an option is chosen, it runs until it terminates, at which point the top-level policy can choose a new option.
            </p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-chess-king"></i>
            12.3 Key Architectures in HRL
          </summary>
          <div class="details-content">
            <h4>Feudal Reinforcement Learning</h4>
            <p>
              <strong>Feudal RL</strong>, first proposed by Dayan and Hinton, provides a powerful and intuitive architecture for HRL. It consists of a two-level hierarchy:
            </p>
            <ul>
                <li><strong>The Manager:</strong> The high-level policy. The manager operates on a slower time scale. It observes the state and produces a sub-goal, $g_t$. The manager is trained to choose sub-goals that will maximize the overall environment reward.</li>
                <li><strong>The Worker:</strong> The low-level policy. The worker operates at every time step. Its goal is to achieve the sub-goal, $g_t$, given to it by the manager. The worker is rewarded by the manager for achieving these goals (<strong>intrinsic motivation</strong>), not by the environment.</li>
            </ul>
            
            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    %% -------- Manager layer --------
    subgraph mgrBlock["Manager"]
        S_t[State sₜ] --> MgrPol["Manager Policy"]
        MgrPol --> G_t[Sub‑goal gₜ]
    end

    %% -------- Worker layer --------
    subgraph wrkBlock["Worker"]
        S_t_worker["State sₜ"] --> WrkPol["Worker Policy"]
        G_t --> WrkPol
        WrkPol --> A_t[Action aₜ]
    end

    %% -------- Environment --------
    subgraph envBlock["Environment"]
        A_t --> Env((Env))
        Env --> S_t1["Next State sₜ₊₁"]
        Env --> R_ext[Extrinsic Reward]
    end

    R_ext --> MgrPol

    %% ----- Intrinsic Reward -----
    subgraph intBlock["Intrinsic Reward"]
        S_t --> IR["r_int"]
        G_t --> IR
        S_t1 --> IR
        IR --> WrkPol
    end

    %% Optional styling hooks
    class MgrPol manager-class
    class WrkPol worker-class
              </div>
            </div>

            <h4>Hierarchical DQN (h-DQN)</h4>
            <p>
              The <strong>h-DQN</strong> architecture applies these hierarchical principles to the DQN framework. It consists of a top-level DQN (the "meta-controller") and a set of lower-level DQNs (the "controllers").
            </p>
            <ul>
                <li>The meta-controller's action space is the set of sub-goals. It learns a Q-function, $Q_{meta}(s, g)$, over which goal to select in a given state.</li>
                <li>Each lower-level controller is a standard DQN trained to achieve a specific sub-goal. It is trained on an intrinsic reward signal for reaching its assigned goal.</li>
            </ul>
            <p>
              This architecture allows for the end-to-end learning of both the skills and the meta-policy that selects them.
            </p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-question-circle"></i>
            12.4 Open Challenges in HRL
          </summary>
          <div class="details-content">
            <p>
              Despite its promise, HRL faces several significant challenges:
            </p>
            <ul>
                <li><strong>Automatic Goal Discovery:</strong> How can the agent discover meaningful sub-goals on its own, without human specification? This is a major open problem, with research focusing on techniques like discovering "bottleneck" states in the environment's state space.</li>
                <li><strong>Off-Policy Learning:</strong> Applying off-policy learning in an HRL context is complex. A sub-policy executed in the past might not be useful for the current high-level policy, a problem known as "hindsight action" problem.</li>
                <li><strong>The Hierarchy Depth:</strong> Most research has focused on two-level hierarchies. How to effectively train deeper, multi-level hierarchies remains an open question.</li>
            </ul>
          </div>
        </details>
        
        <div class="interactive-lab">
            <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Hierarchical Exploration</div>
            <p>This lab visualizes the difference between flat and hierarchical exploration in a simple grid world. The agent must navigate to a key, then to a door. A flat agent explores randomly, while a hierarchical agent explores by selecting sub-goals ("go to key", "go to door").</p>
            <div class="lab-controls">
                <button id="hrl-run-btn" style="padding: 8px 16px;">Run Exploration</button>
            </div>
            <div style="display: flex; justify-content: space-around;">
                <div>
                    <p>Flat Exploration</p>
                    <div id="flat-grid" style="width: 200px; height: 200px; border: 1px solid var(--color-border); position: relative;"></div>
                </div>
                <div>
                    <p>Hierarchical Exploration</p>
                    <div id="hrl-grid" style="width: 200px; height: 200px; border: 1px solid var(--color-border); position: relative;"></div>
                </div>
            </div>
        </div>
        <script>
        document.addEventListener('DOMContentLoaded', () => {
            const flatGrid = document.getElementById('flat-grid');
            const hrlGrid = document.getElementById('hrl-grid');
            if (!flatGrid || !hrlGrid) return;

            document.getElementById('hrl-run-btn').addEventListener('click', () => {
                flatGrid.innerHTML = '';
                hrlGrid.innerHTML = '';
                
                // Flat agent
                const flat_agent = document.createElement('div');
                flat_agent.style.cssText = 'width: 20px; height: 20px; background: blue; position: absolute; top: 90px; left: 90px; transition: all 0.5s;';
                flatGrid.appendChild(flat_agent);
                setTimeout(() => {
                    flat_agent.style.top = `${Math.random() * 180}px`;
                    flat_agent.style.left = `${Math.random() * 180}px`;
                }, 500);

                // Hierarchical agent
                const hrl_agent = document.createElement('div');
                hrl_agent.style.cssText = 'width: 20px; height: 20px; background: red; position: absolute; top: 90px; left: 90px; transition: all 0.5s;';
                hrlGrid.appendChild(hrl_agent);
                const key = {top: 20, left: 150};
                const door = {top: 150, left: 20};
                // Draw key and door
                const key_el = document.createElement('div');
                key_el.style.cssText = `width: 20px; height: 20px; background: yellow; position: absolute; top: ${key.top}px; left: ${key.left}px;`;
                hrlGrid.appendChild(key_el);
                const door_el = document.createElement('div');
                door_el.style.cssText = `width: 20px; height: 20px; background: green; position: absolute; top: ${door.top}px; left: ${door.left}px;`;
                hrlGrid.appendChild(door_el);

                setTimeout(() => {
                    // Go to key
                    hrl_agent.style.top = `${key.top}px`;
                    hrl_agent.style.left = `${key.left}px`;
                    setTimeout(() => {
                        // Go to door
                        hrl_agent.style.top = `${door.top}px`;
                        hrl_agent.style.left = `${door.left}px`;
                    }, 1000);
                }, 500);
            });
        });
        </script>
</section>
      <hr/>
      <section id="part13">
        <h2 id="part13-title">
          <i class="fas fa-link"></i>
          Part 13: Causal Reinforcement Learning
        </h2>
        <p>
          A fundamental, often unspoken, assumption in most of reinforcement learning is that "correlation implies causation." An agent observes that taking action $A$ in state $S$ is correlated with high rewards, and it learns to repeat that action. However, in the complex, messy real world, this assumption can be dangerously wrong. The intersection of <strong>Causality</strong> and Reinforcement Learning is an emerging frontier that aims to build agents that can reason about the true causal mechanisms of their environment, leading to more robust and generalizable policies.
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-umbrella-beach"></i>
            13.1 The Problem: Spurious Correlations
          </summary>
          <div class="details-content">
            <p>
              A <strong>spurious correlation</strong> is a statistical relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a third, unseen factor (a "confounder"). An RL agent that learns a policy based on a spurious correlation will perform well during training but may fail catastrophically when the underlying conditions change.
            </p>
            <div class="admonition danger">
                <span class="admonition-title"><i class="fas fa-ice-cream"></i>Failure Case: The Ice Cream and Drowning Epidemic</span>
                <p>
                    Imagine an RL agent tasked with reducing the number of drowning deaths in a city. The agent is given data on daily ice cream sales and daily drowning incidents.
                </p>
                <ul>
                    <li><strong>Observation:</strong> The agent observes a strong positive correlation: on days when ice cream sales are high, drowning deaths are also high.</li>
                    <li><strong>Flawed Policy:</strong> A standard RL agent, acting on this correlation, might learn a policy to "ban ice cream sales." It believes that this action will cause a reduction in drownings.</li>
                    <li><strong>The Confounder:</strong> The policy would fail because the correlation is spurious. The true causal factor is a hidden confounder: the <strong>temperature</strong>. On hot days, more people buy ice cream, AND more people go swimming, which leads to more drownings.</li>
                </ul>
                <p>
                    Banning ice cream would have no effect on drownings. A causal RL agent would aim to first discover the true causal graph of the system and then learn to intervene on the correct variable (e.g., by promoting swimming safety on hot days).
                </p>
            </div>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-project-diagram"></i>
            13.2 The Language of Causality: Structural Causal Models
          </summary>
          <div class="details-content">
            <p>
              Causal inference provides a formal language to talk about these relationships, primarily through <strong>Structural Causal Models (SCMs)</strong> and their graphical representations. An SCM consists of a set of variables and a set of functions that describe how each variable is generated from its direct causes.
            </p>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    subgraph "Correlation-Based View"
        IceCream["Ice Cream Sales"] --- Drowning["Drowning Deaths"]
    end
    
    subgraph "Causal View"
        Temp["Temperature"] --> IceCream_C["Ice Cream Sales"]
        Temp --> Drowning_C["Drowning Deaths"]
    end

    class Temp confounder-class
                </div>
                <p style="text-align:center; font-style:italic;">A causal graph reveals the confounding variable (Temperature) that creates a spurious correlation between Ice Cream Sales and Drowning Deaths.</p>
            </div>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-cogs"></i>
            13.3 How Causal RL Can Help
          </summary>
          <div class="details-content">
            <p>
              By incorporating causal reasoning, RL agents can learn policies that are more robust and transferable. Key areas of research include:
            </p>
            <ul>
                <li><strong>Causal Discovery in MDPs:</strong> Algorithms that try to learn the causal structure of the environment's dynamics from interventional data. For example, the agent might learn that its wheel motors' RPMs cause the robot to move, but the color of the floor does not, even if a specific floor color is correlated with high speed in the training data.</li>
                <li><strong>Counterfactual Reasoning:</strong> Causal models allow an agent to ask counterfactual questions, such as "What would the reward have been if I had taken action B instead of action A in this state?" This is particularly powerful in offline RL, where it can help to evaluate policies that take actions not seen in the dataset.</li>
                <li><strong>Generalization and Transfer Learning:</strong> A policy based on true causal links is much more likely to generalize to a new, unseen environment. The agent that learns that "hot weather causes drownings" can apply this knowledge in any city, whereas the agent that learns "ice cream sales are correlated with drownings" cannot.</li>
            </ul>
          </div>
        </details>
        
        <div class="interactive-lab">
            <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Confounding</div>
            <p>This lab visualizes the concept of confounding. We have data on two variables, X and Y, that are correlated. However, there is a third, hidden variable Z that is the true cause of both. You can see how observing Z reveals the true, non-causal relationship between X and Y.</p>
            <div class="lab-controls">
                <button id="confounding-run-btn" style="padding: 8px 16px;">Generate Data</button>
            </div>
            <div id="confounding-plot" style="width:100%; height:350px;"></div>
        </div>
        <script>
        document.addEventListener('DOMContentLoaded', () => {
            const confoundingPlotDiv = document.getElementById('confounding-plot');
            if (!confoundingPlotDiv) return;

            document.getElementById('confounding-run-btn').addEventListener('click', () => {
                const z = Array.from({length: 100}, () => Math.random());
                const x = z.map(val => val + Math.random() * 0.2);
                const y = z.map(val => val + Math.random() * 0.2);

                Plotly.react(confoundingPlotDiv, [
                    {x: x, y: y, mode: 'markers', type: 'scatter', marker: {color: z, colorscale: 'Viridis'}}
                ], {
                    title: 'Confounding: X and Y are Correlated due to Z',
                    xaxis: {title: 'X'},
                    yaxis: {title: 'Y'}
                }, {responsive: true});
            });
        });
        </script>
        
        <details open class="fade-in">
            <summary>
                <i class="fas fa-exclamation-triangle"></i>
                13.4 Challenges and Open Problems in Causal RL
            </summary>
            <div class="details-content">
                <p>
                    The integration of causality into RL is a promising but challenging frontier. Several key problems remain open:
                </p>
                <ul>
                    <li><strong>Causal Discovery from Observational Data:</strong> In many cases, we only have access to observational data, where the agent cannot perform interventions to test causal hypotheses. Discovering causal relationships from purely observational data is a notoriously difficult problem.</li>
                    <li><strong>Scalability:</strong> Causal discovery algorithms are often computationally expensive and do not scale well to the high-dimensional state and action spaces of modern RL problems.</li>
                    <li><strong>Integration with Deep Learning:</strong> How to effectively integrate the symbolic nature of causal graphs with the sub-symbolic processing of deep neural networks is an active area of research.</li>
                </ul>
            </div>
        </details>

        <details open class="fade-in">
            <summary>
                <i class="fas fa-notes-medical"></i>
                13.5 Case Study: Causal RL in Healthcare
            </summary>
            <div class="details-content">
                <p>
                    A powerful application of Causal RL is in learning treatment policies from electronic health records (EHR). In this setting, the state is a patient's clinical information, the action is a treatment, and the reward is a health outcome. A standard RL agent might learn spurious correlations, for example, that patients who receive a certain aggressive treatment have worse outcomes. However, this could be due to confounding: doctors may only give this treatment to the most severely ill patients.
                </p>
                <p>
                    Causal RL methods can be used to disentangle the effect of the treatment from the effect of the patient's underlying health. By modeling the causal graph of the disease, the agent can learn a policy that recommends the best treatment for each patient, based on the true causal effect of the treatment on the outcome.
                </p>
            </div>
        </details>
</section>
      <hr/>
      <section id="part14">
        <h2 id="part14-title">
          <i class="fas fa-search-plus"></i>
          Part 14: Explainability and Interpretability in RL (XAI-RL)
        </h2>
        <p>
            As the deep reinforcement learning models we build become more powerful and are deployed in higher-stakes domains like healthcare, finance, and autonomous systems, a critical question emerges: <strong>Can we trust them?</strong> Modern deep neural networks are often referred to as "black boxes" because their decision-making processes are opaque to human understanding. <strong>Explainable AI (XAI)</strong> is a field of research dedicated to making these models more transparent. When applied to RL, this is often called XAI-RL.
        </p>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-question"></i>
            14.1 The "Black Box" Problem: Why Do We Need Explanations?
          </summary>
          <div class="details-content">
            <p>
              The need for explainability in RL goes beyond pure academic curiosity. It is essential for:
            </p>
            <ul>
                <li><strong>Debugging and Verification:</strong> If an agent is not performing as expected, we need to understand why. Is it paying attention to the right things? Has it learned a flawed or "lazy" strategy?</li>
                <li><strong>Building Trust:</strong> For a human to delegate a critical decision to an AI (e.g., a doctor considering a treatment recommended by an RL agent), they need to understand the reasoning behind the recommendation.</li>
                <li><strong>Safety and Fairness:</strong> We need to ensure that agents are not making decisions based on biased or unfair correlations in their training data.</li>
                <li><strong>Scientific Discovery:</strong> In some cases, an RL agent might discover a novel, high-performance strategy (e.g., in a game like Go or in scientific problems like protein folding). Explaining this strategy can lead to new human insights.</li>
            </ul>
          </div>
        </details>
        <details open class="fade-in">
          <summary>
            <i class="fas fa-highlighter"></i>
            14.2 Key Technique: Saliency Maps
          </summary>
          <div class="details-content">
            <p>
              For RL agents that take high-dimensional inputs like images, one of the most common and intuitive XAI techniques is the <strong>saliency map</strong>. A saliency map is a visualization, usually an overlay on the input image, that highlights the parts of the state that were most influential in the agent's decision to take a particular action. It answers the question: "What was the agent looking at when it made that choice?"
            </p>
            
            <h4>How Saliency Maps are Generated</h4>
            <p>
              The most common method for generating a saliency map for a Q-network is to use the gradient of the output with respect to the input. The process is as follows:
            </p>
            <ol>
                <li><strong>Forward Pass:</strong> Feed the current state image $S_t$ into the Q-network to get the Q-values for all actions, $Q(S_t, a; \theta)$.</li>
                <li><strong>Select Action:</strong> Identify the action $a^*$ that the agent chose (e.g., the one with the highest Q-value).</li>
                <li><strong>Backward Pass (Gradient Calculation):</strong> Compute the gradient of the Q-value for that specific action, $Q(S_t, a^*; \theta)$, with respect to the input image pixels, $\nabla_{S_t} Q(S_t, a^*; \theta)$.</li>
                <li><strong>Visualize:</strong> The resulting gradient has the same dimensions as the input image. We can take the absolute value of this gradient and normalize it to create a heatmap. Pixels with a large gradient magnitude are the ones where a small change would have a large impact on the Q-value of the chosen action. These are the "salient" pixels.</li>
            </ol>

            <div class="interactive-lab">
                <div class="lab-header"><i class="fas fa-vial"></i> Interactive Lab: Saliency in a Grid World</div>
                <p>This lab provides a dynamic visualization of a saliency map. The agent (blue circle) is navigating a grid world to reach the goal (green square). The background color of each cell represents the learned value function (brighter is better). The <strong>saliency map</strong> is shown as a red overlay, indicating the gradient of the value function. Notice how the saliency map always "points" from the agent's current location towards the higher-value states, providing a clear explanation for its movement.</p>
                <div class="lab-controls">
                    <button id="saliency-run-btn" style="padding: 8px 16px;">Run/Reset Agent</button>
                </div>
                <div id="saliency-grid-interactive" style="width: 250px; height: 250px; border: 1px solid var(--color-border); position: relative; margin: 0 auto;"></div>
            </div>
            <script>
            document.addEventListener('DOMContentLoaded', () => {
                const saliencyGrid = document.getElementById('saliency-grid-interactive');
                if (!saliencyGrid) return;

                const GRID_SIZE = 10;
                const CELL_SIZE = 25;
                const GOAL = {x: 8, y: 1};
                let agent_pos = {x: 1, y: 8};
                let animation_interval = null;

                function initializeGrid() {
                    saliencyGrid.innerHTML = '';
                    // Create value function background
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            const cell = document.createElement('div');
                            cell.style.cssText = `width: ${CELL_SIZE}px; height: ${CELL_SIZE}px; position: absolute; top: ${r * CELL_SIZE}px; left: ${c * CELL_SIZE}px;`;
                            
                            if (r === GOAL.y && c === GOAL.x) {
                                cell.style.background = 'var(--color-accent-success)';
                            } else {
                                const dist = Math.sqrt(Math.pow(c - GOAL.x, 2) + Math.pow(r - GOAL.y, 2));
                                const max_dist = Math.sqrt(Math.pow(GRID_SIZE-1, 2) * 2);
                                const value = 1 - (dist / max_dist);
                                cell.style.background = `rgba(0, 255, 0, ${value * 0.3})`;
                            }
                            saliencyGrid.appendChild(cell);
                        }
                    }
                    // Create agent element
                    const agent_el = document.createElement('div');
                    agent_el.id = 'saliency-agent';
                    agent_el.style.cssText = `width: ${CELL_SIZE}px; height: ${CELL_SIZE}px; background: var(--color-accent-primary); border-radius: 50%; position: absolute; transition: all 0.2s linear;`;
                    saliencyGrid.appendChild(agent_el);
                    updateAgentPosition();
                }

                function updateAgentPosition() {
                    const agent_el = document.getElementById('saliency-agent');
                    agent_el.style.top = `${agent_pos.y * CELL_SIZE}px`;
                    agent_el.style.left = `${agent_pos.x * CELL_SIZE}px`;
                    updateSaliency();
                }
                
                function updateSaliency() {
                    // Remove old saliency
                    saliencyGrid.querySelectorAll('.saliency-arrow').forEach(el => el.remove());
                    
                    // Calculate gradient vector (direction to goal)
                    const dx = GOAL.x - agent_pos.x;
                    const dy = GOAL.y - agent_pos.y;
                    const mag = Math.sqrt(dx*dx + dy*dy);
                    const ux = dx / mag;
                    const uy = dy / mag;
                    
                    // Create arrow element
                    const arrow = document.createElement('div');
                    arrow.className = 'saliency-arrow';
                    arrow.style.cssText = `
                        position: absolute;
                        top: ${agent_pos.y * CELL_SIZE + CELL_SIZE/4}px;
                        left: ${agent_pos.x * CELL_SIZE + CELL_SIZE/4}px;
                        width: ${CELL_SIZE/2}px;
                        height: ${CELL_SIZE/2}px;
                        border-right: 3px solid red;
                        border-top: 3px solid red;
                        transform: rotate(${Math.atan2(uy, ux) * 180 / Math.PI + 45}deg);
                        opacity: 0.8;
                    `;
                    saliencyGrid.appendChild(arrow);
                }

                function step() {
                    if (agent_pos.x === GOAL.x && agent_pos.y === GOAL.y) {
                        clearInterval(animation_interval);
                        return;
                    }
                    // Move one step greedily towards the goal
                    if (agent_pos.x < GOAL.x) agent_pos.x++;
                    else if (agent_pos.x > GOAL.x) agent_pos.x--;
                    
                    if (agent_pos.y < GOAL.y) agent_pos.y++;
                    else if (agent_pos.y > GOAL.y) agent_pos.y--;
                    
                    updateAgentPosition();
                }

                document.getElementById('saliency-run-btn').addEventListener('click', () => {
                    if (animation_interval) clearInterval(animation_interval);
                    agent_pos = {x: 1, y: 8};
                    initializeGrid();
                    animation_interval = setInterval(step, 200);
                });

                initializeGrid();
            });
            </script>
            
            <h4>Limitations of Saliency Maps</h4>
            <p>
                While intuitive, saliency maps have limitations. They can be noisy and are sensitive to small changes in the network weights. More importantly, they can sometimes be misleading. Research has shown that it's possible to create models that produce plausible-looking saliency maps that have no real connection to the model's actual decision-making process. Therefore, they should be treated as a useful debugging tool, but not as a definitive ground truth explanation.
            </p>
          </div>
        </details>	        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-project-diagram"></i>
            14.3 Key Technique: Counterfactual Explanations
          </summary>
          <div class="details-content">
            <p>
              While saliency maps answer the question "What did you look at?", another, often more powerful, form of explanation answers the question "Why did you do X instead of Y?". This is the domain of <strong>counterfactual explanations</strong>.
            </p>
            <p>
              A counterfactual explanation identifies the smallest change to the state that would have caused the agent to take a different, specified action. It provides a "what if" scenario that can be incredibly insightful for understanding an agent's decision boundaries.
            </p>
            
            <div class="admonition info">
                <span class="admonition-title"><i class="fas fa-car"></i>Analogy: A Self-Driving Car's Decision</span>
                <p>
                    Imagine a self-driving car stops at a yellow light. The passenger asks, "Why did you stop? We could have made it."
                </p>
                <ul>
                    <li><strong>A Saliency Explanation:</strong> The car might show a heatmap highlighting the yellow light and the intersection line. This is helpful, but doesn't fully explain the trade-off.</li>
                    <li><strong>A Counterfactual Explanation:</strong> The car could respond, "I stopped because the light was yellow and we were 2.1 seconds from the intersection. <strong>If we had been 1.9 seconds away, I would have proceeded.</strong>"</li>
                </ul>
                <p>
                    This counterfactual is a much more powerful explanation. It precisely identifies the critical feature (time to intersection) and the decision boundary (2.0 seconds) that led to the chosen action.
                </p>
            </div>
            
            <h4>Generating Counterfactuals</h4>
            <p>
              Finding a counterfactual explanation is typically formulated as an optimization problem. Given an original state $s$, the action taken $a = \pi(s)$, and a desired alternative action $a'$, we want to find a new state $s'$ that is:
            </p>
            <ol>
                <li><strong>As close as possible to the original state $s$.</strong> We want a minimal change, so we minimize a distance metric, $d(s, s')$.</li>
                <li><strong>Such that the policy now chooses the alternative action.</strong> We require that $\pi(s') = a'$.</li>
            </ol>
            <p>This can be written as:</p>
            $$ \arg\min_{s'} d(s, s') \quad \text{subject to} \quad \arg\max_{a''} Q(s', a'') = a' $$
            <p>
              Solving this optimization problem can be challenging, but it provides a very powerful and human-understandable form of explanation.
            </p>
          </div>
        </details>
        
        <details open class="fade-in">
          <summary>
            <i class="fas fa-tree"></i>
            14.4 Key Technique: Intrinsically Interpretable Models
          </summary>
          <div class="details-content">
            <p>
              The methods we've discussed so far are forms of <strong>post-hoc explainability</strong>: we first train a complex, black-box model (like a deep neural network) and then use another technique to try to explain its behavior. An alternative philosophy is to use a model that is <strong>interpretable by design</strong>.
            </p>
            <p>
              The idea is to sacrifice some of the expressive power of a deep network for the benefit of full transparency. While these models may not achieve the same raw, superhuman performance on complex benchmarks, their decision-making process is transparent and can be fully understood by a human expert. This is often a critical requirement in safety-critical applications.
            </p>
            <p>Examples of interpretable models in RL include:</p>
            <ul>
                <li><strong>Decision Trees:</strong> A policy can be represented as a tree of "if-then" rules. This is highly interpretable, as the full logic for every decision can be traced.</li>
                <li><strong>Linear Models:</strong> A policy or value function can be a linear combination of a set of understandable features. The weights of the linear model directly tell you the importance of each feature.</li>
                <li><strong>Generalized Additive Models (GAMs):</strong> These models represent the value function as a sum of simple, non-linear functions of each feature, allowing for non-linear effects while still being interpretable.</li>
            </ul>

            <div class="visualization-container">
                <div class="vis-toolbar"><button data-action="reset" title="Reset View"><i class="fas fa-arrows-rotate"></i></button></div>
                <div class="mermaid">
graph TD
    A["Ball Y &gt; Paddle Y?"] -->|Yes| B{Ball X &lt; Paddle X?}
    B -->|Yes| C[Move Left]
    B -->|No| D[Move Right]
    A -->|No| E{Paddle centered?}
    E -->|Yes| F[Stay Still]
    E -->|No| G[Center Paddle]
                </div>
                <p style="text-align:center; font-style:italic;">A simple, interpretable decision tree policy for the game of Pong. The entire logic of the agent can be read and understood.</p>
            </div>
          </div>
        </details>

        <details open class="fade-in">
          <summary>
            <i class="fas fa-exclamation-triangle"></i>
            14.5 Challenges and Open Problems in XAI-RL
          </summary>
          <div class="details-content">
            <p>
              XAI-RL is a young and rapidly developing field with many open challenges:
            </p>
            <ul>
                <li><strong>Faithfulness vs. Plausibility:</strong> How do we know if an explanation is <strong>faithful</strong> (accurately reflects the model's internal logic) versus just being <strong>plausible</strong> (a convincing story that a human finds believable but might be incorrect)? Many simple XAI methods can be fooled.</li>
                <li><strong>The Temporal Dimension:</strong> Explaining a decision that is based on a long sequence of past states and actions (the agent's "memory") is much harder than explaining a decision based on a single, static image.</li>
                <li><strong>Human-in-the-Loop Interaction:</strong> What is the most effective way for a human to use these explanations? How should we design interfaces that allow a human expert to understand an agent's reasoning, provide feedback, and collaboratively improve its policy?</li>
                <li><strong>Scalability:</strong> Many powerful explanation methods are computationally expensive and do not scale well to the massive neural networks used in state-of-the-art RL.</li>
            </ul>
            <p>
              As RL agents become more autonomous and are deployed in more critical roles, the need for robust, faithful, and human-understandable explanations will only continue to grow.
            </p>
          </div>
        </details>
</section>
    </main>
    
    <footer>
      <div class="footer-content">
        <div class="feedback-container">
          <span>This document is still developing. Your feedback and contributions are welcome.</span>
          <div class="footer-socials">
            <a href="mailto:taherifarnam@gmail.com" aria-label="Email" title="Email me">
              <i class="fas fa-envelope"></i>
            </a>
            <a href="https://github.com/AmirrezaFarnamTaheri" target="_blank" rel="noopener noreferrer" aria-label="GitHub" title="Find me on GitHub">
              <i class="fab fa-github"></i>
            </a>
          </div>
        </div>
        <div class="footer-meta">
          <span>Version 2.0.0</span> | <span>
          <i class="fas fa-copyright"></i> Authored and designed by Amirreza "Farnam" Taheri.
        </span>| <span id="last-updated">Last Updated: July 30, 2025</span> </span>
        </div>
      </div>
    </footer>

  </div>

  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // --- THEME MANAGEMENT ---
      const THEMES = [
          { id: 'cyberpunk', name: 'Cyberpunk' },
          { id: 'synthwave', name: '80s Synthwave' },
          { id: 'matrix', name: 'Matrix' },
          { id: 'blueprint', name: 'Blueprint' },
          { id: 'nord', name: 'Nord' },
          { id: 'dracula', name: 'Dracula' },
          { id: 'monokai', name: 'Monokai Pro' },
          { id: 'github-dark', name: 'GitHub Dark' },
          { id: 'gruvbox-dark', name: 'Gruvbox Dark' },
          { id: 'solarized-dark', name: 'Solarized Dark' },
          { id: 'solarized-light', name: 'Solarized Light' },
          { id: 'academic-light', name: 'Academic Light' },
          { id: 'catppuccin-latte', name: 'Catppuccin Latte' },
          { id: 'catppuccin-mocha', name: 'Catppuccin Mocha' },
          { id: 'gruvbox-light', name: 'Gruvbox Light' },
          { id: 'ayu-dark', name: 'Ayu Dark' },
          { id: 'palenight', name: 'Palenight' },
          { id: 'one-dark', name: 'One Dark' },
          { id: 'tokyo-night', name: 'Tokyo Night' },
          { id: 'material-light', name: 'Material Light' },
          { id: 'solarized-earth', name: 'Solarized Earth' },
          { id: 'neon-night', name: 'Neon Night' },
      ];
      const themeToggleBtnContainer = document.getElementById('theme-toggle-btn-container');
      const themeSwitcher = document.getElementById('theme-switcher');
      let currentTheme = localStorage.getItem('theme') || 'cyberpunk';

      function setTheme(themeId) {
          document.documentElement.setAttribute('data-theme', themeId);
          localStorage.setItem('theme', themeId);
          currentTheme = themeId;
          
          themeSwitcher.querySelectorAll('.theme-option').forEach(btn => {
              btn.classList.toggle('active', btn.dataset.theme === themeId);
          });

          const isDark = !['solarized-light','academic-light','catppuccin-latte','gruvbox-light','material-light','solarized-earth'].includes(themeId);
          mermaid.initialize({ startOnLoad: false, theme: isDark ? 'dark' : 'default' });
          try {
            mermaid.run();
          } catch(e) {
            console.error("Mermaid run failed:", e);
          }
      }

      function populateThemeSwitcher() {
          themeSwitcher.innerHTML = '';
          THEMES.forEach(theme => {
              const button = document.createElement('button');
              button.className = 'theme-option';
              button.textContent = theme.name;
              button.dataset.theme = theme.id;
              if (theme.id === currentTheme) button.classList.add('active');
              button.addEventListener('click', () => {
                  setTheme(theme.id);
                  themeToggleBtnContainer.classList.remove('active');
              });
              themeSwitcher.appendChild(button);
          });
      }
      
      themeToggleBtnContainer.addEventListener('click', (e) => {
        e.stopPropagation();
        themeToggleBtnContainer.classList.toggle('active');
      });

      document.addEventListener('click', (e) => {
        if (!themeToggleBtnContainer.contains(e.target)) {
            themeToggleBtnContainer.classList.remove('active');
        }
      });
      
      populateThemeSwitcher();
      setTheme(currentTheme);

      // --- TOC & NAVIGATION ---
      const sidebarNav = document.getElementById('sidebar-nav');
      const mainContent = document.getElementById('main-content');
      const container = document.getElementById('container');
      const tocToggleBtn = document.getElementById('toc-toggle-btn');
      const backToTopBtn = document.getElementById('back-to-top');

      function buildTOC() {
          if (!sidebarNav || !mainContent) return;
          const headings = mainContent.querySelectorAll('h2, h3, h4');
          const toc = document.createElement('ul');
          let currentH2List, currentH3List;

          headings.forEach(h => {
              if (!h.id) h.id = h.textContent.trim().toLowerCase().replace(/[^a-z0-9\s-]/g, '').replace(/\s+/g, '-');
              const level = parseInt(h.tagName.substring(1), 10);
              const li = document.createElement('li');
              const a = document.createElement('a');
              a.href = `#${h.id}`;
              a.textContent = h.textContent.trim();
              li.appendChild(a);

              if (level === 2) {
                  toc.appendChild(li);
                  currentH2List = document.createElement('ul');
                  li.appendChild(currentH2List);
                  currentH3List = null;
              } else if (level === 3 && currentH2List) {
                  currentH2List.appendChild(li);
                  currentH3List = document.createElement('ul');
                  li.appendChild(currentH3List);
              } else if (level === 4 && currentH3List) {
                  currentH3List.appendChild(li);
              }
          });
          sidebarNav.innerHTML = '';
          sidebarNav.appendChild(toc);
      }

      function highlightTOC() {
        const sections = Array.from(mainContent.querySelectorAll('h2, h3, h4'));
        const links = sidebarNav.querySelectorAll('a');
        if (links.length === 0) return;
        
        const observer = new IntersectionObserver(entries => {
            let visibleSections = entries.filter(e => e.isIntersecting).map(e => e.target);
            if (visibleSections.length > 0) {
                visibleSections.sort((a,b) => a.offsetTop - b.offsetTop);
                const id = visibleSections[0].id;
                links.forEach(link => {
                    link.classList.remove('active');
                    if (decodeURIComponent(link.getAttribute('href')) === `#${id}`) {
                        link.classList.add('active');
                    }
                });
            }
        }, { rootMargin: '0px 0px -80% 0px', threshold: 0 });

        sections.forEach(s => observer.observe(s));
      }
      
      if(tocToggleBtn) {
        tocToggleBtn.addEventListener('click', () => container.classList.toggle('toc-hidden'));
      }
      
      // --- PROGRESS BAR & BACK TO TOP ---
      const progressBar = document.getElementById('progress-bar');
      function updateProgressBar() {
        const scrollableHeight = document.documentElement.scrollHeight - window.innerHeight;
        const scrolled = window.scrollY;
        if (progressBar) progressBar.style.width = scrollableHeight > 0 ? `${(scrolled / scrollableHeight) * 100}%` : '0%';
        if (backToTopBtn) backToTopBtn.classList.toggle('visible', scrolled > 300);
      }
      if (backToTopBtn) {
        backToTopBtn.addEventListener('click', () => window.scrollTo({ top: 0, behavior: 'smooth' }));
      }
      window.addEventListener('scroll', updateProgressBar);
      updateProgressBar();
      
      // --- FONT SIZE ADJUSTER ---
      const fontSizeDisplay = document.getElementById('font-size-display');
      const fontIncreaseBtn = document.getElementById('font-increase');
      const fontDecreaseBtn = document.getElementById('font-decrease');
      let currentFontSize = parseInt(localStorage.getItem('fontSize') || getComputedStyle(document.documentElement).fontSize);
      if (fontSizeDisplay) {
        document.documentElement.style.fontSize = currentFontSize + 'px';
        fontSizeDisplay.textContent = currentFontSize + 'px';
        if (fontIncreaseBtn) fontIncreaseBtn.addEventListener('click', () => {
          currentFontSize = Math.min(currentFontSize + 2, 32);
          document.documentElement.style.fontSize = currentFontSize + 'px';
          fontSizeDisplay.textContent = currentFontSize + 'px';
          localStorage.setItem('fontSize', currentFontSize);
        });
        if (fontDecreaseBtn) fontDecreaseBtn.addEventListener('click', () => {
          currentFontSize = Math.max(currentFontSize - 2, 12);
          document.documentElement.style.fontSize = currentFontSize + 'px';
          fontSizeDisplay.textContent = currentFontSize + 'px';
          localStorage.setItem('fontSize', currentFontSize);
        });
      }

      // --- CODE BLOCK ACTIONS ---
      document.querySelectorAll('.copy-code-btn').forEach(button => {
          button.addEventListener('click', async () => {
              const code = button.closest('.code-container').querySelector('pre code').innerText;
              try {
                  await navigator.clipboard.writeText(code);
                  const originalText = button.innerHTML;
                  button.innerHTML = '<i class="fas fa-check"></i> Copied!';
                  setTimeout(() => { button.innerHTML = originalText; }, 2000);
              } catch (err) { console.error('Failed to copy text: ', err); }
          });
      });
      
      // --- VISUALIZATION TOOLBAR ---
      document.querySelectorAll('.visualization-container').forEach(container => {
        const visualElement = container.querySelector('.mermaid, svg, img');
        if (!visualElement) return;
        const zoomInBtn = container.querySelector('[data-action="zoom-in"]');
        const zoomOutBtn = container.querySelector('[data-action="zoom-out"]');
        const resetBtn = container.querySelector('[data-action="reset"]');
        const fullscreenBtn = container.querySelector('[data-action="fullscreen"]');

        let scale = 1, panning = false, pointX = 0, pointY = 0, start = { x: 0, y: 0 };
        function setTransform() { visualElement.style.transform = `translate(${pointX}px, ${pointY}px) scale(${scale})`; }

        visualElement.addEventListener('mousedown', e => { e.preventDefault(); panning = true; start = { x: e.clientX - pointX, y: e.clientY - pointY }; });
        visualElement.addEventListener('mouseup', () => { panning = false; });
        visualElement.addEventListener('mouseleave', () => { panning = false; });
        visualElement.addEventListener('mousemove', e => { if (!panning) return; pointX = e.clientX - start.x; pointY = e.clientY - start.y; setTransform(); });

        if(zoomInBtn) zoomInBtn.addEventListener('click', () => { scale *= 1.2; setTransform(); });
        if(zoomOutBtn) zoomOutBtn.addEventListener('click', () => { scale /= 1.2; setTransform(); });
        if(resetBtn) resetBtn.addEventListener('click', () => { scale = 1; pointX = 0; pointY = 0; visualElement.style.transform = ''; });
        if(fullscreenBtn) fullscreenBtn.addEventListener('click', () => { if (container.requestFullscreen) container.requestFullscreen(); });
      });

      // --- SEARCH ---
      const searchInput = document.getElementById('search-input');
      let markInstance = null;
      searchInput.addEventListener('input', () => {
        const query = searchInput.value.trim();
        if (markInstance) markInstance.unmark();
        if (query.length > 2) {
            import('https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js').then((Mark) => {
                markInstance = new Mark.default(content);
                markInstance.mark(query, {
                    "separateWordSearch": false,
                });
            });
        }
      });

      // --- PART-LEVEL TOGGLE BUTTONS ---
      document.querySelectorAll('h2').forEach(function(h2) {
        const text = h2.textContent.trim();
        if (/^Part\s+\d+/.test(text)) {
          const btn = document.createElement('button');
          btn.className = 'part-toggle';
          btn.innerHTML = '<i class="fas fa-layer-group"></i>'; // Use Font Awesome icon
          btn.title = 'Toggle All Sections'; // Add tooltip
          btn.addEventListener('click', function(e) {
            e.stopPropagation();
            let next = h2.nextElementSibling;
            let shouldOpen = false;
            while (next && next.tagName !== 'H2') {
              if (next.tagName === 'DETAILS' && !next.open) {
                shouldOpen = true;
                break;
              }
              next = next.nextElementSibling;
            }
            next = h2.nextElementSibling;
            while (next && next.tagName !== 'H2') {
              if (next.tagName === 'DETAILS') {
                next.open = shouldOpen;
              }
              next = next.nextElementSibling;
            }
          });
          h2.appendChild(btn);
        }
      });

      // --- INITIALIZE ALL ---
      buildTOC();
      highlightTOC();
    });
  </script>

</body>
</html>